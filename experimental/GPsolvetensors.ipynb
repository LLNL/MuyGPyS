{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2283c1-ce8d-4680-93cd-9d3a1a798c94",
   "metadata": {},
   "source": [
    "# Do the MuyGPs math with general shaped tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746ba87-102d-4314-86fe-5bc3974da505",
   "metadata": {},
   "source": [
    "## Standard GP math\n",
    "\n",
    "Observe $n$ values of $Y$: $y_1, y_2, \\ldots, y_n$.\n",
    "\n",
    "Want to predict $m$ values of $Z$: $z_{1'}, z_{2'}, \\ldots, z_{m'}$.\n",
    "\n",
    "(Primed indices will correspond to outputs/predictions and unprimed indices will correspond to input/observed things.)\n",
    "\n",
    "Ingredients:\n",
    "\n",
    "<!-- $$\n",
    "\\begin{align}\n",
    "\\mu_i &\\equiv \\mathrm{E}\\left[ Y_i \\right] & &(i=1, \\ldots, n) \\\\\n",
    "\\mu_{i'} &\\equiv \\mathrm{E}\\left[ Z_{i'} \\right] & &(i'=1, \\ldots, m) \\\\\n",
    "K_{ij} &\\equiv \\mathrm{E}\\left[ \\left(Y_i - \\mu_i \\right) \\left(Y_j - \\mu_j \\right) \\right] & &(i=1, \\ldots, n, \\, j=1,\\ldots,n) \\\\\n",
    "K_{i{j'}} &\\equiv \\mathrm{E}\\left[ \\left(Y_i - \\mu_i \\right) \\left(Z_{j'} - \\mu_{j'} \\right) \\right]  \\equiv K_{{j'}i} & &(i=1, \\ldots, n, \\, j'=1,\\ldots,m) \\\\\n",
    "K_{{i'}{j'}} &\\equiv \\mathrm{E}\\left[ \\left(Z_{i'} - \\mu_{i'} \\right) \\left(Z_{j'} - \\mu_{j'} \\right) \\right] & &(i'=1, \\ldots, m, \\, j'=1,\\ldots,m)\n",
    "\\end{align}\n",
    "$$\n",
    " -->\n",
    "$$\n",
    "\\mu_i \\equiv \\mathrm{E}\\left[ Y_i \\right] \\quad\\quad (i=1, \\ldots, n) \\\\\n",
    "\\mu_{i'} \\equiv \\mathrm{E}\\left[ Z_{i'} \\right]  \\quad\\quad (i'=1, \\ldots, m) \\\\\n",
    "K_{ij} \\equiv \\mathrm{E}\\left[ \\left(Y_i - \\mu_i \\right) \\left(Y_j - \\mu_j \\right) \\right]  \\quad\\quad (i=1, \\ldots, n, \\, j=1,\\ldots,n) \\\\\n",
    "K_{i{j'}} \\equiv \\mathrm{E}\\left[ \\left(Y_i - \\mu_i \\right) \\left(Z_{j'} - \\mu_{j'} \\right) \\right]  \\equiv K_{{j'}i}  \\quad\\quad (i=1, \\ldots, n, \\, j'=1,\\ldots,m) \\\\\n",
    "K_{{i'}{j'}} \\equiv \\mathrm{E}\\left[ \\left(Z_{i'} - \\mu_{i'} \\right) \\left(Z_{j'} - \\mu_{j'} \\right) \\right]  \\quad\\quad (i'=1, \\ldots, m, \\, j'=1,\\ldots,m)\n",
    "$$\n",
    "\n",
    "\n",
    "The conditional mean of $Z_{i'}$ given the observed $y_1, \\ldots y_n$ is (in all these equations repeated indices are summed over),\n",
    "\n",
    "$$\n",
    "\\hat{z}_{i'} = \\mu_{i'} + \\left(y_k-\\mu_k \\right) F_{k i'},\n",
    "$$\n",
    "\n",
    "and the conditional covariance between $Z_{i'}$ and $Z_{j'}$ given the observed $y_1, \\ldots y_n$ is,\n",
    "\n",
    "$$\n",
    "S_{i'j'} = K_{i'j'} - K_{i'k} F_{k j'}.\n",
    "$$\n",
    "\n",
    "The quantities $F_{k i'}$ are constants whose defining equations are,\n",
    "\n",
    "$$\n",
    "K_{ik} F_{k j'}= K_{i j'}.\n",
    "$$\n",
    "\n",
    "Finding $F$ is the \"solve\" that occurs in the GP math.\n",
    "\n",
    "\n",
    "<!-- The linear unbiased estimate of $Z_{i'}$ given observed $y_1, \\ldots y_n$ is,\n",
    "$$\n",
    "\\hat{z}_{i'} = \\mu_{i'} + F_{i' j} \\left(y_j-\\mu_j \\right),\n",
    "$$\n",
    "for some constant matrix $F_{i'j}$ (in all these equations repeated indices are summed over).\n",
    "\n",
    "Set $F_{i'j}$ by minimizing the variance of the residual $r_{i'} \\equiv \\hat{z}_{i'} - z_{i'}$ (a.k.a. the standard error $S_{i'i'}$ of the estimator),\n",
    "\\begin{align}\n",
    "S_{i'j'} \\equiv \\mathrm{E}\\left[r_{i'}r_{j'} \\right] &=  \\mathrm{E}\\left[ \\left(F_{i' k} \\left(Y_k-\\mu_k \\right) - \\left(Z_{i'}-\\mu_{i'}\\right)\\right) \\left(F_{j' l} \\left(Y_l-\\mu_l \\right) - \\left(Z_{j'}-\\mu_{j'}\\right)\\right) \\right], \\\\\n",
    "&= F_{i' k} F_{j' l} K_{kl} - F_{i' k} K_{k{j'}} - K_{{i'}l} F_{j'l} + K_{i'j'}.\n",
    "\\end{align}\n",
    "\n",
    "The variation with respect to $F_{a'b}$ is,\n",
    "\\begin{align}\n",
    "\\frac{\\partial S_{i'j'}}{\\partial F_{a'b}} &= \\delta_{i' a'} F_{j' l} K_{bl} + \\delta_{j' a'} F_{i'k} K_{kb}\n",
    "-\\delta_{i' a'} K_{b j'} - K_{i'b} \\delta_{j'a'}.\n",
    "\\end{align}\n",
    "\n",
    "We want this to be zero for all $i'$, $a'$, and $b$ with $i'=j'$. When $i'=j'$ it's a nontrivial equation only if $a'=i'=j'$. In this case,\n",
    "\\begin{align}\n",
    "0 &= \\frac{\\partial S_{i'i'}}{\\partial F_{i'b}} = F_{i' l} K_{bl} + F_{i'k} K_{kb} - K_{b i'} - K_{i'b},\\\\\n",
    "  &= F_{i' k} K_{kb} - K_{b i'}.\n",
    "\\end{align}\n",
    "\n",
    "This is the defining equation for $F_{i'j}$,\n",
    "$$\n",
    "F_{i'k} K_{kj} = K_{i' j}.\n",
    "$$\n",
    "\n",
    "Inserting this back into the covariance of the residual gives,\n",
    "\\begin{align}\n",
    "S_{i'j'} &= K_{i'l} F_{j' l} - F_{i' k} K_{k{j'}} - K_{{i'}l} F_{j'l} + K_{i'j'},\\\\\n",
    "&= K_{i'j'} - F_{j'k} K_{i'k}.\n",
    "\\end{align}\n",
    "\n",
    "If $\\mathbf{Y}$ and $\\mathbf{Z}$ are jointly Gaussian distributed then the conditional distribution of $\\mathbf{Z}$ given $\\mathbf{y}$ is also Gaussian and the mean and covariance of this conditional distribution are $\\hat{\\mathbf{z}}$ and $S$. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5862b2-59fc-4b95-8114-384b5a8bcaf9",
   "metadata": {},
   "source": [
    "## With tensor inputs and outputs\n",
    "\n",
    "Note that no meaning was given to any of the indices $i$ and $i'$ above. We can imagine these indices as multi-indices which range over the various $x$-values where $y_i$ and $z_{i'}$ are \"located\" as well as over the vector or tensor components of the $y$ and $z$ objects.\n",
    "\n",
    "For example, $i$ might be shorthand for $(i_1, i_2, i_3)$ and when you sum over $i$ in any equation above you sum over all possible combinations of $i_1$, $i_2$, and $i_3$. Maybe $y_i$ is a matrix observed at various locations. Then $i_1$ could index the nearest neighbors $x_{i_1}$ while $i_2$ and $i_3$ index the row and column of the matrix $y_{i_1}$ observed at $x_{i_1}$.\n",
    "\n",
    "### Batching\n",
    "\n",
    "The batching means that we are doing $b$ completely separate calculations (where $b$ is the batch size). In MuyGPs, each batch contains a single target point $xâ€²$ and its $k$ nearest neighbors $x_1, \\ldots x_k$.\n",
    "\n",
    "\n",
    "### Ordering of the multi-indices\n",
    "The batch indices will come first, followed by indices corresponding to the observations $y$, followed by indices corresponding to the outputs $z$.\n",
    "\n",
    "We can have complete generality in terms of tensor shapes. We don't need to have a single dimension for the batch. We also can have multiple target points $x'$ per batch (at the expense of larger cross covariance tensors). The $y$'s and $z$'s can have any number of dimensions or they can be scalars with no dimension.\n",
    "\n",
    "See the documentation for the functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076d6c0-d982-47d1-8371-063ae1ba10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc54d21-5613-4065-88e9-0b43ab04e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_mean(y, Kin, Kcross):\n",
    "    '''\n",
    "    Compute the GP posterior mean given observations y and covariances between y and the output z.\n",
    "    \n",
    "    y : array of shape (b_1, ..., b_b, i_1, ..., i_n)\n",
    "        The observations (actually the observations minus their prior means)\n",
    "        b_1 to b_b are batch dimensions, i_1 to i_n correspond to the observed y object,\n",
    "        e.g. if you observe a 3-vector at each of 10 x-values and you have a batch size of 100\n",
    "        the shape of y could be (100, 10, 3) and y[3, 7, :] would be the observed vector\n",
    "        y at location x_7 in batch 3.\n",
    "          \n",
    "    Kin : array of shape (b_1, ..., b_b, i_1, ..., i_n, i_1, ... i_n)\n",
    "          The covariance among the observations (usually a kernel plus a nugget).\n",
    "          In the example above Kin[3, 4, 1, 8, 0] is the covariance between the 1st component of \n",
    "          the vector y(x_4) and the 0th component of the vector y(x_8).\n",
    "          Kin has a \"square\" shape.\n",
    "          \n",
    "    Kcross : array of shape (b_1, ..., b_b, i_1, ..., i_n, j_1, ..., j_m)\n",
    "             The cross covariance between the inputs and outputs.\n",
    "             e.g. if you are predicting the value of the GP at 3 target points x'\n",
    "             and you are predicting a 2-vector at each of those points then\n",
    "             (j_1, ..., j_m) could be (3, 2).\n",
    "    \n",
    "    The batch shape (b_1, ..., b_b) and/or the output tensor shape (j_1, ..., j_m) can be absent.\n",
    "      I.e. you could give y : (10,), Kin : (10,10), and Kcross : (10,) to do a single GP prediction\n",
    "      from 10 observed values to a single target point.\n",
    "      Or you could do the same but with Kcross : (10,3) to use the 10 observed values to predict\n",
    "      at 3 target points.\n",
    "    \n",
    "    Returns\n",
    "    z : array of shape (b_1, ..., b_b, j_1, ..., j_m)\n",
    "        The posterior mean given y (actually the posterior mean of z minus its prior mean).\n",
    "        In the example above with one batch dimension and (j_1, ..., j_m) = (3,2), \n",
    "        z[30, 0, 1] corresponds to the prediction of the 1st\n",
    "        component of the output vector z at location x'[0].\n",
    "    '''\n",
    "    \n",
    "    #np.linalg.solve does almost exactly what we want but it needs the dimensions summed\n",
    "    # over in matrix multiplication to be 1d. So we have to unravel i_1, ... i_n into a\n",
    "    # single dimension, and similarly for j_1, ..., j_m\n",
    "    \n",
    "    #first figure out what indices correspond to batch, input, and output\n",
    "    inshape = Kin.shape[y.ndim:] #(i_1, ..., i_n)\n",
    "    outshape = Kcross.shape[y.ndim:] #(j_1, ..., j_m)\n",
    "    batchshape = y.shape[:-len(inshape)] #(b_1, ..., b_b)\n",
    "    \n",
    "    insize = math.prod(inshape)\n",
    "    outsize = math.prod(outshape)\n",
    "    \n",
    "    y_flatT = y.reshape(batchshape + (1,insize,)) # stack of row vectors y\n",
    "    Kin_flat = Kin.reshape(batchshape + (insize,insize))\n",
    "    Kcross_flat = Kcross.reshape(batchshape + (insize,outsize))\n",
    "    \n",
    "    F_flat = np.linalg.solve(Kin_flat, Kcross_flat) #shape = (b_1, ..., b_b, insize, outsize)\n",
    "    z = y_flatT @ F_flat #(b_1, ..., b_b, 1, outsize)\n",
    "    print(f\"y_flatT.shape = {y_flatT.shape}\")\n",
    "    print(f\"Kin_flat.shape = {Kin_flat.shape}\")\n",
    "    print(f\"Kcross_flat.shape = {Kcross_flat.shape}\")\n",
    "    print(f\"F_flat.shape = {F_flat.shape}\")\n",
    "    print(f\"z.shape = {z.shape}\")\n",
    "    return z.reshape(batchshape + outshape) #(b_1, ..., b_b, j_1, ..., j_m)\n",
    "\n",
    "\n",
    "def posterior_covariance(Kin, Kcross, Kout, nbatchdims):\n",
    "    '''\n",
    "    Compute the GP posterior mean given observations y and covariances between y and the output z.\n",
    "    \n",
    "    Kin : array of shape (b_1, ..., b_b, i_1, ..., i_n, i_1, ... i_n)\n",
    "          The covariance among the observations (usually a kernel plus a nugget).\n",
    "          \n",
    "    Kcross : array of shape (b_1, ..., b_b, i_1, ..., i_n, j_1, ..., j_m)\n",
    "             The cross covariance between the inputs and outputs.\n",
    "    \n",
    "    Kout : array of shape (b_1, ..., b_b, j_1, ... j_m, j_1, ..., j_m)\n",
    "           The prior covariance among the outputs.\n",
    "           E.g. if you are predicting the components of a 3-vector at 5 output \n",
    "           locations x' and doing this independently for a batch of size 100 then\n",
    "           Kout could have shape (100, 5, 3, 5, 3).\n",
    "    \n",
    "    nbatchdims : int, length of (b_1, ..., b_b) \n",
    "                (unfortunately, you can't figure this out from the shapes of\n",
    "                 Kin, Kcross, and Kout)\n",
    "    \n",
    "    Returns\n",
    "    Kpost : array of shape (b_1, ..., b_b, j_1, ..., j_m, j_1, ..., j_m)\n",
    "        The posterior variance given y.\n",
    "    '''\n",
    "    nindims = (Kin.ndim-nbatchdims)//2\n",
    "    \n",
    "    batchshape = Kin.shape[:nbatchdims] #(b_1, ..., b_b)\n",
    "    inshape = Kin.shape[nbatchdims+nindims:] #(i_1, ..., i_n)\n",
    "    outshape = Kcross.shape[nbatchdims+nindims:] #(j_1, ..., j_m)\n",
    "    \n",
    "    insize = math.prod(inshape)\n",
    "    outsize = math.prod(outshape)\n",
    "    \n",
    "    Kin_flat = Kin.reshape(batchshape + (insize,insize))\n",
    "    Kcross_flat = Kcross.reshape(batchshape + (insize,outsize))\n",
    "    \n",
    "    F_flat = np.linalg.solve(Kin_flat, Kcross_flat) #shape = (b_1, ..., b_b, insize, outsize)\n",
    "    \n",
    "    Kpost = F_flat.swapaxes(-2, -1) @ Kcross_flat # (b_1, ..., b_b, outsize, outsize)\n",
    "    return Kout - Kpost.reshape(batchshape + outshape + outshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e2596-23d9-4938-b1c4-903fb76d3238",
   "metadata": {},
   "source": [
    "### note\n",
    "It doesn't really make sense to call posterior_mean and posterior_covariance separately and do the solve for F_flat twice. If you combined them you also wouldn't have to pass in the nbatchdims arg to posterior_covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76c03a-87fe-4534-9b50-37e1b4418ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_mean_covariance(y, Kin, Kcross, Kout):\n",
    "    '''\n",
    "    Compute the GP posterior mean given observations y and covariances between outputs z given y.\n",
    "    \n",
    "    y : array of shape (b_1, ..., b_b, i_1, ..., i_n)\n",
    "        The observations (actually the observations minus their prior means)\n",
    "        b_1 to b_b are batch dimensions, i_1 to i_n correspond to the observed y object,\n",
    "        e.g. if you observe a 3-vector at each of 10 x-values and you have a batch size of 100\n",
    "        the shape of y could be (100, 10, 3) and y[3, 7, :] would be the observed vector\n",
    "        y at location x_7 in batch 3.\n",
    "          \n",
    "    Kin : array of shape (b_1, ..., b_b, i_1, ..., i_n, i_1, ... i_n)\n",
    "          The covariance among the observations (usually a kernel plus a nugget).\n",
    "          In the example above Kin[3, 4, 1, 8, 0] is the covariance between the 1st component of \n",
    "          the vector y(x_4) and the 0th component of the vector y(x_8).\n",
    "          Kin has a \"square\" shape.\n",
    "          \n",
    "    Kcross : array of shape (b_1, ..., b_b, i_1, ..., i_n, j_1, ..., j_m)\n",
    "             The cross covariance between the inputs and outputs.\n",
    "             e.g. if you are predicting the value of the GP at 3 target points x'\n",
    "             and you are predicting a 2-vector at each of those points then\n",
    "             (j_1, ..., j_m) could be (3, 2).\n",
    "    \n",
    "    Kout : array of shape (b_1, ..., b_b, j_1, ... j_m, j_1, ..., j_m)\n",
    "           The prior covariance among the outputs.\n",
    "           E.g. if you are predicting the components of a 3-vector at 5 output \n",
    "           locations x' and doing this independently for a batch of size 100 then\n",
    "           Kout could have shape (100, 5, 3, 5, 3).\n",
    "    \n",
    "    The batch shape (b_1, ..., b_b) and/or the output tensor shape (j_1, ..., j_m) can be absent.\n",
    "      I.e. you could give y : (10,), Kin : (10,10), and Kcross : (10,) to do a single GP prediction\n",
    "      from 10 observed values to a single target point.\n",
    "      Or you could do the same but with Kcross : (10,3) to use the 10 observed values to predict\n",
    "      at 3 target points.\n",
    "    \n",
    "    Returns\n",
    "    mu : array of shape (b_1, ..., b_b, j_1, ..., j_m)\n",
    "         The posterior mean given y (actually the mean of z minus its prior mean).\n",
    "         In the example above with one batch dimension and (j_1, ..., j_m) = (3,2), \n",
    "         z[30, 0, 1] corresponds to the prediction of the 1st\n",
    "         component of the output vector z at location x'[0].\n",
    "    \n",
    "    Kpost : array of shape (b_1, ..., b_b, j_1, ..., j_m, j_1, ..., j_m)\n",
    "            The posterior variance given y.\n",
    "    '''\n",
    "    \n",
    "    #np.linalg.solve does almost exactly what we want but it needs the dimensions summed\n",
    "    # over in matrix multiplication to be 1d. So we have to unravel i_1, ... i_n into a\n",
    "    # single dimension, and similarly for j_1, ..., j_m\n",
    "    \n",
    "    #first figure out what indices correspond to batch, input, and output\n",
    "    inshape = Kin.shape[y.ndim:] #(i_1, ..., i_n)\n",
    "    outshape = Kcross.shape[y.ndim:] #(j_1, ..., j_m)\n",
    "    batchshape = y.shape[:-len(inshape)] #(b_1, ..., b_b)\n",
    "    \n",
    "    insize = math.prod(inshape)\n",
    "    outsize = math.prod(outshape)\n",
    "    \n",
    "    y_flatT = y.reshape(batchshape + (1,insize,)) # stack of row vectors y\n",
    "    Kin_flat = Kin.reshape(batchshape + (insize,insize))\n",
    "    Kcross_flat = Kcross.reshape(batchshape + (insize,outsize))\n",
    "    \n",
    "    F_flat = np.linalg.solve(Kin_flat, Kcross_flat) #shape = (b_1, ..., b_b, insize, outsize)\n",
    "    \n",
    "    z = y_flatT @ F_flat #(b_1, ..., b_b, 1, outsize)\n",
    "    z = z.reshape(batchshape + outshape) #(b_1, ..., b_b, j_1, ..., j_m)\n",
    "    \n",
    "    Kpost = F_flat.swapaxes(-2, -1) @ Kcross_flat # (b_1, ..., b_b, outsize, outsize)\n",
    "    Kpost = Kout - Kpost.reshape(batchshape + outshape + outshape)\n",
    "    return z, Kpost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac5f42-dd98-4c83-a8a1-add6073f7122",
   "metadata": {},
   "source": [
    "The above functions should work for the usual case of a scalar GP.\n",
    "\n",
    "In that case,\n",
    "```\n",
    "(i_1, ..., i_n) = (nn_count,),\n",
    "(j_1, ..., j_m) is an empty tuple,\n",
    "(b_1, ..., b_b) = (batch_count,).\n",
    "```\n",
    "Then the above functions expect these tensor shapes:\n",
    "```\n",
    "  Kin    : (batch_count, nn_count, nn_count)\n",
    "  Kcross : (batch_count, nn_count)\n",
    "  y      : (batch_count, nn_count)\n",
    "  Kout   : (batch_count,)\n",
    "```\n",
    "and they will return:\n",
    "```\n",
    " z     : (batch_count,)\n",
    " Kpost : (batch_count,)\n",
    "```\n",
    "\n",
    "Therefore, I think the above `posterior_mean` and `posterior_covariance` functions can possibly replace the `_muygps_posterior_mean` and `_muygps_diagonal_variance` functions in muygpys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993c599-dc5f-4b9a-ac31-9d6862df5cdb",
   "metadata": {},
   "source": [
    "## test that shapes work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372537ff-fb7f-486b-8742-7fd126a24757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usual scalar GP predicting at a single target point in each batch\n",
    "batch_count = 100\n",
    "nn_count = 30\n",
    "\n",
    "y = np.random.rand(batch_count, nn_count)\n",
    "Kin = np.random.rand(batch_count, nn_count, nn_count)\n",
    "Kin = Kin @ Kin.swapaxes(-2,-1)\n",
    "Kcross = np.random.rand(batch_count, nn_count)\n",
    "Kout = np.random.rand(batch_count)\n",
    "\n",
    "\n",
    "assert posterior_mean(y, Kin, Kcross).shape == (batch_count,)\n",
    "assert posterior_covariance(Kin, Kcross, Kout, 1).shape == (batch_count,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f52146-5c21-46f1-b339-5f1f90ad056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar GP predicting at several target points in each batch\n",
    "batch_count = 100\n",
    "nn_count = 30\n",
    "target_count = 3\n",
    "\n",
    "y = np.random.rand(batch_count, nn_count)\n",
    "Kin = np.random.rand(batch_count, nn_count, nn_count)\n",
    "Kin = Kin @ Kin.swapaxes(-2,-1)\n",
    "Kcross = np.random.rand(batch_count, nn_count, target_count)\n",
    "Kout = np.random.rand(batch_count, target_count, target_count)\n",
    "\n",
    "\n",
    "assert posterior_mean(y, Kin, Kcross).shape == (batch_count, target_count)\n",
    "assert posterior_covariance(Kin, Kcross, Kout, 1).shape == (batch_count, target_count, target_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465732cb-d945-4c2c-970b-86a205e3dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector-valued GP predicting a vector at single target point in each batch\n",
    "batch_count = 100\n",
    "nn_count = 30\n",
    "response_count = 3\n",
    "\n",
    "y = np.random.rand(batch_count, nn_count, response_count)\n",
    "Kin = np.random.rand(batch_count, nn_count*response_count, nn_count*response_count)\n",
    "Kin = (Kin @ Kin.swapaxes(-2,-1)).reshape(batch_count, nn_count, response_count, nn_count, response_count)\n",
    "Kcross = np.random.rand(batch_count, nn_count, response_count, response_count)\n",
    "Kout = np.random.rand(batch_count, response_count, response_count)\n",
    "\n",
    "\n",
    "assert posterior_mean(y, Kin, Kcross).shape == (batch_count, response_count)\n",
    "assert posterior_covariance(Kin, Kcross, Kout, 1).shape == (batch_count, response_count, response_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a1ebf-4fd6-42b9-919d-864620e58932",
   "metadata": {},
   "source": [
    "# example of a 2d GP\n",
    "\n",
    "Observe the gradient of a GP with squared exp kernel and zero mean.\n",
    "\n",
    "$$\n",
    "K^{ij}(x_1,x_2)= \\frac{\\sigma^2}{l^2} \\exp\\left[-\\frac{1}{2l} \\lVert x_1-x_2 \\rVert^2 \\right] \\left( \\delta^{ij} - \\frac{(x_1^i-x_2^i)(x_1^j-x_2^j)}{l^2} \\right).\n",
    "$$\n",
    "\n",
    "For the vector-valued function $f^i(x)$, $K^{ij}(x_1,x_2)$ is the covariance between the the $i$-th component of the vector at location $x_1$ and the $j$-th component of the vector at $x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd80a5-ae71-4923-97fd-4aefba2b0652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6961b5d-4ab4-419f-be45-2857687f03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernelfunc(x1, x2, sigma=1, l=1):\n",
    "    '''\n",
    "    x1 : array of shape (n,d) meaning n x-values living in a d-dimensional space\n",
    "    x2 : array of shape (m,d)\n",
    "    \n",
    "    Returns\n",
    "    K : (n,d,m,d), where K[a,i,b,j] the covariance between the ith component of x1[a] and the jth component of x2[b]\n",
    "    '''\n",
    "    n,d = x1.shape\n",
    "    m,d = x2.shape\n",
    "    \n",
    "    diffs = np.empty((n,d,m,d), dtype=np.float64)\n",
    "    for i in range(d):\n",
    "        for j in range(d):\n",
    "            diffs[:,i,:,j] = (x1[:,i].reshape(-1,1)-x2[:,i].reshape(1,-1)) * (x1[:,j].reshape(-1,1)-x2[:,j].reshape(1,-1))\n",
    "    \n",
    "    dsq = sum([diffs[:,i,:,i] for i in range(d)]) #(n,m)\n",
    "    \n",
    "    eye = np.zeros((n,d,m,d), dtype=np.float64)\n",
    "    for i in range(d):\n",
    "        eye[:,i,:,i] = 1\n",
    "        \n",
    "    K = (sigma**2/l**2) * np.exp(-1/(2*l**2) * dsq[:,None,:,None]) * (eye - diffs/l**2)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefcb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 10\n",
    "l = 7\n",
    "noise = 0.3\n",
    "\n",
    "n_train = 50 \n",
    "\n",
    "x_train = 10*np.random.rand(n_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d23f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin = kernelfunc(x_train, x_train, sigma, l) #(ntrain,2,ntrain,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d961746",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvn = scipy.stats.multivariate_normal(mean=np.zeros(2*n_train), cov=Kin.reshape(n_train*2,n_train*2), allow_singular=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7dba6f-3fc3-4501-9731-f303354c9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 10\n",
    "l = 7\n",
    "noise = 0.3\n",
    "\n",
    "n_train = 50 \n",
    "\n",
    "x_train = 10*np.random.rand(n_train, 2)\n",
    "\n",
    "Kin = kernelfunc(x_train, x_train, sigma, l) #(ntrain,2,ntrain,2)\n",
    "\n",
    "# observations y_train\n",
    "mvn = scipy.stats.multivariate_normal(mean=np.zeros(2*n_train), cov=Kin.reshape(n_train*2,n_train*2), allow_singular=True)\n",
    "z_train = mvn.rvs().reshape(n_train, 2)\n",
    "\n",
    "#add noise\n",
    "y_train = z_train + np.random.randn(n_train, 2)*noise #batch size is zero and (i_1, ..., i_n) = (n_train, 2)\n",
    "\n",
    "nugget = noise**2*np.eye(n_train*2).reshape(n_train,2,n_train,2)\n",
    "Kin = Kin + nugget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd0bc5-e90a-4ee4-9865-064a4fd48d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions at a grid of points: (j_1, ..., j_m) = (g,g,2)\n",
    "g = 5 # grid size\n",
    "x_target = np.empty((g,g,2))\n",
    "x_target[:,:,0] = np.linspace(0,10,g).reshape(1,-1)\n",
    "x_target[:,:,1] = np.linspace(0,10,g).reshape(-1,1)\n",
    "\n",
    "Kcross = kernelfunc(x_train, x_target.reshape(-1,2), sigma, l).reshape(n_train, 2, g, g, 2)\n",
    "\n",
    "Kout = kernelfunc(x_target.reshape(-1,2), x_target.reshape(-1,2), sigma, l).reshape(g, g, 2, g, g, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95819a3b-81e1-4b15-8eb3-4ad40730f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred = posterior_mean(y_train, Kin, Kcross) #(g,g,2)\n",
    "z_predcov = posterior_covariance(Kin, Kcross, Kout, nbatchdims=0) #(g,g,2, g,g,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f46bba-6308-4997-9d81-c98108c1df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate 20 posterior samples for each output vector (full covariance between all output vectors in the grid)\n",
    "mvnpost = scipy.stats.multivariate_normal(mean=z_pred.reshape(z_pred.size), cov=z_predcov.reshape(z_pred.size,z_pred.size), allow_singular=True)\n",
    "zsamp = mvnpost.rvs(size=20).reshape((-1,)+z_pred.shape) #(20, g,g,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc9d07-9375-41a3-b350-b43d1cf9a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,5))\n",
    "\n",
    "plt.plot(x_train[:,0], x_train[:,1], 'o', ms=4, zorder=1)\n",
    "plt.quiver(x_train[:,0], x_train[:,1], z_train[:,0], z_train[:,1], \n",
    "           angles='xy', zorder=2, scale=1, scale_units='xy', color='k', width=0.003, label='observations (before noise)')\n",
    "plt.quiver(x_train[:,0], x_train[:,1], y_train[:,0], y_train[:,1], \n",
    "           angles='xy', zorder=2, scale=1, scale_units='xy', color='C0', width=0.003, label='observations (with noise)')\n",
    "\n",
    "# plt.plot(x_target[:,:,0].ravel(), x_target[:,:,1].ravel(), 'o', ms=4, mfc='none', color='C1')\n",
    "plt.quiver(x_target[:,:,0], x_target[:,:,1], z_pred[:,:,0], z_pred[:,:,1],\n",
    "           angles='xy', zorder=3, scale=1, scale_units='xy', color='C1', width=0.004, label='posterior mean')\n",
    "           \n",
    "\n",
    "for i in range(zsamp.shape[0]):\n",
    "    plt.quiver(x_target[:,:,0], x_target[:,:,1], zsamp[i,:,:,0], zsamp[i,:,:,1],\n",
    "               angles='xy', zorder=2.5, scale=1, scale_units='xy', color='C2', width=0.003, alpha=0.5,\n",
    "               label='posterior samples' if i==0 else None)\n",
    "           \n",
    "plt.hlines(x_target[:,0,1], x_target[0,0,0], x_target[0,-1,0], color='0.7', lw=0.5, zorder=0.5)\n",
    "plt.vlines(x_target[0,:,0], x_target[-1,0,1], x_target[0,0,1], color='0.7', lw=0.5, zorder=0.5)\n",
    "    \n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend()\n",
    "plt.xlim(-5,25)\n",
    "plt.ylim(-4,14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca67ec-924a-4bf6-8134-4023cfafd86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
