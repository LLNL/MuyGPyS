{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2023-2023 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shear Kernel 2x3 Investigation\n",
    "\n",
    "This notebook demonstrates how to use the specialized lensing shear kernel (hard-coded to RBF at the moment).\n",
    "In particular, this notebook investigates differences between the 2x3 kernel and the 3x3 variant in predictions of the $\\kappa$ convergence parameter, which appears to have an additive offset that we do not yet understand.\n",
    "\n",
    "⚠️ _Note that this is still an experimental feature._ ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LogNorm, SymLogNorm\n",
    "\n",
    "from MuyGPyS._test.shear import (\n",
    "    conventional_Kout,\n",
    "    conventional_mean,\n",
    "    conventional_variance,\n",
    "    conventional_shear,\n",
    "    targets_from_GP,\n",
    ")\n",
    "from MuyGPyS.gp import MuyGPS\n",
    "from MuyGPyS.gp.deformation import DifferenceIsotropy, F2\n",
    "from MuyGPyS.gp.hyperparameter import Parameter\n",
    "from MuyGPyS.gp.kernels.experimental import ShearKernel, ShearKernel2in3out\n",
    "from MuyGPyS.neighbors import NN_Wrapper\n",
    "from MuyGPyS.gp.noise import HomoscedasticNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fix a random seed.\n",
    "Try adjusting the value to see a different sampled realization (and therefore a different \"offset\" in the posterior mean error for the $\\kappa$ prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cmap = copy.copy(cm.get_cmap('viridis'))\n",
    "my_cmap.set_bad(\"white\")\n",
    "# my_sym_cmap = copy.copy(cm.get_cmap('coolwarm'))\n",
    "# my_sym_cmap.set_bad((0, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Here we simulate some simple data from a GP prior using the 3x3 shear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25  # number of galaxies on a side\n",
    "xmin = 0\n",
    "xmax = 1\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "\n",
    "xx = np.linspace(xmin, xmax, n)\n",
    "yy = np.linspace(ymin, ymax, n)\n",
    "\n",
    "x, y = np.meshgrid(xx, yy)\n",
    "features = np.vstack((x.flatten(), y.flatten())).T\n",
    "data_count = features.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the noise prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_prior = 1e-4\n",
    "length_scale = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the target matrices by sampling from the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets_from_GP(features, n, length_scale, noise_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a train/test split in the dataset.\n",
    "Modify the `train_ratio` to specify the proportion of data to hold out for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=1)\n",
    "interval_count = int(data_count * train_ratio)\n",
    "interval = int(data_count / interval_count)\n",
    "sfl = rng.permutation(np.arange(data_count))\n",
    "train_mask = np.zeros(data_count, dtype=bool)\n",
    "for i in range(interval_count):\n",
    "    idx = np.random.choice(sfl[i * interval : (i + 1) * interval])\n",
    "    train_mask[idx] = True\n",
    "test_mask = np.invert(train_mask)\n",
    "train_count = np.count_nonzero(train_mask)\n",
    "test_count = np.count_nonzero(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = targets[train_mask, :]\n",
    "test_targets = targets[test_mask, :]\n",
    "train_features = features[train_mask, :]\n",
    "test_features = features[test_mask, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the train/test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_im(vec, mask):\n",
    "    ret = np.zeros(len(mask))\n",
    "    ret[mask] = vec\n",
    "    ret[np.invert(mask)] = -np.inf\n",
    "    return ret.reshape(n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3,figsize = (10,7))\n",
    "ax[0, 0].imshow(make_im(train_targets[:,0], train_mask))\n",
    "ax[0, 0].set_ylabel(\"train\", fontsize = 15)\n",
    "ax[0, 0].set_title(\"$\\kappa$\", fontsize = 15)\n",
    "ax[1, 0].imshow(make_im(test_targets[:,0], test_mask))\n",
    "ax[1, 0].set_ylabel(\"test\", fontsize = 15)\n",
    "ax[0, 1].imshow(make_im(train_targets[:,1], train_mask))\n",
    "ax[0, 1].set_title(\"g1\", fontsize = 15)\n",
    "ax[1, 1].imshow(make_im(test_targets[:,1], test_mask))\n",
    "ax[0, 2].imshow(make_im(train_targets[:,2], train_mask))\n",
    "ax[0, 2].set_title(\"g2\", fontsize = 15)\n",
    "ax[1, 2].imshow(make_im(test_targets[:,2], test_mask))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3x3 Matrices\n",
    "\n",
    "Explicitly define the target matrices.\n",
    "Here assume that we are composing block matrices from component parts relating to the covariates $\\kappa$, $\\gamma_1$, and $\\gamma_2$.\n",
    "We assume that we have been given coordinates `X \\in \\mathbb{R}^{n \\times 2}` and responses `y_3(X) \\in \\mathbb{R}^{3n}`.\n",
    "We define $y_3(\\cdot)$ to be the result of concatenating the covariate responses $y_\\kappa(\\cdot)$, $y_{\\gamma_1}(\\cdot)$, and $y_{\\gamma_2}(\\cdot)$.\n",
    "To construct the 3x3 posterior mean for new observations $Z \\in \\mathbb{R}^{m \\times 2}$, we construct the following system:\n",
    "\n",
    "\\begin{align}\n",
    "\\widehat{y}_3(Z | X, y_3(X)) &= K_{3,3}(Z, X) K_{3,3}(X, X)^{-1} y_3(X), \\textrm{ where}\\\\\n",
    "K_{3,3}(X, X) \n",
    "  &= \\begin{pmatrix} \n",
    "    K_{\\kappa, \\kappa}(X, X) & K_{\\kappa, \\gamma_1}(X, X) & K_{\\kappa, \\gamma_2}(X, X) \\\\\n",
    "    K_{\\gamma_1, \\kappa}(X, X) & K_{\\gamma_1, \\gamma_1}(X, X) & K_{\\gamma_1, \\gamma_2}(X, X) \\\\\n",
    "    K_{\\gamma_2, \\kappa}(X, X) & K_{\\gamma_2, \\gamma_1}(X, X) & K_{\\gamma_2, \\gamma_2}(X, X) \\\\\n",
    "  \\end{pmatrix}, \\\\\n",
    "K_{3,3}(Z, X) \n",
    "  &= \\begin{pmatrix} \n",
    "    K_{\\kappa, \\kappa}(Z, X) & K_{\\kappa, \\gamma_1}(Z, X) & K_{\\kappa, \\gamma_2}(Z, X) \\\\\n",
    "    K_{\\gamma_1, \\kappa}(Z, X) & K_{\\gamma_1, \\gamma_1}(Z, X) & K_{\\gamma_1, \\gamma_2}(Z, X) \\\\\n",
    "    K_{\\gamma_2, \\kappa}(Z, X) & K_{\\gamma_2, \\gamma_1}(Z, X) & K_{\\gamma_2, \\gamma_2}(Z, X) \\\\\n",
    "  \\end{pmatrix}, \\textrm{ and} \\\\\n",
    "y_3(X) \n",
    "  &= \\begin{pmatrix} \n",
    "    y_\\kappa(X) \\\\\n",
    "    y_{\\gamma_1}(X) \\\\\n",
    "    y_{\\gamma_2}(X) \\\\\n",
    "  \\end{pmatrix}. \\\\\n",
    "\\end{align}\n",
    "\n",
    "Here, $K_{\\alpha, \\beta}(A, B)$ is the matrix of covariances between covariates $\\alpha$ and $\\beta$ between all pairwise combinations of the points in $A$ and $B$.\n",
    "In particular, $K_{\\alpha, \\beta}(X, X) \\in \\mathbb{R}^{n \\times n}$ and $K_{\\alpha, \\beta}(Z, X) \\in \\mathbb{R}^{m \\times n}$ for all $\\alpha, \\beta$.\n",
    "\n",
    "In the code, we call `Kin_33` = $K_{3,3}(X, X)$, `Kcross_33` = $K_{3,3}(Z, X)$, and `train_targets_33` = $y_3(X)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets_33 = train_targets.swapaxes(0, 1).reshape(3 * train_count)\n",
    "test_targets_33 = test_targets.swapaxes(0, 1).reshape(3 * test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need this model to find the Kout form from its kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shear_model = MuyGPS(\n",
    "        kernel=ShearKernel(\n",
    "            deformation=DifferenceIsotropy(\n",
    "                F2,\n",
    "                length_scale=Parameter(length_scale),\n",
    "            ),\n",
    "        ),\n",
    "        noise = HomoscedasticNoise(1e-4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realize the 3x3 kernel matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_33 = conventional_shear(train_features, train_features, length_scale=length_scale)\n",
    "Kcross_33 = conventional_shear(test_features, train_features, length_scale=length_scale)\n",
    "Kout_33 = conventional_Kout(shear_model.kernel, test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shapes of 3x3 matrices:\")\n",
    "print(f\"\\tKout: {Kout_33.shape}\")\n",
    "print(f\"\\tKcross: {Kcross_33.shape}\")\n",
    "print(f\"\\tKin: {Kin_33.shape}\")\n",
    "print(f\"\\ttrain targets: {train_targets_33.shape}\")\n",
    "print(f\"\\ttest targets: {test_targets_33.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_33 = conventional_mean(\n",
    "    Kin_33, Kcross_33, train_targets_33, noise_prior\n",
    ")\n",
    "covariance_33 = conventional_variance(\n",
    "    Kin_33, Kcross_33, Kout_33, noise_prior\n",
    ")\n",
    "diag_variance_33 = np.diag(covariance_33)\n",
    "ci_analytic_33 = np.sqrt(diag_variance_33) * 1.96\n",
    "ci_analytic_33 = ci_analytic_33.reshape(test_count, 3)\n",
    "coverage_analytic_33 = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_targets - mean_33) < ci_analytic_33, axis=0\n",
    "    ) / test_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2x3 Matrices\n",
    "\n",
    "Here we explore the 2in3out variant of the shear kernel, which trains on observations only of $gamma_1$ and $gamma_2, but predicts onto all three covariates.\n",
    "The 2x3 model uses training targets `y_2(X) \\in \\mathbb{R}^{2n}`.\n",
    "We define $y_2(\\cdot)$ to be the result of concatenating $y_{\\gamma_1}(\\cdot)$ and $y_{\\gamma_2}(\\cdot)$.\n",
    "The 2x3 posterior mean is then instead computed as\n",
    "\n",
    "\\begin{align}\n",
    "\\widehat{y}_2(Z | X, y_2(X)) &= K_{2,3}(Z, X) K_{2,3}(X, X)^{-1} y_2(X), \\textrm{ where}\\\\\n",
    "K_{2,3}(X, X) \n",
    "  &= \\begin{pmatrix} \n",
    "    K_{\\gamma_1, \\gamma_1}(X, X) & K_{\\gamma_1, \\gamma_2}(X, X) \\\\\n",
    "    K_{\\gamma_2, \\gamma_1}(X, X) & K_{\\gamma_2, \\gamma_2}(X, X) \\\\\n",
    "  \\end{pmatrix}, \\\\\n",
    "K_{2,3}(Z, X) \n",
    "  &= \\begin{pmatrix} \n",
    "    K_{\\kappa, \\gamma_1}(Z, X) & K_{\\kappa, \\gamma_2}(Z, X) \\\\\n",
    "    K_{\\gamma_1, \\gamma_1}(Z, X) & K_{\\gamma_1, \\gamma_2}(Z, X) \\\\\n",
    "    K_{\\gamma_2, \\gamma_1}(Z, X) & K_{\\gamma_2, \\gamma_2}(Z, X) \\\\\n",
    "  \\end{pmatrix}, \\textrm{ and} \\\\\n",
    "y_2(X) \n",
    "  &= \\begin{pmatrix} \n",
    "    y_{\\gamma_1}(X) \\\\\n",
    "    y_{\\gamma_2}(X) \\\\\n",
    "  \\end{pmatrix}. \\\\\n",
    "\\end{align}\n",
    "\n",
    "$K_{2,3}(X,X)$ is obtained by deleting the first $n$ rows and columns of $K_{3, 3}(X, X)$.\n",
    "Similarly, $K_{2, 3}(Z, X)$ is obtained by deleting the first $n$ columns of $K_{3, 3}(X, X)$.\n",
    "$y_2(X)$ is obtained by deleting the first $n$ elements of $y_3(X)$.\n",
    "\n",
    "Note that $K_{2,3}(X, X)$ and $y_2(X)$ are defined only in terms of $\\gamma_1$ and $\\gamma_2$.\n",
    "The block of the first $m$ rows of $K_{2, 3}(Z, X)$ are the only part of the equation that depend on $\\kappa$.\n",
    "\n",
    "In the code, we call `Kin_23` = $K_{2,3}(X, X)$, `Kcross_23` = $K_{2,3}(Z, X)$, and `train_targets_23` = $y_2(X)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we delete the relevant columns of the 3x3 kernel matrices to produce the 2x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_23 = Kin_33[train_count:, train_count:]\n",
    "Kcross_23 = Kcross_33[:, train_count:]\n",
    "train_targets_23 = train_targets_33[train_count:] \n",
    "test_targets_23 = test_targets_33[test_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shapes of 2x3 matrices:\")\n",
    "print(f\"\\tKout: {Kout_33.shape}\")  # we still use the 3x3 Kout prior, since we are predicting a 3-dimensional response\n",
    "print(f\"\\tKcross: {Kcross_23.shape}\")\n",
    "print(f\"\\tKin: {Kin_23.shape}\")\n",
    "print(f\"\\ttrain targets: {train_targets_23.shape}\")\n",
    "print(f\"\\ttest targets: {test_targets_23.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_23 = conventional_mean(\n",
    "    Kin_23, Kcross_23, train_targets_23, noise_prior\n",
    ")\n",
    "covariance_23 = conventional_variance(\n",
    "    Kin_23, Kcross_23, Kout_33, noise_prior\n",
    ")\n",
    "diag_variance_23 = np.diag(covariance_23)\n",
    "ci_23 = np.sqrt(diag_variance_23) * 1.96\n",
    "ci_23 = ci_23.reshape(test_count, 3)\n",
    "coverage_23 = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_targets - mean_23) < ci_23, axis=0\n",
    "    ) / test_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    mean_33.shape, covariance_33.shape, diag_variance_33.shape\n",
    ")\n",
    "print(\n",
    "    mean_23.shape, covariance_23.shape, diag_variance_23.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the mean comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_im(vec, mask, ax):\n",
    "    mat = make_im(vec, mask)\n",
    "    im = ax.imshow(mat.reshape(n, n), norm=LogNorm(), cmap=my_cmap)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "def compare_means(truth, first, second, fname, sname, fontsize=12, all_colorbar=False):\n",
    "    f_residual = np.abs(truth - first) + 1e-15\n",
    "    s_residual = np.abs(truth - second) + 1e-15\n",
    "    fs_residual = np.abs(first - second) + 1e-15\n",
    "\n",
    "    fig, ax = plt.subplots(6, 3, figsize = (10, 18))\n",
    "    \n",
    "    for axis_set in ax:\n",
    "        for axis in axis_set:\n",
    "            axis.set_xticks([])\n",
    "            axis.set_yticks([])\n",
    "\n",
    "    ax[0, 0].set_title(\"$\\kappa$\")\n",
    "    ax[0, 1].set_title(\"g1\")\n",
    "    ax[0, 2].set_title(\"g2\")\n",
    "    ax[0, 0].set_ylabel(\"Truth\", fontsize=fontsize)\n",
    "    ax[1, 0].set_ylabel(f\"{fname} Mean\", fontsize=fontsize)\n",
    "    ax[2, 0].set_ylabel(f\"|truth - {fname}|\", fontsize=fontsize)\n",
    "    ax[3, 0].set_ylabel(f\"{sname} Mean\", fontsize=fontsize)\n",
    "    ax[4, 0].set_ylabel(f\"|truth - {sname}|\", fontsize=fontsize)\n",
    "    ax[5, 0].set_ylabel(f\"|{fname} - {sname}|\", fontsize=fontsize)\n",
    "\n",
    "    # truth\n",
    "    im00 = ax[0, 0].imshow(make_im(truth[:,0], test_mask))\n",
    "    im01 = ax[0, 1].imshow(make_im(truth[:,1], test_mask))\n",
    "    im02 = ax[0, 2].imshow(make_im(truth[:,2], test_mask))\n",
    "    if all_colorbar is True:\n",
    "        fig.colorbar(im00, ax=ax[0, 0])\n",
    "        fig.colorbar(im01, ax=ax[0, 1])\n",
    "        fig.colorbar(im02, ax=ax[0, 2])\n",
    "\n",
    "    # first model\n",
    "    im10 = ax[1, 0].imshow(make_im(first[:,0], test_mask))\n",
    "    im11 = ax[1, 1].imshow(make_im(first[:,1], test_mask))\n",
    "    im12 = ax[1, 2].imshow(make_im(first[:,2], test_mask))\n",
    "    if all_colorbar is True:\n",
    "        fig.colorbar(im10, ax=ax[1, 0])\n",
    "        fig.colorbar(im11, ax=ax[1, 1])\n",
    "        fig.colorbar(im12, ax=ax[1, 2])\n",
    "\n",
    "    # first model residual\n",
    "    show_im(f_residual[:,0], test_mask, ax=ax[2, 0])\n",
    "    show_im(f_residual[:,1], test_mask, ax=ax[2, 1])\n",
    "    show_im(f_residual[:,2], test_mask, ax=ax[2, 2])\n",
    "\n",
    "    # second model\n",
    "    im30 = ax[3, 0].imshow(make_im(second[:,0], test_mask))\n",
    "    im31 = ax[3, 1].imshow(make_im(second[:,1], test_mask))\n",
    "    im32 = ax[3, 2].imshow(make_im(second[:,2], test_mask))\n",
    "    if all_colorbar is True:\n",
    "        fig.colorbar(im30, ax=ax[3, 0])\n",
    "        fig.colorbar(im31, ax=ax[3, 1])\n",
    "        fig.colorbar(im32, ax=ax[3, 2])\n",
    "\n",
    "    # second model residual\n",
    "    show_im(s_residual[:, 0], test_mask, ax=ax[4, 0])\n",
    "    show_im(s_residual[:, 1], test_mask, ax=ax[4, 1])\n",
    "    show_im(s_residual[:, 2], test_mask, ax=ax[4, 2])\n",
    "\n",
    "    # residual between the two models\n",
    "    show_im(fs_residual[:, 0], test_mask, ax=ax[5, 0])\n",
    "    show_im(fs_residual[:, 1], test_mask, ax=ax[5, 1])\n",
    "    show_im(fs_residual[:, 2], test_mask, ax=ax[5, 2])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_means(test_targets, mean_23, mean_33, \"2x3 Model\", \"3x3 Model\", all_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 2x3 residual appears to be nearly constant.\n",
    "We are hypothesizing that there is an additive offset in the posterior mean results returned by the 2x3 solution in the $\\kappa$ response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = np.mean(mean_23, axis=0) - np.mean(mean_33, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare the 2x3 posterior mean after subtracting this offset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_means(test_targets, mean_23 - offset, mean_33, \"2x3 Model\", \"3x3 Model\", all_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the corrected 2x3 mean in the $\\kappa$ response is still further off than the 3x3 mean, but the residual is no longer nearly constant as the majority of the residual appears to be accounted for in this scalar offset term.\n",
    "Of course, the way I've set this offset is synthetic and requires access to $\\kappa$ targets in the training data ($y_\\kappa(X)$).\n",
    "How do we deal with this in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, different random samples appear to produce different offsets (try rerunning with a different random seed).\n",
    "Is there perhaps no closed form expression of this offset?\n",
    "Is there a way to disciplined way to deal with this?\n",
    "Do we have enough information about $\\kappa$ to do anything?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
