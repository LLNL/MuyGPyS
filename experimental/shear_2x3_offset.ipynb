{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2023-2023 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shear Kernel 2x3 Investigation\n",
    "\n",
    "This notebook demonstrates how to use the specialized lensing shear kernel (hard-coded to RBF at the moment).\n",
    "In particular, this notebook investigates differences between the 2x3 kernel and the 3x3 variant in predictions of the $\\kappa$ convergence parameter, which appears to have an additive offset that we do not yet understand.\n",
    "\n",
    "⚠️ _Note that this is still an experimental feature._ ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LogNorm, SymLogNorm\n",
    "\n",
    "from MuyGPyS._test.shear import (\n",
    "    conventional_Kout,\n",
    "    conventional_mean,\n",
    "    conventional_variance,\n",
    "    conventional_shear,\n",
    "    targets_from_GP,\n",
    ")\n",
    "from MuyGPyS.gp import MuyGPS\n",
    "from MuyGPyS.gp.deformation import DifferenceIsotropy, F2\n",
    "from MuyGPyS.gp.hyperparameter import Parameter\n",
    "from MuyGPyS.gp.kernels.experimental import ShearKernel, ShearKernel2in3out\n",
    "from MuyGPyS.neighbors import NN_Wrapper\n",
    "from MuyGPyS.gp.noise import HomoscedasticNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fix a random seed.\n",
    "Try adjusting the value to see a different sampled realization (and therefore a different \"offset\" in the posterior mean error for the $\\kappa$ prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cmap = copy.copy(cm.get_cmap('viridis'))\n",
    "my_cmap.set_bad(\"white\")\n",
    "# my_sym_cmap = copy.copy(cm.get_cmap('coolwarm'))\n",
    "# my_sym_cmap.set_bad((0, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Here we simulate some simple data from a GP prior using the 3x3 shear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25  # number of galaxies on a side\n",
    "xmin = 0\n",
    "xmax = 1\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "\n",
    "xx = np.linspace(xmin, xmax, n)\n",
    "yy = np.linspace(ymin, ymax, n)\n",
    "\n",
    "x, y = np.meshgrid(xx, yy)\n",
    "features = np.vstack((x.flatten(), y.flatten())).T\n",
    "data_count = features.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the noise prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_prior = 1e-9\n",
    "length_scale = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the target matrices by sampling from the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets_from_GP(features, n, length_scale, noise_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a train/test split in the dataset.\n",
    "Modify the `train_ratio` to specify the proportion of data to hold out for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=1)\n",
    "interval_count = int(data_count * train_ratio)\n",
    "interval = int(data_count / interval_count)\n",
    "sfl = rng.permutation(np.arange(data_count))\n",
    "train_mask = np.zeros(data_count, dtype=bool)\n",
    "for i in range(interval_count):\n",
    "    idx = np.random.choice(sfl[i * interval : (i + 1) * interval])\n",
    "    train_mask[idx] = True\n",
    "test_mask = np.invert(train_mask)\n",
    "train_count = np.count_nonzero(train_mask)\n",
    "test_count = np.count_nonzero(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = targets[train_mask, :]\n",
    "test_targets = targets[test_mask, :]\n",
    "train_features = features[train_mask, :]\n",
    "test_features = features[test_mask, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the train/test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_im(vec, mask):\n",
    "    ret = np.zeros(len(mask))\n",
    "    ret[mask] = vec\n",
    "    ret[np.invert(mask)] = -np.inf\n",
    "    return ret.reshape(n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3,figsize = (10,7))\n",
    "ax[0, 0].imshow(make_im(train_targets[:,0], train_mask))\n",
    "ax[0, 0].set_ylabel(\"train\", fontsize = 15)\n",
    "ax[0, 0].set_title(\"$\\kappa$\", fontsize = 15)\n",
    "ax[1, 0].imshow(make_im(test_targets[:,0], test_mask))\n",
    "ax[1, 0].set_ylabel(\"test\", fontsize = 15)\n",
    "ax[0, 1].imshow(make_im(train_targets[:,1], train_mask))\n",
    "ax[0, 1].set_title(\"g1\", fontsize = 15)\n",
    "ax[1, 1].imshow(make_im(test_targets[:,1], test_mask))\n",
    "ax[0, 2].imshow(make_im(train_targets[:,2], train_mask))\n",
    "ax[0, 2].set_title(\"g2\", fontsize = 15)\n",
    "ax[1, 2].imshow(make_im(test_targets[:,2], test_mask))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3x3 Matrices\n",
    "\n",
    "Explicitly define the target matrices.\n",
    "Here assume that we are composing block matrices from component parts relating to the covariates $\\kappa$, $\\gamma_1$, and $\\gamma_2$.\n",
    "We assume that we have been given coordinates `X \\in \\mathbb{R}^{n \\times 2}` and responses `y_3(X) \\in \\mathbb{R}^{3n}`.\n",
    "We define $y_3(\\cdot)$ to be the result of concatenating the covariate responses $y_\\kappa(\\cdot)$, $y_{\\gamma_1}(\\cdot)$, and $y_{\\gamma_2}(\\cdot)$.\n",
    "To construct the 3x3 posterior mean for new observations $Z \\in \\mathbb{R}^{m \\times 2}$, we construct the following system:\n",
    "\n",
    "\\begin{align}\n",
    "\\widehat{y}_3(Z | X, y_3(X)) &= K_{3,3}(Z, X) K_{3,3}(X, X)^{-1} y_3(X), \\textrm{ where}\\\\\n",
    "K_{3,3}(X, X) \n",
    "  &= \\begin{pmatrix} \n",
    "    K_{\\kappa, \\kappa}(X, X) & K_{\\kappa, \\gamma_1}(X, X) & K_{\\kappa, \\gamma_2}(X, X) \\\\\n",
    "    K_{\\gamma_1, \\kappa}(X, X) & K_{\\gamma_1, \\gamma_1}(X, X) & K_{\\gamma_1, \\gamma_2}(X, X) \\\\\n",
    "    K_{\\gamma_2, \\kappa}(X, X) & K_{\\gamma_2, \\gamma_1}(X, X) & K_{\\gamma_2, \\gamma_2}(X, X) \\\\\n",
    "  \\end{pmatrix}, \\\\\n",
    "K_{3,3}(Z, X) \n",
    "  &= \\begin{pmatrix} \n",
    "    K_{\\kappa, \\kappa}(Z, X) & K_{\\kappa, \\gamma_1}(Z, X) & K_{\\kappa, \\gamma_2}(Z, X) \\\\\n",
    "    K_{\\gamma_1, \\kappa}(Z, X) & K_{\\gamma_1, \\gamma_1}(Z, X) & K_{\\gamma_1, \\gamma_2}(Z, X) \\\\\n",
    "    K_{\\gamma_2, \\kappa}(Z, X) & K_{\\gamma_2, \\gamma_1}(Z, X) & K_{\\gamma_2, \\gamma_2}(Z, X) \\\\\n",
    "  \\end{pmatrix}, \\textrm{ and} \\\\\n",
    "y_3(X) \n",
    "  &= \\begin{pmatrix} \n",
    "    y_\\kappa(X) \\\\\n",
    "    y_{\\gamma_1}(X) \\\\\n",
    "    y_{\\gamma_2}(X) \\\\\n",
    "  \\end{pmatrix}. \\\\\n",
    "\\end{align}\n",
    "\n",
    "Here, $K_{\\alpha, \\beta}(A, B)$ is the matrix of covariances between covariates $\\alpha$ and $\\beta$ between all pairwise combinations of the points in $A$ and $B$.\n",
    "In particular, $K_{\\alpha, \\beta}(X, X) \\in \\mathbb{R}^{n \\times n}$ and $K_{\\alpha, \\beta}(Z, X) \\in \\mathbb{R}^{m \\times n}$ for all $\\alpha, \\beta$.\n",
    "\n",
    "In the code, we call `Kin_33` = $K_{3,3}(X, X)$, `Kcross_33` = $K_{3,3}(Z, X)$, and `train_targets_33` = $y_3(X)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets_33 = train_targets.swapaxes(0, 1).reshape(3 * train_count)\n",
    "test_targets_33 = test_targets.swapaxes(0, 1).reshape(3 * test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need this model to find the Kout form from its kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shear_model = MuyGPS(\n",
    "        kernel=ShearKernel(\n",
    "            deformation=DifferenceIsotropy(\n",
    "                F2,\n",
    "                length_scale=Parameter(length_scale),\n",
    "            ),\n",
    "        ),\n",
    "        noise = HomoscedasticNoise(1e-4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realize the 3x3 kernel matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_33 = conventional_shear(train_features, train_features, length_scale=length_scale)\n",
    "Kcross_33 = conventional_shear(test_features, train_features, length_scale=length_scale)\n",
    "# Kout_33 = conventional_Kout(shear_model.kernel, test_count)\n",
    "Kout_33 = conventional_shear(test_features, test_features, length_scale=length_scale) + noise_prior * np.eye(3 * test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shapes of 3x3 matrices:\")\n",
    "print(f\"\\tKout: {Kout_33.shape}\")\n",
    "print(f\"\\tKcross: {Kcross_33.shape}\")\n",
    "print(f\"\\tKin: {Kin_33.shape}\")\n",
    "print(f\"\\ttrain targets: {train_targets_33.shape}\")\n",
    "print(f\"\\ttest targets: {test_targets_33.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_33 = conventional_mean(\n",
    "    Kin_33, Kcross_33, train_targets_33, noise_prior\n",
    ")\n",
    "covariance_33 = conventional_variance(\n",
    "    Kin_33, Kcross_33, Kout_33, noise_prior\n",
    ")\n",
    "diag_variance_33 = np.diag(covariance_33)\n",
    "ci_analytic_33 = np.sqrt(diag_variance_33) * 1.96\n",
    "ci_analytic_33 = ci_analytic_33.reshape(test_count, 3)\n",
    "coverage_analytic_33 = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_targets - mean_33) < ci_analytic_33, axis=0\n",
    "    ) / test_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2x3 Matrices\n",
    "\n",
    "Here we explore the 2in3out variant of the shear kernel, which trains on observations only of $\\gamma_1$ and $\\gamma_2$, but predicts onto all three covariates.\n",
    "The 2x3 model uses training targets `y_2(X) \\in \\mathbb{R}^{2n}`.\n",
    "We define $y_2(\\cdot)$ to be the result of concatenating $y_{\\gamma_1}(\\cdot)$ and $y_{\\gamma_2}(\\cdot)$.\n",
    "The 2x3 posterior mean is then instead computed as\n",
    "\n",
    "\\begin{align}\n",
    "\\widehat{y}_2(Z | X, y_2(X)) &= K_{2,3}(Z, X) K_{2,3}(X, X)^{-1} y_2(X), \\textrm{ where}\\\\\n",
    "K_{2,3}(X, X) \n",
    "  &= \\begin{pmatrix} \n",
    "    K_{\\gamma_1, \\gamma_1}(X, X) & K_{\\gamma_1, \\gamma_2}(X, X) \\\\\n",
    "    K_{\\gamma_2, \\gamma_1}(X, X) & K_{\\gamma_2, \\gamma_2}(X, X) \\\\\n",
    "  \\end{pmatrix}, \\\\\n",
    "K_{2,3}(Z, X) \n",
    "  &= \\begin{pmatrix} \n",
    "    K_{\\kappa, \\gamma_1}(Z, X) & K_{\\kappa, \\gamma_2}(Z, X) \\\\\n",
    "    K_{\\gamma_1, \\gamma_1}(Z, X) & K_{\\gamma_1, \\gamma_2}(Z, X) \\\\\n",
    "    K_{\\gamma_2, \\gamma_1}(Z, X) & K_{\\gamma_2, \\gamma_2}(Z, X) \\\\\n",
    "  \\end{pmatrix}, \\textrm{ and} \\\\\n",
    "y_2(X) \n",
    "  &= \\begin{pmatrix} \n",
    "    y_{\\gamma_1}(X) \\\\\n",
    "    y_{\\gamma_2}(X) \\\\\n",
    "  \\end{pmatrix}. \\\\\n",
    "\\end{align}\n",
    "\n",
    "$K_{2,3}(X,X)$ is obtained by deleting the first $n$ rows and columns of $K_{3, 3}(X, X)$.\n",
    "Similarly, $K_{2, 3}(Z, X)$ is obtained by deleting the first $n$ columns of $K_{3, 3}(X, X)$.\n",
    "$y_2(X)$ is obtained by deleting the first $n$ elements of $y_3(X)$.\n",
    "\n",
    "Note that $K_{2,3}(X, X)$ and $y_2(X)$ are defined only in terms of $\\gamma_1$ and $\\gamma_2$.\n",
    "The block of the first $m$ rows of $K_{2, 3}(Z, X)$ are the only part of the equation that depend on $\\kappa$.\n",
    "\n",
    "In the code, we call `Kin_23` = $K_{2,3}(X, X)$, `Kcross_23` = $K_{2,3}(Z, X)$, and `train_targets_23` = $y_2(X)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we delete the relevant columns of the 3x3 kernel matrices to produce the 2x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_23 = Kin_33[train_count:, train_count:]\n",
    "Kcross_23 = Kcross_33[:, train_count:]\n",
    "train_targets_23 = train_targets_33[train_count:] \n",
    "test_targets_23 = test_targets_33[test_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shapes of 2x3 matrices:\")\n",
    "print(f\"\\tKout: {Kout_33.shape}\")  # we still use the 3x3 Kout prior, since we are predicting a 3-dimensional response\n",
    "print(f\"\\tKcross: {Kcross_23.shape}\")\n",
    "print(f\"\\tKin: {Kin_23.shape}\")\n",
    "print(f\"\\ttrain targets: {train_targets_23.shape}\")\n",
    "print(f\"\\ttest targets: {test_targets_23.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_23 = conventional_mean(\n",
    "    Kin_23, Kcross_23, train_targets_23, noise_prior\n",
    ")\n",
    "covariance_23 = conventional_variance(\n",
    "    Kin_23, Kcross_23, Kout_33, noise_prior\n",
    ")\n",
    "diag_variance_23 = np.diag(covariance_23)\n",
    "ci_23 = np.sqrt(diag_variance_23) * 1.96\n",
    "ci_23 = ci_23.reshape(test_count, 3)\n",
    "coverage_23 = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_targets - mean_23) < ci_23, axis=0\n",
    "    ) / test_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    mean_33.shape, covariance_33.shape, diag_variance_33.shape\n",
    ")\n",
    "print(\n",
    "    mean_23.shape, covariance_23.shape, diag_variance_23.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the mean comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_im(vec, mask, ax):\n",
    "    mat = make_im(vec, mask)\n",
    "    im = ax.imshow(mat.reshape(n, n), norm=LogNorm(), cmap=my_cmap)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "def compare_means(truth, first, second, fname, sname, fontsize=12, all_colorbar=False):\n",
    "    f_residual = np.abs(truth - first) + 1e-15\n",
    "    s_residual = np.abs(truth - second) + 1e-15\n",
    "    fs_residual = np.abs(first - second) + 1e-15\n",
    "\n",
    "    fig, ax = plt.subplots(6, 3, figsize = (10, 18))\n",
    "    \n",
    "    for axis_set in ax:\n",
    "        for axis in axis_set:\n",
    "            axis.set_xticks([])\n",
    "            axis.set_yticks([])\n",
    "\n",
    "    ax[0, 0].set_title(\"$\\kappa$\")\n",
    "    ax[0, 1].set_title(\"g1\")\n",
    "    ax[0, 2].set_title(\"g2\")\n",
    "    ax[0, 0].set_ylabel(\"Truth\", fontsize=fontsize)\n",
    "    ax[1, 0].set_ylabel(f\"{fname} Mean\", fontsize=fontsize)\n",
    "    ax[2, 0].set_ylabel(f\"|truth - {fname}|\", fontsize=fontsize)\n",
    "    ax[3, 0].set_ylabel(f\"{sname} Mean\", fontsize=fontsize)\n",
    "    ax[4, 0].set_ylabel(f\"|truth - {sname}|\", fontsize=fontsize)\n",
    "    ax[5, 0].set_ylabel(f\"|{fname} - {sname}|\", fontsize=fontsize)\n",
    "\n",
    "    # truth\n",
    "    im00 = ax[0, 0].imshow(make_im(truth[:,0], test_mask))\n",
    "    im01 = ax[0, 1].imshow(make_im(truth[:,1], test_mask))\n",
    "    im02 = ax[0, 2].imshow(make_im(truth[:,2], test_mask))\n",
    "    if all_colorbar is True:\n",
    "        fig.colorbar(im00, ax=ax[0, 0])\n",
    "        fig.colorbar(im01, ax=ax[0, 1])\n",
    "        fig.colorbar(im02, ax=ax[0, 2])\n",
    "\n",
    "    # first model\n",
    "    im10 = ax[1, 0].imshow(make_im(first[:,0], test_mask))\n",
    "    im11 = ax[1, 1].imshow(make_im(first[:,1], test_mask))\n",
    "    im12 = ax[1, 2].imshow(make_im(first[:,2], test_mask))\n",
    "    if all_colorbar is True:\n",
    "        fig.colorbar(im10, ax=ax[1, 0])\n",
    "        fig.colorbar(im11, ax=ax[1, 1])\n",
    "        fig.colorbar(im12, ax=ax[1, 2])\n",
    "\n",
    "    # first model residual\n",
    "    show_im(f_residual[:,0], test_mask, ax=ax[2, 0])\n",
    "    show_im(f_residual[:,1], test_mask, ax=ax[2, 1])\n",
    "    show_im(f_residual[:,2], test_mask, ax=ax[2, 2])\n",
    "\n",
    "    # second model\n",
    "    im30 = ax[3, 0].imshow(make_im(second[:,0], test_mask))\n",
    "    im31 = ax[3, 1].imshow(make_im(second[:,1], test_mask))\n",
    "    im32 = ax[3, 2].imshow(make_im(second[:,2], test_mask))\n",
    "    if all_colorbar is True:\n",
    "        fig.colorbar(im30, ax=ax[3, 0])\n",
    "        fig.colorbar(im31, ax=ax[3, 1])\n",
    "        fig.colorbar(im32, ax=ax[3, 2])\n",
    "\n",
    "    # second model residual\n",
    "    show_im(s_residual[:, 0], test_mask, ax=ax[4, 0])\n",
    "    show_im(s_residual[:, 1], test_mask, ax=ax[4, 1])\n",
    "    show_im(s_residual[:, 2], test_mask, ax=ax[4, 2])\n",
    "\n",
    "    # residual between the two models\n",
    "    show_im(fs_residual[:, 0], test_mask, ax=ax[5, 0])\n",
    "    show_im(fs_residual[:, 1], test_mask, ax=ax[5, 1])\n",
    "    show_im(fs_residual[:, 2], test_mask, ax=ax[5, 2])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_means(test_targets, mean_23, mean_33, \"2x3 Model\", \"3x3 Model\", all_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's with the near-constant residual?\n",
    "\n",
    "Note that the 2x3 residual appears to be nearly constant.\n",
    "We hypothesize that, because there is no information about the $\\kappa$ response, the posterior predictions for $\\kappa$ are highly correlated but subject to noisy realization.\n",
    "\n",
    "First, we check that additively removing the mean trend yields much more reasonable predictions.\n",
    "Note that, of course, this is cheating and is not a viable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = np.mean(mean_23, axis=0) - np.mean(mean_33, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare the 2x3 posterior mean after subtracting this offset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_means(test_targets, mean_23 - offset, mean_33, \"2x3 Model\", \"3x3 Model\", all_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the corrected 2x3 mean in the $\\kappa$ response is still further off than the 3x3 mean, but the residual is no longer nearly constant as the majority of the residual appears to be accounted for in this scalar offset term.\n",
    "Of course, the way I've set this offset is synthetic and requires access to $\\kappa$ targets in the training data ($y_\\kappa(X)$).\n",
    "How do we deal with this in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, different random samples appear to produce different offsets (try rerunning with a different random seed).\n",
    "However, the general trend in the posterior mean always matches the data.\n",
    "This suggests that there is a noisy, data-dependent process with too many degrees of freedom in the pure 2x3 kernel that results in an arbitrary offset due to a lack of information about the true scale of $\\kappa$ in the training observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the posterior correlation\n",
    "\n",
    "We hypothesize that the posterior $\\kappa$ predictions are highly correlated.\n",
    "If true, then conditioning predictions on a small amount of the information in $y_\\kappa(X)$ should allow us to effectively remove this stochastic offset and \"snap\" the posterior trend onto the correct scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_var_33 = np.sqrt(diag_variance_33.reshape(1500, 1))\n",
    "sqrt_var_23 = np.sqrt(diag_variance_23.reshape(1500, 1))\n",
    "\n",
    "correlation_33 = covariance_33 / (sqrt_var_33 * sqrt_var_33.T)\n",
    "correlation_23 = covariance_23 / (sqrt_var_23 * sqrt_var_23.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3x3 diagonal variances:\")\n",
    "print(f\"\\tkappa mean variance    = {np.mean(diag_variance_33[:test_count])}\")\n",
    "print(f\"\\tgamma_1 mean variance  = {np.mean(diag_variance_33[test_count:2 * test_count])}\")\n",
    "print(f\"\\tgamma_2 mean variance  = {np.mean(diag_variance_33[2 * test_count:])}\")\n",
    "print()\n",
    "print(\"2x3 diagonal variances:\")\n",
    "print(f\"\\tkappa mean variance    = {np.mean(diag_variance_23[:test_count])}\")\n",
    "print(f\"\\tgamma_1 mean variance  = {np.mean(diag_variance_23[test_count:2 * test_count])}\")\n",
    "print(f\"\\tgamma_2 mean variance  = {np.mean(diag_variance_23[2 * test_count:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the model correctly notes that it is very unconfident about the 2x3 $\\kappa$ predictions.\n",
    "Now we plot the full posterior covariance for $\\kappa$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "im0 = axes[0].imshow(covariance_33)\n",
    "axes[0].set_title(\"3x3 covariance\")\n",
    "fig.colorbar(im0, ax=axes[0])\n",
    "\n",
    "im1 = axes[1].imshow(covariance_23)\n",
    "axes[1].set_title(\"2x3 covariance\")\n",
    "fig.colorbar(im1, ax=axes[1])\n",
    "\n",
    "im2 = axes[2].imshow(covariance_33 - covariance_23)\n",
    "axes[2].set_title(\"covariance residual\")\n",
    "fig.colorbar(im2, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the $\\kappa \\times \\kappa$ block of the 2x3 covariance is much larger than the background!\n",
    "We compute the correlation for confirmation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "im0 = axes[0].imshow(correlation_33)\n",
    "axes[0].set_title(\"3x3 correlation\")\n",
    "fig.colorbar(im0, ax=axes[0])\n",
    "\n",
    "im1 = axes[1].imshow(correlation_23)\n",
    "axes[1].set_title(\"2x3 correlation\")\n",
    "fig.colorbar(im1, ax=axes[1])\n",
    "\n",
    "im2 = axes[2].imshow(correlation_33 - correlation_23)\n",
    "axes[2].set_title(\"correlation residual\")\n",
    "fig.colorbar(im2, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the $\\kappa \\times \\kappa$ block of the 2x3 correlation is almost fixed at 1.\n",
    "This means that, should we condition on any of the information in $y_\\kappa(X)$, it should hopefully shift all of the posterior information to the \"correct offset\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with partial conditioning on $y_\\kappa(X)$\n",
    "\n",
    "Here we attempt to condition on a single $x \\in X$, and add the information in $y_\\kappa(x)$ to the solve.\n",
    "We call this new vector $y_{2 + 1}(X)$, and is the result of concatenating $y_\\kappa(x)$, $y_{\\gamma_1}(X)$, and $y_{\\gamma_2}(X)$.\n",
    "This creates the following system of equations:\n",
    "\n",
    "\\begin{align}\n",
    "\\widehat{y}_{2 + 1}(Z | X, y_{2 + 1}(X)) &= K_{2 + 1,3}(Z, X) K_{2 + 1,3}(X, X)^{-1} y_{2 + 1}(X), \\textrm{ where}\\\\\n",
    "K_{2,3}(X, X) \n",
    "  &= \\begin{pmatrix} \n",
    "    K_{\\kappa, \\kappa}(x, X)   & K_{\\kappa, \\gamma_1}(x, X)   & K_{\\kappa, \\gamma_2}(x, X) \\\\\n",
    "    K_{\\gamma_1, \\kappa}(x, X) & K_{\\gamma_1, \\gamma_1}(X, X) & K_{\\gamma_1, \\gamma_2}(X, X) \\\\\n",
    "    K_{\\gamma_2, \\kappa}(x, X) & K_{\\gamma_2, \\gamma_1}(X, X) & K_{\\gamma_2, \\gamma_2}(X, X) \\\\\n",
    "  \\end{pmatrix}, \\\\\n",
    "K_{2,3}(Z, X) \n",
    "  &= \\begin{pmatrix} \n",
    "    K_{\\kappa, \\kappa}(Z, x) & K_{\\kappa, \\gamma_1}(Z, X) & K_{\\kappa, \\gamma_2}(Z, X) \\\\\n",
    "    K_{\\kappa, \\gamma_1}(Z, x) & K_{\\gamma_1, \\gamma_1}(Z, X) & K_{\\gamma_1, \\gamma_2}(Z, X) \\\\\n",
    "    K_{\\kappa, \\gamma_2}(Z, x) & K_{\\gamma_2, \\gamma_1}(Z, X) & K_{\\gamma_2, \\gamma_2}(Z, X) \\\\\n",
    "  \\end{pmatrix}, \\textrm{ and} \\\\\n",
    "y_{2 + 1}(X) \n",
    "  &= \\begin{pmatrix} \n",
    "    y_\\kappa(x) \\\\\n",
    "    y_{\\gamma_1}(X) \\\\\n",
    "    y_{\\gamma_2}(X) \\\\\n",
    "  \\end{pmatrix}. \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_idx = int(train_count / 2)\n",
    "selection = np.hstack((np.zeros(train_count, dtype=bool), np.ones(train_count * 2, dtype=bool)))\n",
    "selection[x_idx] = True\n",
    "\n",
    "Kin_13 = Kin_33[np.ix_(selection, selection)]\n",
    "Kcross_13 = Kcross_33[:, selection]\n",
    "train_targets_13 = train_targets_33[selection] \n",
    "train_targets_13 = train_targets_33[selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shapes of 2+1x3 matrices:\")\n",
    "print(f\"\\tKout: {Kout_33.shape}\")  # we still use the 3x3 Kout prior, since we are predicting a 3-dimensional response\n",
    "print(f\"\\tKcross: {Kcross_13.shape}\")\n",
    "print(f\"\\tKin: {Kin_13.shape}\")\n",
    "print(f\"\\ttrain targets: {train_targets_13.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_13 = conventional_mean(\n",
    "    Kin_13, Kcross_13, train_targets_13, noise_prior\n",
    ")\n",
    "covariance_13 = conventional_variance(\n",
    "    Kin_13, Kcross_13, Kout_33, noise_prior\n",
    ")\n",
    "diag_variance_13 = np.diag(covariance_13)\n",
    "ci_13 = np.sqrt(diag_variance_13) * 1.96\n",
    "ci_13 = ci_13.reshape(test_count, 3)\n",
    "coverage_13 = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_targets - mean_13) < ci_13, axis=0\n",
    "    ) / test_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_13.shape, covariance_13.shape, diag_variance_13.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_means(test_targets, mean_13, mean_33, \"(2+1)x3 Model\", \"3x3 Model\", all_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat incredibly, it seems that conditioning on a single $\\kappa$ observation corrects the offset issue and achieves accurate predictions.\n",
    "Moreover, we can see that it also drastically reduces the posterior variance of the (2+1)x3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2x3 diagonal variances:\")\n",
    "print(f\"\\tkappa mean variance    = {np.mean(diag_variance_23[:test_count])}\")\n",
    "print(f\"\\tgamma_1 mean variance  = {np.mean(diag_variance_23[test_count:2 * test_count])}\")\n",
    "print(f\"\\tgamma_2 mean variance  = {np.mean(diag_variance_23[2 * test_count:])}\")\n",
    "print(\"(2+1)x3 diagonal variances:\")\n",
    "print(f\"\\tkappa mean variance    = {np.mean(diag_variance_13[:test_count])}\")\n",
    "print(f\"\\tgamma_1 mean variance  = {np.mean(diag_variance_13[test_count:2 * test_count])}\")\n",
    "print(f\"\\tgamma_2 mean variance  = {np.mean(diag_variance_13[2 * test_count:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "im0 = axes[0].imshow(covariance_33)\n",
    "axes[0].set_title(\"3x3 covariance\")\n",
    "fig.colorbar(im0, ax=axes[0])\n",
    "\n",
    "im1 = axes[1].imshow(covariance_23)\n",
    "axes[1].set_title(\"2x3 covariance\")\n",
    "fig.colorbar(im1, ax=axes[1])\n",
    "\n",
    "im2 = axes[2].imshow(covariance_13)\n",
    "axes[2].set_title(\"(2+1)x3 covariance\")\n",
    "fig.colorbar(im2, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Schur complement?\n",
    "\n",
    "⚠️ _This section is still under construction_ ⚠️\n",
    "\n",
    "Here we try to use the Schur complement to implicitly invert `Kin_23` = $K_{2, 3}(X, X)$ blockwise and hopefully avoid some numerical instability associated with high condition number matrices.\n",
    "\n",
    "We use the following identity for a symmetric, invertible blockwise matrix:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    "A & C^\\top \\\\\n",
    "C & D \\\\\n",
    "\\end{pmatrix}^{-1} &=\n",
    "\\begin{pmatrix}\n",
    "A^{-1} + A^{-1} C^\\top S^{-1} C A^{-1} & -A^{-1}C^\\top S^{-1} \\\\\n",
    "-S^{-1} CA^{-1} & S^{-1} \\\\\n",
    "\\end{pmatrix}, \\textrm{ where} \\\\\n",
    "S &= D - C A^{-1} C^\\top\n",
    "\\end{align}.\n",
    "\n",
    "Rewriting for $K_{2,3}(X, X)$ and dropping the $(X, X)$ arguments for legibility, this resolves to\n",
    "\n",
    "\\begin{align}\n",
    "K_{2, 3}^{-1} &=\n",
    "\\begin{pmatrix}\n",
    "K_{\\gamma_1, \\gamma_1} & K_{\\gamma_1, \\gamma_2} \\\\\n",
    "K_{\\gamma_2, \\gamma_1} & K_{\\gamma_2, \\gamma_2} \\\\\n",
    "\\end{pmatrix}^{-1} \\\\\n",
    "&=\n",
    "\\begin{pmatrix}\n",
    "K_{\\gamma_1, \\gamma_1}^{-1} + K_{\\gamma_1, \\gamma_1}^{-1} K_{\\gamma_1, \\gamma_2} S^{-1} K_{\\gamma_2, \\gamma_1} K_{\\gamma_1, \\gamma_1}^{-1} & -K_{\\gamma_1, \\gamma_1}^{-1} K_{\\gamma_1, \\gamma_2} S^{-1} \\\\\n",
    "-S^{-1} K_{\\gamma_2, \\gamma_1} K_{\\gamma_1, \\gamma_1}^{-1} & S^{-1} \\\\\n",
    "\\end{pmatrix}, \\textrm{ where} \\\\\n",
    "S &= K_{\\gamma_2, \\gamma_2} - K_{\\gamma_2, \\gamma_1} K_{\\gamma_1, \\gamma_1}^{-1} K_{\\gamma_1, \\gamma_2}\n",
    "\\end{align}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schur_solve(Kin, targets, train_count):\n",
    "    A = Kin[:train_count, :train_count]\n",
    "    C = Kin[train_count:, :train_count]\n",
    "    D = Kin[train_count:, train_count:]\n",
    "    \n",
    "    S = D - C @ np.linalg.solve(A, C.T)\n",
    "    \n",
    "    ret = np.zeros(targets.shape)\n",
    "    \n",
    "    y1 = targets[:train_count]\n",
    "    y2 = targets[train_count:]\n",
    "    \n",
    "    AinvY1 = np.linalg.solve(A, y1)\n",
    "    AinvCT = np.linalg.solve(A, C.T)\n",
    "    SinvC = np.linalg.solve(S, C)\n",
    "    SinvY2 = np.linalg.solve(S, y2)\n",
    "    \n",
    "    ret[:train_count] = AinvY1 + AinvCT @ SinvC @ AinvY1 - AinvCT @ SinvY2\n",
    "    ret[train_count:] = SinvY2 - SinvC @ AinvY1\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schur_mean(Kin, Kcross, targets, noise_prior):\n",
    "    train_count = Kin.shape[0]\n",
    "    Kinp = Kin + noise_prior * np.eye(train_count)\n",
    "    return Kcross @ schur_solve(Kinp, targets, train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_2s = schur_mean(\n",
    "    Kin_23, Kcross_23, train_targets_23, noise_prior\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_2s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_23.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
