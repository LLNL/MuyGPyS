{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2023-2023 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shear Kernel Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the specialized lensing shear kernel (hard-coded to RBF at the moment).\n",
    "\n",
    "⚠️ _Note that this is still an experimental feature._ ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LogNorm, SymLogNorm\n",
    "\n",
    "from MuyGPyS._test.shear import (\n",
    "    conventional_Kout,\n",
    "    conventional_mean,\n",
    "    conventional_variance,\n",
    "    conventional_shear,\n",
    "    targets_from_GP,\n",
    ")\n",
    "from MuyGPyS.gp import MuyGPS\n",
    "from MuyGPyS.gp.deformation import DifferenceIsotropy, F2\n",
    "from MuyGPyS.gp.hyperparameter import Parameter\n",
    "from MuyGPyS.gp.kernels.experimental import ShearKernel\n",
    "from MuyGPyS.neighbors import NN_Wrapper\n",
    "from MuyGPyS.gp.noise import HomoscedasticNoise, ShearNoise33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set a random seed here for consistency when building docs.\n",
    "In practice we would not fix a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Implementation Comparisons\n",
    "\n",
    "Here we will compare the analytic implementation of the kernel function to the `MuyGPyS` implementation, using some simple data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build some simple data, which is mean to represent a grid of sky coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25  # number of galaxies on a side\n",
    "xmin = 0\n",
    "xmax = 1\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "\n",
    "xx = np.linspace(xmin, xmax, n)\n",
    "yy = np.linspace(ymin, ymax, n)\n",
    "\n",
    "x, y = np.meshgrid(xx, yy)\n",
    "features = np.vstack((x.flatten(), y.flatten())).T\n",
    "data_count = features.shape[0]\n",
    "length_scale = 0.05\n",
    "shear_model = MuyGPS(\n",
    "        kernel=ShearKernel(\n",
    "            deformation=DifferenceIsotropy(\n",
    "                F2,\n",
    "                length_scale=Parameter(length_scale),\n",
    "            ),\n",
    "        ),\n",
    "        noise = ShearNoise33(1e-4),\n",
    ")\n",
    "diffs = shear_model.kernel.deformation.pairwise_tensor(features, np.arange(data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cmap = copy.copy(cm.get_cmap('viridis'))\n",
    "my_cmap.set_bad(\"white\")\n",
    "my_sym_cmap = copy.copy(cm.get_cmap('coolwarm'))\n",
    "my_sym_cmap.set_bad((0, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use an Isotropic distance functor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise kernels (`Kin`)\n",
    "This code computes the `Kin` kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_analytic = conventional_shear(features, length_scale=length_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the same using the MuyGPyS implementation. Note the increased efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_muygps = shear_model.kernel(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Kin_muygps` is a more generalized tensor, so we need to flatten it to a conforming shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_flat = Kin_muygps.reshape(data_count * 3, data_count * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Kin_analytic.shape = {Kin_analytic.shape}\")\n",
    "print(f\"Kin_muygps.shape = {Kin_muygps.shape}\")\n",
    "print(f\"Kin_flat.shape = {Kin_flat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the two implementations agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.allclose(Kin_analytic, Kin_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_residual = np.abs(Kin_analytic - Kin_flat)\n",
    "print(f\"Kin residual max: {np.max(Kin_residual)}, min: {np.min(Kin_residual)}, mean : {np.mean(Kin_residual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results of the baseline and MuyGPyS implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].set_title(\"original shear kernel\")\n",
    "axes[0].imshow(Kin_analytic)\n",
    "axes[1].set_title(\"MuyGPyS shear kernel\")\n",
    "axes[1].imshow(Kin_flat)\n",
    "axes[2].set_title(\"Residual\")\n",
    "im = axes[2].imshow(Kin_residual, norm=LogNorm(), cmap=my_cmap)\n",
    "fig.colorbar(im, ax=axes[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Covariance (`Kcross`)\n",
    "Now we perform a similar analysis for the cross-covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 200\n",
    "X1 = features[:split]\n",
    "X2 = features[split:]\n",
    "n1, _ = X1.shape\n",
    "n2, _ = X2.shape\n",
    "crosswise_diffs = shear_model.kernel.deformation.crosswise_tensor(\n",
    "    X1, X2, np.arange(n1), np.arange(n2)\n",
    ")\n",
    "print(X1.shape, X2.shape, crosswise_diffs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_analytic = conventional_shear(X1, X2, length_scale=length_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_muygps = shear_model.kernel(crosswise_diffs, adjust=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_flat = Kcross_muygps.reshape(n1 * 3, n2 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Kcross_analytic.shape = {Kcross_analytic.shape}\")\n",
    "print(f\"Kcross_muygps.shape = {Kcross_muygps.shape}\")\n",
    "print(f\"Kcross_flat.shape = {Kcross_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(Kcross_analytic, Kcross_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_residual = np.abs(Kcross_analytic - Kcross_flat)\n",
    "print(f\"Kcross residual max: {np.max(Kcross_residual)}, min: {np.min(Kcross_residual)}, mean : {np.mean(Kcross_residual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "axes[0].set_title(\"original shear kernel\")\n",
    "axes[0].imshow(Kcross_analytic)\n",
    "axes[1].set_title(\"MuyGPyS shear kernel\")\n",
    "axes[1].imshow(Kcross_flat)\n",
    "axes[2].set_title(\"Residual\")\n",
    "im = axes[2].imshow(Kcross_residual, norm=LogNorm(), cmap=my_cmap)\n",
    "fig.colorbar(im, ax=axes[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime comparison of the two implementations (Change `False` to `True` to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    %timeit conventional_shear(features)\n",
    "    %timeit ShearKernel(deformation=dist_fn)(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Comparisons\n",
    "\n",
    "Now we will test the `posterior_mean` and `posterior_variance` of the analytic and muygps implementations.\n",
    "We first simulate a dataset.\n",
    "Targets should be square matrices like a grid of a swath of sky.\n",
    "Ulitimately, the target array will have shape `(625,3)`, given `n=25` above.\n",
    "\n",
    "We will then evaluate the posterior means and variances of the analytic and `MuyGPyS` implementations.\n",
    "We will plot both, along with their residuals (compared against both each other and the ground truth for the mean). \n",
    "\n",
    "We compute the posterior mean as\n",
    "$$ \\hat{Y}(X^*|X) = K_{\\theta}(X^*,X)(K_{\\theta}(X,X)+\\epsilon)^{-1}Y(X) $$\n",
    "where $\\epsilon = \\sigma^2 I$ with $\\sigma^2$ being the noise variance.\n",
    "\n",
    "The posterior variance is defined as\n",
    "$$ \\mathrm{Var}(\\hat{Y}(X^*|X)) = K_{\\theta}(X^*,X^*) - K_{\\theta}(X^*,X)[K_{\\theta}(X,X) - \\epsilon]^{-1}K_{\\theta}(X,X^*) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_prior = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the target matrices.\n",
    "Initially was run with arbitrary targets, now as of 1/25/24 can sample targets from GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets_from_GP(features, n, length_scale, noise_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a train/test split in the dataset.\n",
    "Modify the `train_ratio` to specify the proportion of data to hold out for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=1)\n",
    "interval_count = int(data_count * train_ratio)\n",
    "interval = int(data_count / interval_count)\n",
    "sfl = rng.permutation(np.arange(data_count))\n",
    "train_mask = np.zeros(data_count, dtype=bool)\n",
    "for i in range(interval_count):\n",
    "    idx = np.random.choice(sfl[i * interval : (i + 1) * interval])\n",
    "    train_mask[idx] = True\n",
    "test_mask = np.invert(train_mask)\n",
    "train_count = np.count_nonzero(train_mask)\n",
    "test_count = np.count_nonzero(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = targets[train_mask, :]\n",
    "test_targets = targets[test_mask, :]\n",
    "train_features = features[train_mask, :]\n",
    "test_features = features[test_mask, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the train/test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_im(vec, mask):\n",
    "    ret = np.zeros(len(mask))\n",
    "    ret[mask] = vec\n",
    "    ret[np.invert(mask)] = -np.inf\n",
    "    return ret.reshape(n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3,figsize = (10,7))\n",
    "ax[0, 0].imshow(make_im(train_targets[:,0], train_mask))\n",
    "ax[0, 0].set_ylabel(\"train\", fontsize = 15)\n",
    "ax[0, 0].set_title(\"$\\kappa$\", fontsize = 15)\n",
    "ax[1, 0].imshow(make_im(test_targets[:,0], test_mask))\n",
    "ax[1, 0].set_ylabel(\"test\", fontsize = 15)\n",
    "ax[0, 1].imshow(make_im(train_targets[:,1], train_mask))\n",
    "ax[0, 1].set_title(\"g1\", fontsize = 15)\n",
    "ax[1, 1].imshow(make_im(test_targets[:,1], test_mask))\n",
    "ax[0, 2].imshow(make_im(train_targets[:,2], train_mask))\n",
    "ax[0, 2].set_title(\"g2\", fontsize = 15)\n",
    "ax[1, 2].imshow(make_im(test_targets[:,2], test_mask))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicitly define the target matrices.\n",
    "Also add leading unitary dimension to `targets_muygpys` for things to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets_flat = train_targets.swapaxes(0, 1).reshape(3 * train_count)\n",
    "test_targets_flat = test_targets.swapaxes(0, 1).reshape(3 * test_count)\n",
    "print(train_targets_flat.shape, test_targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytic: for the analytic implementation, I'll do things with the full \"flattened\" difference tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_analytic = conventional_shear(train_features, train_features, length_scale=length_scale)\n",
    "Kcross_analytic = conventional_shear(test_features, train_features, length_scale=length_scale)\n",
    "Kout_analytic = conventional_Kout(shear_model.kernel, test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Kcross_analytic.shape, Kin_analytic.shape, Kout_analytic.shape, train_targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mean_analytic = conventional_mean(\n",
    "    Kin_analytic,\n",
    "    Kcross_analytic,\n",
    "    train_targets_flat,\n",
    "    noise_prior,\n",
    ")\n",
    "posterior_variance_analytic = conventional_variance(\n",
    "    Kin_analytic, \n",
    "    Kcross_analytic,\n",
    "    Kout_analytic,\n",
    "    noise_prior,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posterior_mean_analytic.shape, posterior_variance_analytic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create flat solve using MuyGPyS functions.\n",
    "This should be very close to the analytic solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_diffs = shear_model.kernel.deformation.pairwise_tensor(\n",
    "    train_features, np.arange(train_count)\n",
    ")\n",
    "crosswise_diffs = shear_model.kernel.deformation.crosswise_tensor(\n",
    "    test_features, train_features, np.arange(test_count), np.arange(train_count)\n",
    ")\n",
    "Kin_muygps = shear_model.kernel(pairwise_diffs, adjust=False)\n",
    "Kcross_muygps = shear_model.kernel(crosswise_diffs, adjust=False)\n",
    "Kin_flat = Kin_muygps.reshape(3 * train_count, 3 * train_count)\n",
    "Kcross_flat = Kcross_muygps.reshape(3 * test_count, 3 * train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Kin_muygps.shape, Kcross_muygps.shape, Kin_flat.shape, Kcross_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the flattened kernel tensors agree with the analytic tensors (should pass if the above passed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.allclose(Kin_analytic, Kin_flat),\n",
    "    np.allclose(Kcross_analytic, Kcross_flat),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_im(vec, mask, ax):\n",
    "    mat = make_im(vec, mask)\n",
    "    im = ax.imshow(mat.reshape(n, n), norm=LogNorm(), cmap=my_cmap)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "def compare_means(truth, first, second, fname, sname, fontsize=12, all_colorbar=False):\n",
    "    f_residual = np.abs(truth - first) + 1e-15\n",
    "    s_residual = np.abs(truth - second) + 1e-15\n",
    "    fs_residual = np.abs(first - second) + 1e-15\n",
    "\n",
    "    fig, ax = plt.subplots(6, 3, figsize = (10, 18))\n",
    "    \n",
    "    for axis_set in ax:\n",
    "        for axis in axis_set:\n",
    "            axis.set_xticks([])\n",
    "            axis.set_yticks([])\n",
    "\n",
    "    ax[0, 0].set_title(\"$\\kappa$\")\n",
    "    ax[0, 1].set_title(\"g1\")\n",
    "    ax[0, 2].set_title(\"g2\")\n",
    "    ax[0, 0].set_ylabel(\"Truth\", fontsize=fontsize)\n",
    "    ax[1, 0].set_ylabel(f\"{fname} Mean\", fontsize=fontsize)\n",
    "    ax[2, 0].set_ylabel(f\"|truth - {fname}|\", fontsize=fontsize)\n",
    "    ax[3, 0].set_ylabel(f\"{sname} Mean\", fontsize=fontsize)\n",
    "    ax[4, 0].set_ylabel(f\"|truth - {sname}|\", fontsize=fontsize)\n",
    "    ax[5, 0].set_ylabel(f\"|{fname} - {sname}|\", fontsize=fontsize)\n",
    "\n",
    "    # truth\n",
    "    im00 = ax[0, 0].imshow(make_im(truth[:,0], test_mask))\n",
    "    im01 = ax[0, 1].imshow(make_im(truth[:,1], test_mask))\n",
    "    im02 = ax[0, 2].imshow(make_im(truth[:,2], test_mask))\n",
    "    if all_colorbar is True:\n",
    "        fig.colorbar(im00, ax=ax[0, 0])\n",
    "        fig.colorbar(im01, ax=ax[0, 1])\n",
    "        fig.colorbar(im02, ax=ax[0, 2])\n",
    "\n",
    "    # first model\n",
    "    im10 = ax[1, 0].imshow(make_im(first[:,0], test_mask))\n",
    "    im11 = ax[1, 1].imshow(make_im(first[:,1], test_mask))\n",
    "    im12 = ax[1, 2].imshow(make_im(first[:,2], test_mask))\n",
    "    if all_colorbar is True:\n",
    "        fig.colorbar(im10, ax=ax[1, 0])\n",
    "        fig.colorbar(im11, ax=ax[1, 1])\n",
    "        fig.colorbar(im12, ax=ax[1, 2])\n",
    "\n",
    "    # first model residual\n",
    "    show_im(f_residual[:,0], test_mask, ax=ax[2, 0])\n",
    "    show_im(f_residual[:,1], test_mask, ax=ax[2, 1])\n",
    "    show_im(f_residual[:,2], test_mask, ax=ax[2, 2])\n",
    "\n",
    "    # second model\n",
    "    im30 = ax[3, 0].imshow(make_im(second[:,0], test_mask))\n",
    "    im31 = ax[3, 1].imshow(make_im(second[:,1], test_mask))\n",
    "    im32 = ax[3, 2].imshow(make_im(second[:,2], test_mask))\n",
    "    if all_colorbar is True:\n",
    "        fig.colorbar(im30, ax=ax[3, 0])\n",
    "        fig.colorbar(im31, ax=ax[3, 1])\n",
    "        fig.colorbar(im32, ax=ax[3, 2])\n",
    "\n",
    "    # second model residual\n",
    "    show_im(s_residual[:, 0], test_mask, ax=ax[4, 0])\n",
    "    show_im(s_residual[:, 1], test_mask, ax=ax[4, 1])\n",
    "    show_im(s_residual[:, 2], test_mask, ax=ax[4, 2])\n",
    "\n",
    "    # residual between the two models\n",
    "    show_im(fs_residual[:, 0], test_mask, ax=ax[5, 0])\n",
    "    show_im(fs_residual[:, 1], test_mask, ax=ax[5, 1])\n",
    "    show_im(fs_residual[:, 2], test_mask, ax=ax[5, 2])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the flattened `MuyGPyS` conventional solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mean_flat = conventional_mean(\n",
    "    Kin_flat,\n",
    "    Kcross_flat,\n",
    "    train_targets_flat,\n",
    "    noise_prior,\n",
    ")\n",
    "posterior_variance_flat = conventional_variance(\n",
    "    Kin_flat, \n",
    "    Kcross_flat, \n",
    "    Kout_analytic,\n",
    "    noise_prior,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posterior_mean_flat.shape, posterior_variance_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, visually compare the posterior mean and posterior variance.\n",
    "The flat and analytic solutions should be very close, up to ~1e-10 or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.allclose(posterior_mean_flat, posterior_mean_analytic))\n",
    "print(np.allclose(posterior_variance_flat, posterior_variance_analytic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean comparison\n",
    "\n",
    "Here we plot the posterior mean of the analytic and flattened `MuyGPyS` implementations, conditioned on the full training data.\n",
    "We also plot residuals with respect to the ground truth and each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_means(test_targets, posterior_mean_flat, posterior_mean_analytic, \"flat\", \"analytic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full covariance examination\n",
    "\n",
    "Here we plot the full posterior covariance, and find that they agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = np.abs(posterior_variance_analytic - posterior_variance_flat)\n",
    "fig, ax = plt.subplots(1,3,figsize = (14,4))\n",
    "\n",
    "ax[0].imshow(posterior_variance_analytic)\n",
    "ax[0].set_title(\"Analytic Variance\")\n",
    "\n",
    "ax[1].imshow(posterior_variance_flat)\n",
    "ax[1].set_title(\"Flat MuyGPyS Variance\")\n",
    "\n",
    "ax[2].set_title(\"|Analytic - Flat MuyGPyS|\")\n",
    "im = ax[2].imshow(residual)\n",
    "fig.colorbar(im, ax = ax[2])\n",
    "\n",
    "print(\"Min Resid = \", np.min(residual), \", Max Resid = \", np.max(residual), \", Avg Residual = \", np.mean(residual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_var_im(vec, mask, ax):\n",
    "    mat = make_im(vec, mask)\n",
    "    im = ax.imshow(mat.reshape(n, n), norm=SymLogNorm(linthresh=1e-7), cmap=my_sym_cmap)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "def compare_variances(first, second, fname, sname, fontsize=12):\n",
    "    residual = np.abs(first - second) + 1e-15\n",
    "\n",
    "    fig, ax = plt.subplots(9, 3, figsize = (8, 24))\n",
    "    \n",
    "    for axis_set in ax:\n",
    "        for axis in axis_set:\n",
    "            axis.set_xticks([])\n",
    "            axis.set_yticks([])\n",
    "\n",
    "    ax[0, 0].set_title(\"$\\kappa$\")\n",
    "    ax[0, 1].set_title(\"g1\")\n",
    "    ax[0, 2].set_title(\"g2\")\n",
    "    ax[0, 0].set_ylabel(f\"{fname} $\\kappa$\", fontsize=fontsize)\n",
    "    ax[1, 0].set_ylabel(f\"{fname} g1\", fontsize=fontsize)\n",
    "    ax[2, 0].set_ylabel(f\"{fname} g2\", fontsize=fontsize)\n",
    "    ax[3, 0].set_ylabel(f\"{sname} $\\kappa$\", fontsize=fontsize)\n",
    "    ax[4, 0].set_ylabel(f\"{sname} g1\", fontsize=fontsize)\n",
    "    ax[5, 0].set_ylabel(f\"{sname} g2\", fontsize=fontsize)\n",
    "    ax[6, 0].set_ylabel(\"residual $\\kappa$\", fontsize=fontsize)\n",
    "    ax[7, 0].set_ylabel(\"residual g1\", fontsize=fontsize)\n",
    "    ax[8, 0].set_ylabel(\"residual g2\", fontsize=fontsize)\n",
    "\n",
    "    # first variances\n",
    "    show_im(first[:, 0, 0], test_mask, ax=ax[0, 0])\n",
    "    show_var_im(first[:, 0, 1], test_mask, ax=ax[0, 1])\n",
    "    show_var_im(first[:, 0, 2], test_mask, ax=ax[0, 2])\n",
    "    show_var_im(first[:, 1, 0], test_mask, ax=ax[1, 0])\n",
    "    show_im(first[:, 1, 1], test_mask, ax=ax[1, 1])\n",
    "    show_var_im(first[:, 1, 2], test_mask, ax=ax[1, 2])\n",
    "    show_var_im(first[:, 2, 0], test_mask, ax=ax[2, 0])\n",
    "    show_var_im(first[:, 2, 1], test_mask, ax=ax[2, 1])\n",
    "    show_im(first[:, 2, 2], test_mask, ax=ax[2, 2])\n",
    "\n",
    "    # second variances\n",
    "    show_im(second[:, 0, 0], test_mask, ax=ax[3, 0])\n",
    "    show_var_im(second[:, 0, 1], test_mask, ax=ax[3, 1])\n",
    "    show_var_im(second[:, 0, 2], test_mask, ax=ax[3, 2])\n",
    "    show_var_im(second[:, 1, 0], test_mask, ax=ax[4, 0])\n",
    "    show_im(second[:, 1, 1], test_mask, ax=ax[4, 1])\n",
    "    show_var_im(second[:, 1, 2], test_mask, ax=ax[4, 2])\n",
    "    show_var_im(second[:, 2, 0], test_mask, ax=ax[5, 0])\n",
    "    show_var_im(second[:, 2, 1], test_mask, ax=ax[5, 1])\n",
    "    show_im(second[:, 2, 2], test_mask, ax=ax[5, 2])\n",
    "\n",
    "    # variance residuals\n",
    "    show_im(residual[:, 0, 0], test_mask, ax=ax[6, 0])\n",
    "    show_im(residual[:, 0, 1], test_mask, ax=ax[6, 1])\n",
    "    show_im(residual[:, 0, 2], test_mask, ax=ax[6, 2])\n",
    "    show_im(residual[:, 1, 0], test_mask, ax=ax[7, 0])\n",
    "    show_im(residual[:, 1, 1], test_mask, ax=ax[7, 1])\n",
    "    show_im(residual[:, 1, 2], test_mask, ax=ax[7, 2])\n",
    "    show_im(residual[:, 2, 0], test_mask, ax=ax[8, 0])\n",
    "    show_im(residual[:, 2, 1], test_mask, ax=ax[8, 1])\n",
    "    show_im(residual[:, 2, 2], test_mask, ax=ax[8, 2])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_diagonalize(var):\n",
    "    count = int(var.shape[0] / 3)\n",
    "    ret = np.zeros((count, 3, 3))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            ret[:, i, j] = np.diagonal(\n",
    "                var[\n",
    "                    (count * i):(count * (i + 1)),\n",
    "                    (count * j):(count * (j + 1)),\n",
    "                ]\n",
    "            )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior variance comparison\n",
    "\n",
    "Here we compare the posterior variances of the flattened `MuyGPyS` and analytic solutions, and compare their mutual residual.\n",
    "For each setting we plot a 3x3 grid of plots showing the variance or residual of the corresponding pair of shear parameters at each test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_variances(\n",
    "    tensor_diagonalize(posterior_variance_flat),\n",
    "    tensor_diagonalize(posterior_variance_analytic),\n",
    "    \"flat\",\n",
    "    \"analytic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MuyGPyS workflow\n",
    "\n",
    "Here we'll use an nn-sparsified MuyGPyS workflow to the conventional GP using the analytic kernel.\n",
    "The two approaches should converge (up to ~1e-9 precision) when `nn_count == test_count`.\n",
    "As `nn_count` decreases, the `MuyGPyS` workflow will get faster but will correspondingly drift from the conventional predictions.\n",
    "The two solutions should still remain visually similar, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_tensors(nn_count=50):\n",
    "    indices = np.arange(test_count)\n",
    "    if nn_count == train_count:\n",
    "        nn_indices = np.array([\n",
    "            np.arange(train_count) for _ in range(test_count)\n",
    "        ])\n",
    "    else:\n",
    "        nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method='exact', algorithm='ball_tree')\n",
    "        nn_indices, _ = nbrs_lookup.get_nns(test_features)\n",
    "    \n",
    "    (\n",
    "        crosswise_diffs,\n",
    "        pairwise_diffs,\n",
    "        nn_targets,\n",
    "    ) = shear_model.make_predict_tensors(\n",
    "        indices,\n",
    "        nn_indices,\n",
    "        test_features,\n",
    "        train_features,\n",
    "        train_targets,\n",
    "    )\n",
    "\n",
    "    nn_targets= nn_targets.swapaxes(-2, -1)\n",
    "    \n",
    "    x0_features = test_features[indices[0]][None, ...]\n",
    "    x0_nn_features = train_features[nn_indices[0]]\n",
    "    \n",
    "    return crosswise_diffs, pairwise_diffs, x0_features, x0_nn_features, nn_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section consistency checks the various MuyGPyS tensors when `nn_count == train_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crosswise_diffs, pairwise_diffs, x0_features, x0_nn_features, x0_nn_targets = get_nn_tensors(nn_count=train_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the `nn_targets` agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_targets_flat = x0_nn_targets.reshape(test_count, 3 * train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x0_nn_targets.shape, x0_targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all([\n",
    "    np.allclose(train_targets_flat, x0_targets_flat[0])\n",
    "    for _ in range(test_count)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the `Kin`s agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_test = shear_model.kernel(pairwise_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Kin_test.shape, Kin_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all([\n",
    "    np.allclose(\n",
    "        Kin_analytic,\n",
    "        Kin_test[i].reshape(3 * train_count, 3 * train_count),\n",
    "    ) for i in range(test_count)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the `Kcross`es agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_test = shear_model.kernel(crosswise_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Kcross_test.shape, Kcross_analytic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all([\n",
    "    np.allclose(\n",
    "        np.squeeze(Kcross_analytic.reshape(3, test_count, 3 * train_count)[:, i, :]),\n",
    "        Kcross_test[i].reshape(3 * train_count, 3).swapaxes(-2, -1),\n",
    "    ) for i in range(test_count)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check to see if the resulting means agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_analytic = np.array([\n",
    "    conventional_mean(\n",
    "        Kin_analytic,\n",
    "        np.squeeze(Kcross_analytic.reshape(3, test_count, 3 * train_count)[:, i, :]),\n",
    "        train_targets_flat,\n",
    "        noise_prior,\n",
    "    ) for i in range(test_count)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_test = np.array([\n",
    "    conventional_mean(\n",
    "        Kin_test[i].reshape(3 * train_count, 3 * train_count),\n",
    "        Kcross_test[i].reshape(3 * train_count, 3).swapaxes(-2, -1),\n",
    "        x0_targets_flat[i],\n",
    "        noise_prior,\n",
    "    ) for i in range(test_count)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(x0_analytic, x0_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the full `MuyGPyS` workflow.\n",
    "There are a lot of unnecessary internal checks and prints that were used for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def muygps_mean_workflow(nn_count=50):\n",
    "    crosswise_diffs, pairwise_diffs, x0_features, x0_nn_features, nn_targets = get_nn_tensors(nn_count=nn_count)\n",
    "\n",
    "    Kcross = shear_model.kernel(crosswise_diffs)\n",
    "    Kin = shear_model.kernel(pairwise_diffs)\n",
    "    \n",
    "    print(pairwise_diffs.shape, Kin.shape)\n",
    "    \n",
    "    Kin_flat = Kin.reshape(test_count, 3 * nn_count, 3 * nn_count)\n",
    "    Kcross_flat = Kcross.reshape(test_count, 3 * nn_count, 3)\n",
    "    nn_targets_flat = nn_targets.reshape(test_count, 3 * nn_count)\n",
    "    \n",
    "    Kin_an = conventional_shear(\n",
    "        x0_nn_features,\n",
    "        length_scale=length_scale,\n",
    "    )\n",
    "    Kcross_an = conventional_shear(\n",
    "        x0_features,\n",
    "        x0_nn_features,\n",
    "        length_scale=length_scale,\n",
    "    )\n",
    "    Kout_an = conventional_Kout(shear_model.kernel, 1)\n",
    "\n",
    "    # here we are consistency checking the tensors of each implementation\n",
    "    print(f\"Kin.shape = {Kin.shape}\")\n",
    "    print(f\"Kcross.shape = {Kcross.shape}\")\n",
    "    print(f\"nn_targets.shape = {nn_targets.shape}\")\n",
    "    print(\"----------\")\n",
    "    print(f\"Kin_flat.shape = {Kin_flat.shape}\")\n",
    "    print(f\"Kcross_flat.shape = {Kcross_flat.shape}\")\n",
    "    print(f\"nn_targets_flat.shape = {nn_targets_flat.shape}\")\n",
    "    print(\"----------\")\n",
    "    print(f\"Kin_an.shape = {Kin_an.shape}\")\n",
    "    print(f\"Kcross_an.shape = {Kcross_an.shape}\")\n",
    "    print(\"----------\")\n",
    "    print(\"----------\")\n",
    "    print(f\"Kin_flat[0] == Kin_an? {np.allclose(Kin_flat[0], Kin_an)}\")\n",
    "    print(f\"Kcross_flat[0] == Kcross_an? {np.allclose(Kcross_flat[0], Kcross_an.swapaxes(-2, -1))}\")\n",
    "    \n",
    "    mean = shear_model.posterior_mean(Kin, Kcross, nn_targets)\n",
    "    variance = shear_model.posterior_variance(Kin, Kcross)\n",
    "\n",
    "    # This is more spot checking to see whether and to what extent the different implementations\n",
    "    # agree on a particular prediction.\n",
    "    mean_flat = np.squeeze(conventional_mean(\n",
    "        Kin_flat[0],\n",
    "        Kcross_flat[0].swapaxes(-2, -1),\n",
    "        nn_targets_flat[0],\n",
    "        noise_prior,\n",
    "    ))\n",
    "    mean_an = np.squeeze(conventional_mean(\n",
    "        Kin_an,\n",
    "        Kcross_an,\n",
    "        nn_targets_flat[0],\n",
    "        noise_prior,\n",
    "    ))\n",
    "    variance_flat = np.squeeze(conventional_variance(\n",
    "        Kin_flat[0],\n",
    "        Kcross_flat[0].swapaxes(-2, -1),\n",
    "        Kout_an,\n",
    "        noise_prior,\n",
    "    ))\n",
    "    variance_an = np.squeeze(conventional_variance(\n",
    "        Kin_an,\n",
    "        Kcross_an,\n",
    "        Kout_an,\n",
    "        noise_prior,\n",
    "    ))\n",
    "    \n",
    "    print(\"----------\")\n",
    "    print(\"----------\")\n",
    "    print(f\"mean.shape = {mean.shape}\")\n",
    "    print(f\"mean_flat.shape = {mean_flat.shape}\")\n",
    "    print(f\"mean_an.shape = {mean_an.shape}\")\n",
    "    print(\"----------\")\n",
    "    print(f\"mean_flat == mean_an? {np.allclose(mean_flat, mean_an)}\")\n",
    "    print(f\"mean[0] == mean_flat? {np.allclose(mean[0], mean_flat)}\")\n",
    "    print(f\"mean[0] == mean_an? {np.allclose(mean[0], mean_an)}\")\n",
    "    print(\"----------\")\n",
    "    print(\"----------\")\n",
    "    print(f\"variance.shape = {variance.shape}\")\n",
    "    print(f\"variance_flat.shape = {variance_flat.shape}\")\n",
    "    print(f\"variance_an.shape = {variance_an.shape}\")\n",
    "    print(\"----------\")\n",
    "    print(\"----------\")\n",
    "    print(f\"variance_flat == variance_an? {np.allclose(variance_flat, variance_an)}\")\n",
    "    print(f\"variance[0] == variance_flat? {np.allclose(variance[0], variance_flat)}\")\n",
    "    print(f\"variance[0] == variance_an? {np.allclose(variance[0], variance_an)}\")\n",
    "#     print(mean[0])\n",
    "#     print(posterior_mean_analytic[:, 0])\n",
    "#     print(posterior_mean_flat[:, 0])\n",
    "    \n",
    "    return mean, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the MuyGPs posterior mean.\n",
    "If `nn_count == train_count`, the results should agree with the analytic/flat solutions.\n",
    "Smaller `nn_count`s will drift (as expected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mean_muygps, posterior_variance_muygps = muygps_mean_workflow(nn_count=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check numerically if things are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"mean is all close? {np.allclose(posterior_mean_analytic, posterior_mean_muygps)}\")\n",
    "print(f\"variance is all close? {np.allclose(tensor_diagonalize(posterior_variance_analytic), posterior_variance_muygps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the mean error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"mean ME: {np.mean(np.abs(posterior_mean_analytic - posterior_mean_muygps))}\")\n",
    "print(f\"variance ME: {np.mean(np.abs(tensor_diagonalize(posterior_variance_analytic) - posterior_variance_muygps))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean comparison\n",
    "\n",
    "Finally, we compare the `MuyGPyS` predictions conditioned on the specified number of neighbors to the conventional GP predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_means(test_targets, posterior_mean_muygps, posterior_mean_analytic, \"MuyGPyS\", \"Analytic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior variance comparison\n",
    "\n",
    "We follow it up by comparing the posterior variances as before, still conditioned on the specified number of nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_variances(\n",
    "    posterior_variance_muygps,\n",
    "    tensor_diagonalize(posterior_variance_analytic),\n",
    "    \"MuyGPyS\",\n",
    "    \"analytic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Test\n",
    "\n",
    "The next step is to test the optimizer on the mock data generated above.\n",
    "Recall that the mock data are sampled with a GP given a `length_scale`, which means that if we run hyperparameter optimization, we *should* recover this length scale if the optimizer is working properly.\n",
    "\n",
    "First, create the train tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from MuyGPyS.gp.tensors import crosswise_tensor, pairwise_tensor\n",
    "from MuyGPyS.optimize.batch import sample_batch\n",
    "from MuyGPyS.optimize import Bayes_optimize\n",
    "from MuyGPyS.optimize.loss import lool_fn, looph_fn, mse_fn\n",
    "from MuyGPyS.gp.hyperparameter import AnalyticScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_targets.shape, train_features.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_targets = targets.swapaxes(0,1)\n",
    "train_features = features\n",
    "print(train_features.shape, train_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shear_model = MuyGPS(\n",
    "    kernel=ShearKernel(\n",
    "        deformation=DifferenceIsotropy(\n",
    "            F2,\n",
    "            length_scale=Parameter(0.5, [0.01, 0.9]),\n",
    "        ),\n",
    "    ),\n",
    "    noise=ShearNoise33(noise_prior),\n",
    "    scale=AnalyticScale(),\n",
    ")\n",
    "\n",
    "train_features_count = train_features.shape[0]\n",
    "\n",
    "nn_count = 50\n",
    "nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method='exact', algorithm='ball_tree')\n",
    "    \n",
    "\n",
    "batch_count=500\n",
    "batch_indices, batch_nn_indices = sample_batch(\n",
    "    nbrs_lookup, batch_count, train_features_count\n",
    ")\n",
    "\n",
    "# need pairwise and crosswise diffs\n",
    "batch_crosswise_diffs = shear_model.kernel.deformation.crosswise_tensor(\n",
    "    train_features,\n",
    "    train_features,\n",
    "    batch_indices,\n",
    "    batch_nn_indices,\n",
    ")\n",
    "\n",
    "batch_pairwise_diffs = shear_model.kernel.deformation.pairwise_tensor(\n",
    "    train_features, batch_nn_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_targets = train_targets[batch_indices]\n",
    "batch_nn_targets= train_targets[batch_nn_indices].swapaxes(-2, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if the optimization correctly predicts the length scale with the `mse` loss fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shear_mse_optimized = Bayes_optimize(\n",
    "    shear_model,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    train_targets,\n",
    "    loss_fn=mse_fn,\n",
    "    verbose=True,\n",
    "    init_points=5,\n",
    "    n_iter=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_count = test_features.shape[0]\n",
    "\n",
    "indices = np.arange(test_features_count)\n",
    "test_nn_indices, _ = nbrs_lookup.get_nns(test_features)\n",
    "\n",
    "(\n",
    "    test_crosswise_diffs,\n",
    "    test_pairwise_diffs,\n",
    "    test_nn_targets,\n",
    ") = shear_model.make_predict_tensors(\n",
    "    indices,\n",
    "    test_nn_indices,\n",
    "    test_features,\n",
    "    train_features,\n",
    "    train_targets,\n",
    ")\n",
    "\n",
    "test_nn_targets= test_nn_targets.swapaxes(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross = shear_mse_optimized.kernel(test_crosswise_diffs)\n",
    "Kin = shear_mse_optimized.kernel(test_pairwise_diffs)\n",
    "posterior_mean_muygps_optimized = shear_mse_optimized.posterior_mean(Kin, Kcross, test_nn_targets)\n",
    "posterior_variance_muygps_optimized = shear_mse_optimized.posterior_variance(Kin, Kcross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_means(test_targets, posterior_mean_muygps_optimized, posterior_mean_analytic, \"Optimized MuyGPyS\", \"Analytic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, an presentation of the posterior variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_variances(\n",
    "    posterior_variance_muygps_optimized,\n",
    "    tensor_diagonalize(posterior_variance_analytic),\n",
    "    \"Optimized MuyGPyS\",\n",
    "    \"analytic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2in3out Exploration\n",
    "\n",
    "Here we explore the 2in3out variant of the shear kernel, which trains on observations only of `g1` and `g2`, but predicts onto all three covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.gp.kernels.experimental import ShearKernel2in3out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_analytic_33 = Kin_analytic\n",
    "Kcross_analytic_33 = Kcross_analytic\n",
    "targets_flat_33 = train_targets_flat\n",
    "mean_analytic_33 = posterior_mean_analytic\n",
    "variance_analytic_33 = posterior_variance_analytic\n",
    "variance_diag_33 = np.diag(variance_analytic_33)\n",
    "ci_analytic_33 = np.sqrt(variance_diag_33) * 1.96\n",
    "ci_analytic_33 = ci_analytic_33.reshape(test_count, 3)\n",
    "coverage_analytic_33 = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_targets - mean_analytic_33) < ci_analytic_33, axis=0\n",
    "    ) / test_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_analytic_23 = Kin_analytic[train_count:, train_count:]\n",
    "Kcross_analytic_23 = Kcross_analytic[:, train_count:]\n",
    "targets_flat_23 = train_targets_flat[train_count:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_analytic_23 = conventional_mean(\n",
    "    Kin_analytic_23, Kcross_analytic_23, targets_flat_23, noise_prior\n",
    ")\n",
    "variance_analytic_23 = conventional_variance(\n",
    "    Kin_analytic_23, Kcross_analytic_23, Kout_analytic, noise_prior\n",
    ")\n",
    "variance_diag_23 = np.diag(variance_analytic_23)\n",
    "ci_analytic_23 = np.sqrt(variance_diag_23) * 1.96\n",
    "ci_analytic_23 = ci_analytic_23.reshape(test_count, 3)\n",
    "coverage_analytic_23 = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_targets - mean_analytic_23) < ci_analytic_23, axis=0\n",
    "    ) / test_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    mean_analytic_33.shape, Kin_analytic_33.shape, Kcross_analytic_33.shape, targets_flat_33.shape\n",
    ")\n",
    "print(\n",
    "    mean_analytic_23.shape, Kin_analytic_23.shape, Kcross_analytic_23.shape, targets_flat_23.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_means(test_targets, mean_analytic_23, mean_analytic_33, \"2x3 Model\", \"3x3 Model\", all_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the same thing with the 2x3 MuyGPyS implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model33 = MuyGPS(\n",
    "    kernel=ShearKernel(\n",
    "        deformation=DifferenceIsotropy(\n",
    "            F2,\n",
    "            length_scale=Parameter(shear_mse_optimized.kernel.deformation.length_scale()),\n",
    "        ),\n",
    "    ),\n",
    "    noise=ShearNoise33(noise_prior),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model23 = MuyGPS(\n",
    "    kernel=ShearKernel2in3out(\n",
    "        deformation=DifferenceIsotropy(\n",
    "            F2,\n",
    "            length_scale=Parameter(shear_mse_optimized.kernel.deformation.length_scale()),\n",
    "        ),\n",
    "    ),\n",
    "    noise=HomoscedasticNoise(noise_prior),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_33 = model33.kernel(test_crosswise_diffs)\n",
    "Kin_33 = model33.kernel(test_pairwise_diffs)\n",
    "nn_targets_33 = test_nn_targets\n",
    "mean_muygps_33 = model33.posterior_mean(Kin_33, Kcross_33, nn_targets_33)\n",
    "covariance_muygps_33 = model33.posterior_variance(Kin_33, Kcross_33)\n",
    "variance_muygps_33 = np.zeros((test_count, 3))\n",
    "for i in range(test_count):\n",
    "    variance_muygps_33[i, 0] = covariance_muygps_33[i, 0, 0] \n",
    "    variance_muygps_33[i, 1] = covariance_muygps_33[i, 1, 1] \n",
    "    variance_muygps_33[i, 2] = covariance_muygps_33[i, 2, 2] \n",
    "ci_muygps_33 = np.sqrt(variance_muygps_33) * 1.96\n",
    "coverage_muygps_33 = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_targets - mean_muygps_33) < ci_muygps_33, axis=0\n",
    "    ) / test_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_23 = model23.kernel(test_crosswise_diffs)\n",
    "Kin_23 = model23.kernel(test_pairwise_diffs)\n",
    "nn_targets_23 = test_nn_targets[:, 1:, :]\n",
    "mean_muygps_23 = model23.posterior_mean(Kin_23, Kcross_23, nn_targets_23)\n",
    "covariance_muygps_23 = model23.posterior_variance(Kin_23, Kcross_23)\n",
    "variance_muygps_23 = np.zeros((test_count, 3))\n",
    "for i in range(test_count):\n",
    "    variance_muygps_23[i, 0] = covariance_muygps_23[i, 0, 0] \n",
    "    variance_muygps_23[i, 1] = covariance_muygps_23[i, 1, 1] \n",
    "    variance_muygps_23[i, 2] = covariance_muygps_23[i, 2, 2] \n",
    "ci_muygps_23 = np.sqrt(variance_muygps_23) * 1.96\n",
    "coverage_muygps_23 = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_targets - mean_muygps_23) < ci_muygps_23, axis=0\n",
    "    ) / test_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    mean_muygps_23.shape, Kin_23.shape, Kcross_23.shape, nn_targets_23.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare the 2x3 MuyGPyS implementation to the 2x3 Analytic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_means(test_targets, mean_analytic_23, mean_muygps_23, \"2x3 Analytic\", \"2x3 MuyGPs\", all_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare the 2x3 MuyGPyS implementation to the 3x3 Analytic model.\n",
    "Note that the 3x3 analytic model is a better approximation to the truth, but both look at least visually ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_means(test_targets, mean_analytic_33, mean_muygps_23, \"3x3 Analytic\", \"2x3 MuyGPs\", all_colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_muygps_23.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_variances(\n",
    "    covariance_muygps_23,\n",
    "    tensor_diagonalize(variance_analytic_33),\n",
    "    \"2x3 MuyGPyS\",\n",
    "    \"3x3 analytic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the covariate-wise coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ci_a33 = np.mean(ci_analytic_33, axis=0)\n",
    "mean_ci_a23 = np.mean(ci_analytic_23, axis=0)\n",
    "mean_ci_m33 = np.mean(ci_muygps_33, axis=0)\n",
    "mean_ci_m23 = np.mean(ci_muygps_23, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\tconvergence\\t\\tshear 1\\t\\t\\tshear 2\")\n",
    "print(f\"dense 3x3\\t{mean_ci_a33[0]}\\t{mean_ci_a33[1]}\\t{mean_ci_a33[2]}\")\n",
    "print(f\"muygps 3x3\\t{mean_ci_m33[0]}\\t{mean_ci_m33[1]}\\t{mean_ci_m33[2]}\")\n",
    "print(f\"dense 2x3\\t{mean_ci_a23[0]}\\t{mean_ci_a23[1]}\\t{mean_ci_a23[2]}\")\n",
    "print(f\"muygps 2x3\\t{mean_ci_m23[0]}\\t{mean_ci_m23[1]}\\t{mean_ci_m23[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\tconvergence\\tshear 1\\t\\tshear 2\")\n",
    "print(f\"dense 3x3\\t{coverage_analytic_33[0]}\\t\\t{coverage_analytic_33[1]}\\t\\t{coverage_analytic_33[2]}\")\n",
    "print(f\"muygps 3x3\\t{coverage_muygps_33[0]}\\t\\t{coverage_muygps_33[1]}\\t\\t{coverage_muygps_33[2]}\")\n",
    "print(f\"dense 2x3\\t{coverage_analytic_23[0]}\\t\\t{coverage_analytic_23[1]}\\t\\t{coverage_analytic_23[2]}\")\n",
    "print(f\"muygps 2x3\\t{coverage_muygps_23[0]}\\t\\t{coverage_muygps_23[1]}\\t\\t{coverage_muygps_23[2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
