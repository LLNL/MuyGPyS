{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2023-2023 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shear Kernel Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the specialized lensing shear kernel (hard-coded to RBF at the moment).\n",
    "\n",
    "⚠️ _Note that this is still an experimental feature._ ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "from MuyGPyS._src.gp.tensors import _crosswise_differences, _pairwise_differences\n",
    "from MuyGPyS.gp import MuyGPS\n",
    "from MuyGPyS.gp.deformation import Isotropy, l2, F2\n",
    "from MuyGPyS.gp.hyperparameter import Parameter\n",
    "from MuyGPyS.gp.kernels.experimental import ShearKernel\n",
    "from MuyGPyS.gp.tensors import make_predict_tensors\n",
    "from MuyGPyS.neighbors import NN_Wrapper\n",
    "from MuyGPyS.gp.noise import HomoscedasticNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is required to import the implementation from Bob Armstrong's original repository.\n",
    "Must set `shear_kernel_dir = \"path/to/local/shear_kernel/\"` for things to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "# introduce a variable for path/to/shear_kernel\n",
    "shear_kernel_dir = \"../../../projects/shear_kernel/\"\n",
    "if not os.path.isdir(shear_kernel_dir):\n",
    "    shear_kernel_dir = \"../../shear_kernel/\"\n",
    "spec_analytic = importlib.util.spec_from_file_location(\"analytic_kernel\", shear_kernel_dir + \"analytic_kernel.py\") \n",
    "bar = importlib.util.module_from_spec(spec_analytic)\n",
    "sys.modules[\"analytic_kernel\"] = bar\n",
    "spec_analytic.loader.exec_module(bar)\n",
    "from analytic_kernel import shear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set a random seed here for consistency when building docs.\n",
    "In practice we would not fix a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Implementation Comparisons\n",
    "\n",
    "Here we will compare the analytic implementation of the kernel function to the `MuyGPyS` implementation, using some simple data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build some simple data, which is mean to represent a grid of sky coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25  # number of galaxies on a side\n",
    "xmin = 0\n",
    "xmax = 1\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "\n",
    "xx = np.linspace(xmin, xmax, n)\n",
    "yy = np.linspace(ymin, ymax, n)\n",
    "\n",
    "x, y = np.meshgrid(xx, yy)\n",
    "features = np.vstack((x.flatten(), y.flatten())).T\n",
    "data_count = features.shape[0]\n",
    "diffs = _pairwise_differences(features)\n",
    "length_scale = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use an Isotropic distance functor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_fn = Isotropy(\n",
    "    metric=F2,\n",
    "    length_scale=Parameter(length_scale),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct a shear value kernel (partial differential components of RBF), as well as the original RBF kernel using Bob's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_shear(X1, X2=None, length_scale=1.0):\n",
    "    if X2 is None:\n",
    "        X2 = X1\n",
    "    n1, _ = X1.shape\n",
    "    n2, _ = X2.shape\n",
    "    vals = np.zeros((3 * (n1), 3 * (n2)))\n",
    "    vals[:] = np.nan\n",
    "    for i, (ix, iy) in enumerate(X1):\n",
    "        for j, (jx, jy) in enumerate(X2):\n",
    "            tmp = shear_kernel(ix, iy, jx, jy, b=length_scale)\n",
    "            for a in range(3):\n",
    "                for b in range(3):\n",
    "                    vals[(a * n1) + i, (b * n2) + j] = tmp[a, b]\n",
    "            \n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise kernels (`Kin`)\n",
    "This code computes the `Kin` kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_analytic = original_shear(features, length_scale=length_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the same using the MuyGPyS implementation. Note the increased efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_muygps = ShearKernel(deformation=dist_fn)(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Kin_muygps` is a more generalized tensor, so we need to flatten it to a conforming shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_flat = Kin_muygps.reshape(data_count * 3, data_count * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Kin_analytic.shape = {Kin_analytic.shape}\")\n",
    "print(f\"Kin_muygps.shape = {Kin_muygps.shape}\")\n",
    "print(f\"Kin_flat.shape = {Kin_flat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the two implementations agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.allclose(Kin_analytic, Kin_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_residual = np.abs(Kin_analytic - Kin_flat)\n",
    "print(f\"Kin residual max: {np.max(Kin_residual)}, min: {np.min(Kin_residual)}, mean : {np.mean(Kin_residual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results of the baseline and MuyGPyS implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].set_title(\"original shear kernel\")\n",
    "axes[0].imshow(Kin_analytic)\n",
    "axes[1].set_title(\"MuyGPyS shear kernel\")\n",
    "axes[1].imshow(Kin_flat)\n",
    "axes[2].set_title(\"Residual\")\n",
    "im = axes[2].imshow(Kin_residual, norm=LogNorm())\n",
    "fig.colorbar(im, ax=axes[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Covariance (`Kcross`)\n",
    "Now we perform a similar analysis for the cross-covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 200\n",
    "X1 = features[:split]\n",
    "X2 = features[split:]\n",
    "n1, _ = X1.shape\n",
    "n2, _ = X2.shape\n",
    "crosswise_diffs = _crosswise_differences(X1, X2)\n",
    "print(X1.shape, X2.shape, crosswise_diffs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_analytic = original_shear(X1, X2, length_scale=length_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_muygps = ShearKernel(deformation=dist_fn)(crosswise_diffs, adjust=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_flat = Kcross_muygps.reshape(n1 * 3, n2 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Kcross_analytic.shape = {Kcross_analytic.shape}\")\n",
    "print(f\"Kcross_muygps.shape = {Kcross_muygps.shape}\")\n",
    "print(f\"Kcross_flat.shape = {Kcross_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(Kcross_analytic, Kcross_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_residual = np.abs(Kcross_analytic - Kcross_flat)\n",
    "print(f\"Kcross residual max: {np.max(Kcross_residual)}, min: {np.min(Kcross_residual)}, mean : {np.mean(Kcross_residual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].set_title(\"original shear kernel\")\n",
    "axes[0].imshow(Kcross_analytic)\n",
    "axes[1].set_title(\"MuyGPyS shear kernel\")\n",
    "axes[1].imshow(Kcross_flat)\n",
    "axes[2].set_title(\"Residual\")\n",
    "im = axes[2].imshow(Kcross_residual, norm=LogNorm())\n",
    "fig.colorbar(im, ax=axes[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime comparison of the two implementations (Change `False` to `True` to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    %timeit original_shear(features)\n",
    "    %timeit ShearKernel(deformation=dist_fn)(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Mean Comparison\n",
    "\n",
    "Now we will test the `posterior_mean` of the analytic and muygps implementations.\n",
    "First, we set up an arbitrary taget array.\n",
    "Targets should be square matrices like a grid of a swath of sky.\n",
    "Ulitimately, the target array will have shape `(625,3)`, given `n=25` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_prior = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the target matrices.\n",
    "Initially was run with arbitrary targets, now as of 1/25/24 can sample targets from GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targets_from_GP(features, n, ls):\n",
    "\n",
    "    Kernel = ShearKernel(\n",
    "            deformation=Isotropy(\n",
    "                F2,\n",
    "                length_scale=Parameter(ls),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    diffs = _pairwise_differences(features)\n",
    "\n",
    "    #print(diffs.shape)\n",
    "\n",
    "    Kin = 1.0 * Kernel(diffs, adjust=False)\n",
    "\n",
    "    Kin_flat = Kin.reshape(3 * n**2, 3 * n**2) + 1e-10 * np.identity(3 * n**2)\n",
    "\n",
    "    #plt.imshow(Kin)\n",
    "\n",
    "    #print(Kin.shape, np.min(Kin), np.max(Kin))\n",
    "\n",
    "    e = np.random.normal(0, 1, 3 * n**2)\n",
    "    L = np.linalg.cholesky(Kin_flat)\n",
    "    targets = np.dot(L, e).reshape(3, n**2).swapaxes(0,1)\n",
    "\n",
    "    return(targets)\n",
    "\n",
    "def arbitrary_targets(n):\n",
    "\n",
    "    targets_grid_x, targets_grid_y = np.meshgrid(\n",
    "        np.linspace(1, 10, n),\n",
    "        np.linspace(1, 10, n),\n",
    "        indexing = 'ij'\n",
    "    )\n",
    "    targets_grid = targets_grid_x * targets_grid_y\n",
    "\n",
    "    targets = np.vstack(\n",
    "        (targets_grid.flatten(), np.rot90(targets_grid, k=3).flatten(), np.rot90(targets_grid).flatten()),\n",
    "    )\n",
    "\n",
    "    targets_norm = (targets - np.min(targets)) / (np.max(targets) - np.min(targets))\n",
    "\n",
    "    return(targets_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets_from_GP(features, n, length_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a train/test split in the dataset.\n",
    "Modify the `train_ratio` to specify the proportion of data to hold out for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=1)\n",
    "interval_count = int(data_count * train_ratio)\n",
    "interval = int(data_count / interval_count)\n",
    "sfl = rng.permutation(np.arange(data_count))\n",
    "train_mask = np.zeros(data_count, dtype=bool)\n",
    "for i in range(interval_count):\n",
    "    idx = np.random.choice(sfl[i * interval : (i + 1) * interval])\n",
    "    train_mask[idx] = True\n",
    "test_mask = np.invert(train_mask)\n",
    "train_count = np.count_nonzero(train_mask)\n",
    "test_count = np.count_nonzero(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = targets[train_mask, :]\n",
    "test_targets = targets[test_mask, :]\n",
    "train_features = features[train_mask, :]\n",
    "test_features = features[test_mask, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the train/test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_im(vec, mask):\n",
    "    ret = np.zeros(len(mask))\n",
    "    ret[mask] = vec\n",
    "    ret[np.invert(mask)] = -np.inf\n",
    "    return ret.reshape(n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3,figsize = (10,7))\n",
    "ax[0, 0].imshow(make_im(train_targets[:,0], train_mask))\n",
    "ax[0, 0].set_ylabel(\"train\", fontsize = 15)\n",
    "ax[0, 0].set_title(\"$\\kappa$\", fontsize = 15)\n",
    "ax[1, 0].imshow(make_im(test_targets[:,0], test_mask))\n",
    "ax[1, 0].set_ylabel(\"test\", fontsize = 15)\n",
    "ax[0, 1].imshow(make_im(train_targets[:,1], train_mask))\n",
    "ax[0, 1].set_title(\"g1\", fontsize = 15)\n",
    "ax[1, 1].imshow(make_im(test_targets[:,1], test_mask))\n",
    "ax[0, 2].imshow(make_im(train_targets[:,2], train_mask))\n",
    "ax[0, 2].set_title(\"g2\", fontsize = 15)\n",
    "ax[1, 2].imshow(make_im(test_targets[:,2], test_mask))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicitly define the target matrices.\n",
    "Also add leading unitary dimension to `targets_muygpys` for things to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets_flat = train_targets.swapaxes(0,1).reshape(3 * train_count)\n",
    "test_targets_flat = test_targets.swapaxes(0,1).reshape(3 * test_count)\n",
    "print(\n",
    "    train_targets_flat.shape, test_targets_flat.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the `K*` matrices above, compare posterior means, sans-optimization. \n",
    "This comes from (3.4) in Muyskens et al. (2021), which looks like:\n",
    "$$ \\hat{Y}(X^*|X) = K_{\\theta}(X^*,X)(K_{\\theta}(X,X)+\\epsilon)^{-1}Y(X) $$\n",
    "where $\\epsilon = \\sigma^2 I$ with $\\sigma^2$ being the noise variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytic: for the analytic implementation, I'll do things with the full \"flattened\" difference tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_analytic = original_shear(train_features, train_features, length_scale=length_scale)\n",
    "Kcross_analytic = original_shear(test_features, train_features, length_scale=length_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Kcross_analytic.shape, Kin_analytic.shape, train_targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conventional_mean(Kin, Kcross, targets, noise=noise_prior):\n",
    "    nugget_size = Kin.shape[0]\n",
    "    test_count = int(Kcross.shape[0] / 3)\n",
    "    return (\n",
    "        Kcross @ np.linalg.solve(\n",
    "            Kin + noise * np.eye(nugget_size),\n",
    "            targets,\n",
    "        )\n",
    "    ).reshape(3, test_count).swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mean_analytic = conventional_mean(\n",
    "    Kin_analytic,\n",
    "    Kcross_analytic,\n",
    "    train_targets_flat,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll compare to a flattened `MuyGPyS` implementation without nearest neighbors sparsification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create MuyGPS object\n",
    "shear_model = MuyGPS(\n",
    "        kernel=ShearKernel(\n",
    "            deformation=Isotropy(\n",
    "                F2,\n",
    "                length_scale=Parameter(length_scale),\n",
    "            ),\n",
    "        ),\n",
    "#         noise = HomoscedasticNoise(noise_prior),\n",
    "        noise = HomoscedasticNoise(1e-4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create flat solve using MuyGPyS functions.\n",
    "This should be very close to the analytic solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_diffs = _pairwise_differences(train_features)\n",
    "crosswise_diffs = _crosswise_differences(test_features, train_features)\n",
    "Kin_muygps = shear_model.kernel(pairwise_diffs, adjust=False)\n",
    "Kcross_muygps = shear_model.kernel(crosswise_diffs, adjust=False)\n",
    "Kin_flat = Kin_muygps.reshape(3 * train_count, 3 * train_count)\n",
    "Kcross_flat = Kcross_muygps.reshape(3 * test_count, 3 * train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Kin_muygps.shape, Kcross_muygps.shape, Kin_flat.shape, Kcross_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the flattened kernel tensors agree with the analytic tensors (should pass if the above passed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.allclose(Kin_analytic, Kin_flat),\n",
    "    np.allclose(Kcross_analytic, Kcross_flat),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_im(vec, mask, ax):\n",
    "    mat = np.zeros(len(mask))\n",
    "    mat[mask] = vec\n",
    "    mat[np.invert(mask)] = -np.inf\n",
    "    im = ax.imshow(mat.reshape(n, n), norm=LogNorm())\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "def compare_predictions(truth, first, second, fname, sname, fontsize=12):\n",
    "    f_residual = np.abs(truth - first) + 1e-15\n",
    "    s_residual = np.abs(truth - second) + 1e-15\n",
    "    fs_residual = np.abs(first - second) + 1e-15\n",
    "\n",
    "    fig, ax = plt.subplots(6, 3, figsize = (10, 18))\n",
    "    \n",
    "    for axis_set in ax:\n",
    "        for axis in axis_set:\n",
    "            axis.set_xticks([])\n",
    "            axis.set_yticks([])\n",
    "\n",
    "    ax[0, 0].set_title(\"$\\kappa$\")\n",
    "    ax[0, 1].set_title(\"g1\")\n",
    "    ax[0, 2].set_title(\"g2\")\n",
    "    ax[0, 0].set_ylabel(\"Truth\", fontsize=fontsize)\n",
    "    ax[1, 0].set_ylabel(f\"{fname} Mean\", fontsize=fontsize)\n",
    "    ax[2, 0].set_ylabel(f\"|truth - {fname}|\", fontsize=fontsize)\n",
    "    ax[3, 0].set_ylabel(f\"{sname} Mean\", fontsize=fontsize)\n",
    "    ax[4, 0].set_ylabel(f\"|truth - {sname}|\", fontsize=fontsize)\n",
    "    ax[5, 0].set_ylabel(f\"|{fname} - {sname}|\", fontsize=fontsize)\n",
    "\n",
    "    # truth\n",
    "    ax[0, 0].imshow(make_im(truth[:,0], test_mask))\n",
    "    ax[0, 1].imshow(make_im(truth[:,1], test_mask))\n",
    "    ax[0, 2].imshow(make_im(truth[:,2], test_mask))\n",
    "\n",
    "    # first model\n",
    "    ax[1, 0].imshow(make_im(first[:,0], test_mask))\n",
    "    ax[1, 1].imshow(make_im(first[:,1], test_mask))\n",
    "    ax[1, 2].imshow(make_im(first[:,2], test_mask))\n",
    "    \n",
    "    # first model residual\n",
    "    show_im(f_residual[:,0], test_mask, ax=ax[2, 0])\n",
    "    show_im(f_residual[:,1], test_mask, ax=ax[2, 1])\n",
    "    show_im(f_residual[:,2], test_mask, ax=ax[2, 2])\n",
    "\n",
    "    # second model\n",
    "    ax[3, 0].imshow(make_im(second[:,0], test_mask))\n",
    "    ax[3, 1].imshow(make_im(second[:,1], test_mask))\n",
    "    ax[3, 2].imshow(make_im(second[:,2], test_mask))\n",
    "    \n",
    "    # second model residual\n",
    "    show_im(s_residual[:, 0], test_mask, ax=ax[4, 0])\n",
    "    show_im(s_residual[:, 1], test_mask, ax=ax[4, 1])\n",
    "    show_im(s_residual[:, 2], test_mask, ax=ax[4, 2])\n",
    "\n",
    "    # residual between the two models\n",
    "    show_im(fs_residual[:, 0], test_mask, ax=ax[5, 0])\n",
    "    show_im(fs_residual[:, 1], test_mask, ax=ax[5, 1])\n",
    "    show_im(fs_residual[:, 2], test_mask, ax=ax[5, 2])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the flattened `MuyGPyS` conventional solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mean_flat = conventional_mean(\n",
    "    Kin_flat,\n",
    "    Kcross_flat,\n",
    "    train_targets_flat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posterior_mean_flat.shape, posterior_mean_analytic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, visually compare the predictions.\n",
    "The flat and analytic solutions should be very close, up to ~1e-10 or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_predictions(test_targets, posterior_mean_flat, posterior_mean_analytic, \"flat\", \"analytic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MuyGPyS workflow\n",
    "\n",
    "Here we'll use an nn-sparsified MuyGPyS workflow to the conventional GP using the analytic kernel.\n",
    "The two approaches should converge (up to ~1e-9 precision) when `nn_count == test_count`.\n",
    "As `nn_count` decreases, the `MuyGPyS` workflow will get faster but will correspondingly drift from the conventional predictions.\n",
    "The two solutions should still remain visually similar, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_tensors(nn_count=50):\n",
    "    indices = np.arange(test_count)\n",
    "    if nn_count == train_count:\n",
    "        nn_indices = np.array([\n",
    "            np.arange(train_count) for _ in range(test_count)\n",
    "        ])\n",
    "    else:\n",
    "        nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method='exact', algorithm='ball_tree')\n",
    "        nn_indices, _ = nbrs_lookup.get_nns(test_features)\n",
    "    \n",
    "    (\n",
    "        crosswise_diffs,\n",
    "        pairwise_diffs,\n",
    "        nn_targets,\n",
    "    ) = make_predict_tensors(\n",
    "        indices,\n",
    "        nn_indices,\n",
    "        test_features,\n",
    "        train_features,\n",
    "        train_targets,\n",
    "    )\n",
    "\n",
    "    nn_targets= nn_targets.swapaxes(-2, -1)\n",
    "    \n",
    "#     if nn_count == train_count:\n",
    "#         nn_targets = np.array([train_targets for _ in range(test_count)])\n",
    "    \n",
    "    x0_features = test_features[indices[0]][None, ...]\n",
    "    x0_nn_features = train_features[nn_indices[0]]\n",
    "    \n",
    "    return crosswise_diffs, pairwise_diffs, x0_features, x0_nn_features, nn_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section consistency checks the various MuyGPyS tensors when `nn_count == train_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crosswise_diffs, pairwise_diffs, x0_features, x0_nn_features, x0_nn_targets = get_nn_tensors(nn_count=train_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the `nn_targets` agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_targets_flat = x0_nn_targets.reshape(test_count, 3 * train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x0_nn_targets.shape, x0_targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all([\n",
    "    np.allclose(train_targets_flat, x0_targets_flat[0])\n",
    "    for _ in range(test_count)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the `Kin`s agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_test = shear_model.kernel(pairwise_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Kin_test.shape, Kin_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all([\n",
    "    np.allclose(\n",
    "        Kin_analytic,\n",
    "        Kin_test[i].reshape(3 * train_count, 3 * train_count),\n",
    "    ) for i in range(test_count)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the `Kcross`es agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_test = shear_model.kernel(crosswise_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Kcross_test.shape, Kcross_analytic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all([\n",
    "    np.allclose(\n",
    "        np.squeeze(Kcross_analytic.reshape(3, test_count, 3 * train_count)[:, i, :]),\n",
    "        Kcross_test[i].reshape(3 * train_count, 3).swapaxes(-2, -1),\n",
    "    ) for i in range(test_count)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check to see if the resulting means agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_analytic = np.array([\n",
    "    conventional_mean(\n",
    "        Kin_analytic,\n",
    "        np.squeeze(Kcross_analytic.reshape(3, test_count, 3 * train_count)[:, i, :]),\n",
    "        train_targets_flat,\n",
    "    ) for i in range(test_count)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_test = np.array([\n",
    "    conventional_mean(\n",
    "        Kin_test[i].reshape(3 * train_count, 3 * train_count),\n",
    "        Kcross_test[i].reshape(3 * train_count, 3).swapaxes(-2, -1),\n",
    "        x0_targets_flat[i],\n",
    "    ) for i in range(test_count)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(x0_analytic, x0_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the full `MuyGPyS` workflow.\n",
    "There are a lot of unnecessary internal checks and prints that were used for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def muygps_mean_workflow(nn_count=50):\n",
    "    crosswise_diffs, pairwise_diffs, x0_features, x0_nn_features, nn_targets = get_nn_tensors(nn_count=nn_count)\n",
    "\n",
    "    Kcross = shear_model.kernel(crosswise_diffs)\n",
    "    Kin = shear_model.kernel(pairwise_diffs)\n",
    "    \n",
    "    print(pairwise_diffs.shape, Kin.shape)\n",
    "    \n",
    "    Kin_flat = Kin.reshape(test_count, 3 * nn_count, 3 * nn_count)\n",
    "    Kcross_flat = Kcross.reshape(test_count, 3 * nn_count, 3)\n",
    "    nn_targets_flat = nn_targets.reshape(test_count, 3 * nn_count)\n",
    "    \n",
    "    Kin_an = original_shear(\n",
    "        x0_nn_features,\n",
    "        length_scale=length_scale,\n",
    "    )\n",
    "    Kcross_an = original_shear(\n",
    "        x0_features,\n",
    "        x0_nn_features,\n",
    "        length_scale=length_scale,\n",
    "    )\n",
    "\n",
    "    # here we are consistency checking the tensors of each implementation\n",
    "    print(f\"Kin.shape = {Kin.shape}\")\n",
    "    print(f\"Kcross.shape = {Kcross.shape}\")\n",
    "    print(f\"nn_targets.shape = {nn_targets.shape}\")\n",
    "    print(\"----------\")\n",
    "    print(f\"Kin_flat.shape = {Kin_flat.shape}\")\n",
    "    print(f\"Kcross_flat.shape = {Kcross_flat.shape}\")\n",
    "    print(f\"nn_targets_flat.shape = {nn_targets_flat.shape}\")\n",
    "    print(\"----------\")\n",
    "    print(f\"Kin_an.shape = {Kin_an.shape}\")\n",
    "    print(f\"Kcross_an.shape = {Kcross_an.shape}\")\n",
    "    print(\"----------\")\n",
    "    print(\"----------\")\n",
    "    print(f\"Kin_flat[0] == Kin_an? {np.allclose(Kin_flat[0], Kin_an)}\")\n",
    "    print(f\"Kcross_flat[0] == Kcross_an? {np.allclose(Kcross_flat[0], Kcross_an.swapaxes(-2, -1))}\")\n",
    "    \n",
    "    mean = shear_model.posterior_mean(Kin, Kcross, nn_targets)\n",
    "\n",
    "    # This is more spot checking to see whether and to what extent the different implementations\n",
    "    # agree on a particular prediction.\n",
    "    mean_flat = np.squeeze(conventional_mean(\n",
    "        Kin_flat[0],\n",
    "        Kcross_flat[0].swapaxes(-2, -1),\n",
    "        nn_targets_flat[0],\n",
    "#         noise=1e-15,\n",
    "    ))\n",
    "    mean_an = np.squeeze(conventional_mean(\n",
    "        Kin_an,\n",
    "        Kcross_an,\n",
    "        nn_targets_flat[0],\n",
    "#         noise=1e-15,\n",
    "    ))\n",
    "    \n",
    "    print(\"----------\")\n",
    "    print(\"----------\")\n",
    "    print(f\"mean.shape = {mean.shape}\")\n",
    "    print(f\"mean_flat.shape = {mean_flat.shape}\")\n",
    "    print(f\"mean_an.shape = {mean_an.shape}\")\n",
    "    print(\"----------\")\n",
    "    print(f\"mean_flat == mean_an? {np.allclose(mean_flat, mean_an)}\")\n",
    "    print(f\"mean[0] == mean_flat? {np.allclose(mean[0], mean_flat)}\")\n",
    "    print(f\"mean[0] == mean_an? {np.allclose(mean[0], mean_an)}\")\n",
    "    \n",
    "    print(\"----------\")\n",
    "    print(\"----------\")\n",
    "    print(mean[0])\n",
    "    print(posterior_mean_analytic[:, 0])\n",
    "    print(posterior_mean_flat[:, 0])\n",
    "    \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the MuyGPs posterior mean.\n",
    "If `nn_count == train_count`, the results should agree with the analytic/flat solutions.\n",
    "Smaller `nn_count`s will drift (as expected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mean_muygps = muygps_mean_workflow(nn_count=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check numerically if things are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(posterior_mean_analytic, posterior_mean_muygps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the mean error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(posterior_mean_analytic - posterior_mean_muygps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posterior_mean_analytic.shape, posterior_mean_muygps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the `MuyGPyS` predictions to the conventional GP predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_predictions(test_targets, posterior_mean_muygps, posterior_mean_analytic, \"MuyGPyS\", \"Analytic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior Variance Tests\n",
    "\n",
    "Want to do similar analysis as above for the \"conventional\" posterior variance, which is defined as\n",
    "$$ \\mathrm{Var}(\\hat{Y}(X^*|X)) = K_{\\theta}(X^*,X^*) - K_{\\theta}(X^*,X)[K_{\\theta}(X,X) - \\epsilon]^{-1}K_{\\theta}(X,X^*) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conventional_variance(Kin, Kcross, Kin_test, noise=noise_prior):\n",
    "    nugget_size = Kin.shape[0]\n",
    "#    test_count = int(Kcross.shape[0] / 3)\n",
    "    return ( \n",
    "            Kin_test - Kcross @ np.linalg.solve(\n",
    "            Kin + noise * np.eye(nugget_size),\n",
    "            Kcross.T,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the `K*` tensors.\n",
    "Using the `train` and `test` tensors from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_an = original_shear(\n",
    "    train_features,\n",
    "    length_scale=length_scale,\n",
    ")\n",
    "Kcross_an = original_shear(\n",
    "    test_features,\n",
    "    train_features,\n",
    "    length_scale=length_scale,\n",
    ")\n",
    "# Construct the tensors K(X*,X*) and K(X,X*), \n",
    "# although not sure that that the explicit K(X,X*)\n",
    "# is necessary\n",
    "Kin_test_an = original_shear(\n",
    "    test_features,  \n",
    "    length_scale=length_scale\n",
    "    )\n",
    "\n",
    "print(\"----------\")\n",
    "print(\"Kin_an.shape = \", Kin_an.shape)\n",
    "print(\"Kcross_an.shape = \", Kcross_an.shape)\n",
    "print(\"Kin_test_an.shape = \", Kin_test_an.shape)\n",
    "print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_count = 50\n",
    "#crosswise_diffs, pairwise_diffs, x0_features, x0_nn_features, nn_targets = get_nn_tensors(nn_count=nn_count)\n",
    "\n",
    "pairwise_diffs = _pairwise_differences(train_features)\n",
    "crosswise_diffs = _crosswise_differences(test_features, train_features)\n",
    "# test diffs\n",
    "pairwise_diffs_test = _pairwise_differences(test_features)\n",
    "\n",
    "\n",
    "Kin_test_muygps = shear_model.kernel(pairwise_diffs_test, adjust=False)\n",
    "Kin_muygps = shear_model.kernel(pairwise_diffs, adjust=False)\n",
    "Kcross_muygps = shear_model.kernel(crosswise_diffs, adjust=False)\n",
    "\n",
    "Kin_muygps_flat = Kin_muygps.reshape(3 * train_count, 3 * train_count)\n",
    "Kcross_muygps_flat = Kcross_muygps.reshape(3 * test_count, 3 * train_count)\n",
    "Kin_test_muygps_flat = Kin_test_muygps.reshape(3 * test_count, 3 * test_count)\n",
    "\n",
    "print(\"----------\")\n",
    "print(\"Kin_muygps.shape = \", Kin_muygps_flat.shape)\n",
    "print(\"Kcross_muygps.shape = \", Kcross_muygps_flat.shape)\n",
    "print(\"Kin_test_muygps.shape = \", Kin_test_muygps_flat.shape)\n",
    "print(\"----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conventional_var_analytic_flat = conventional_variance(\n",
    "        Kin_an, \n",
    "        Kcross_an, \n",
    "        Kin_test_an,\n",
    "    )\n",
    "conventional_var_muygps_flat = conventional_variance(\n",
    "        Kin_muygps_flat, \n",
    "        Kcross_muygps_flat, \n",
    "        Kin_test_muygps_flat,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = np.abs(conventional_var_analytic_flat-conventional_var_muygps_flat)\n",
    "fig, ax = plt.subplots(1,3,figsize = (14,4))\n",
    "\n",
    "ax[0].imshow(conventional_var_analytic_flat)\n",
    "ax[0].set_title(\"Analytic Variance\")\n",
    "\n",
    "ax[1].imshow(conventional_var_muygps_flat)\n",
    "ax[1].set_title(\"Flat MuyGPyS Variance\")\n",
    "\n",
    "ax[2].set_title(\"|Analytic - Flat MuyGPyS|\")\n",
    "im = ax[2].imshow(residual)\n",
    "fig.colorbar(im, ax = ax[2])\n",
    "\n",
    "print(\"Min Resid = \", np.min(residual), \", Max Resid = \", np.max(residual), \", Avg Residual = \", np.mean(residual))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to unflatten the variances and extract the diagonals from each \"block\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conventional_var_analytic_unflat = np.zeros((3,500,3,500))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        conventional_var_analytic_unflat[i,:,j,:] = conventional_var_analytic_flat[500*i:500*(i+1), 500*j:500*(j+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure things are correct and that reshaping is consistent.\n",
    "First, construct a `posterior_var` in the analytic framework that should be the analog of `shear_model.posterior_variance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_var_an = np.zeros((500,3,3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        posterior_var_an[:,i,j] = np.diagonal(conventional_var_analytic_unflat[i,:,j,:])\n",
    "\n",
    "print(posterior_var_an.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the filled block version\n",
    "block_posterior_var_an = np.zeros((3,500,3,500))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        block_posterior_var_an[i, :, j, :] = (np.diag(posterior_var_an[:,i,j]))\n",
    "print(block_posterior_var_an.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonals in each block of the residual *should* be zero if everything was reshaped consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize = (14,4))\n",
    "\n",
    "ax[0].imshow(conventional_var_analytic_flat, norm = LogNorm())\n",
    "ax[0].set_title(\"Flat Analytic\")\n",
    "\n",
    "ax[1].imshow(block_posterior_var_an.reshape(1500,1500), norm = LogNorm())\n",
    "ax[1].set_title(\"Block Posterior Var An\")\n",
    "\n",
    "resid = np.abs(conventional_var_analytic_flat-block_posterior_var_an.reshape(1500,1500))\n",
    "print(\"----------\")\n",
    "print(\"Min Resid = \", np.min(resid))\n",
    "print(\"----------\")\n",
    "im = ax[2].imshow(resid, norm = LogNorm())\n",
    "fig.colorbar(im, ax = ax[2])\n",
    "ax[2].set_title(\"Residual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_diags = np.zeros((500,3,3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        resid_diags[:,i,j] = np.diagonal(resid[500*i:500*(i+1), 500*j:500*(j+1)])\n",
    "\n",
    "print(\n",
    "    \"Min=\", np.min(resid_diags), \n",
    "    \", Max=\",\n",
    "    np.max(resid_diags), \n",
    "    \", Avg=\",\n",
    "    np.mean(resid_diags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are very tiny. \n",
    "Is this just numerical junk that I can consider acceptably close to zero (ie, that the diagonals have been properly extracted)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the analytic versus the `MuyGPS.posterior_variance`. \n",
    "Create similar function to the above `muygps_mean_workflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def muygps_variance_workflow(nn_count = 50):\n",
    "    \n",
    "    train_features = features[train_mask, :]\n",
    "    test_features = features[test_mask, :]\n",
    "\n",
    "    # analytic version for comparison\n",
    "    Kin_an = original_shear(\n",
    "        train_features,\n",
    "        length_scale=length_scale,\n",
    "    )\n",
    "    Kcross_an = original_shear(\n",
    "        test_features,\n",
    "        train_features,\n",
    "        length_scale=length_scale,\n",
    "    )\n",
    "    # Construct the tensors K(X*,X*) and K(X,X*), \n",
    "    # although not sure that that the explicit K(X,X*)\n",
    "    # is necessary\n",
    "    Kin_test_an = original_shear(\n",
    "        test_features,  \n",
    "        length_scale=length_scale\n",
    "        )\n",
    "    \n",
    "    print(\"----------\")\n",
    "    print(\"Kin_an.shape = \", Kin_an.shape)\n",
    "    print(\"Kcross_an.shape = \", Kcross_an.shape)\n",
    "    print(\"Kin_test_an.shape = \", Kin_test_an.shape)\n",
    "    print(\"----------\")\n",
    "\n",
    "    conventional_var_analytic_flat = conventional_variance(\n",
    "        Kin_an, \n",
    "        Kcross_an, \n",
    "        Kin_test_an,\n",
    "    )\n",
    "\n",
    "    posterior_var_an = np.zeros((500,3,3))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            posterior_var_an[:,i,j] = np.diagonal(conventional_var_analytic_flat[500*i:500*(i+1), 500*j:500*(j+1)])\n",
    "\n",
    "    \n",
    "    #---------------------------\n",
    "    # MuyGPyS version\n",
    "    #---------------------------\n",
    "\n",
    "    # Construct the same matrices as in the mean workflow\n",
    "    crosswise_diffs, pairwise_diffs, _, _, _ = get_nn_tensors(nn_count=nn_count)\n",
    "\n",
    "    Kcross_muygps = shear_model.kernel(crosswise_diffs)\n",
    "    Kin_muygps = shear_model.kernel(pairwise_diffs)\n",
    "\n",
    "    \n",
    "    print(\"----------\")\n",
    "    print(\"Kin_muygps.shape = \", Kin_muygps.shape)\n",
    "    print(\"Kcross_muygps.shape = \", Kcross_muygps.shape)\n",
    "\n",
    "\n",
    "    posterior_var_muygps = shear_model.posterior_variance(Kin_muygps, Kcross_muygps)\n",
    "\n",
    "    print(\"----------\")\n",
    "    print(\"----------\")  \n",
    "    print(\"muygps_variance.shape = \", posterior_var_muygps.shape)\n",
    "    print(\"analytic_variance.shape = \", posterior_var_an.shape)\n",
    "\n",
    "    print(\"----------\")\n",
    "    print(\"----------\")\n",
    "    print(\"muygps and analytic close?\", np.allclose(posterior_var_an, posterior_var_muygps))\n",
    "    \n",
    "\n",
    "    var_residual = np.abs(posterior_var_an - posterior_var_muygps)\n",
    "    print(\"----------\")\n",
    "    print(\"Min Resid = \", np.min(var_residual), \", Max Resid = \", np.max(var_residual), \", Avg Residual = \", np.mean(var_residual))\n",
    "    print(\"----------\")\n",
    "\n",
    "    return( posterior_var_muygps )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_var_muygps = muygps_variance_workflow(nn_count=train_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the `posterior_var_muygps` to the `block_diag_var_an`.\n",
    "Initially looks like the residuals vary dramatically, but the shapes are the same for the `analytic` and `muygps` variance. \n",
    "Check visually if things are organized in a consistent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to diagonal matrices\n",
    "block_posterior_var_muygps = np.zeros((3,500,3,500))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        block_posterior_var_muygps[i,:,j,:] = np.diag(posterior_var_muygps[:,i,j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize = (14,4))\n",
    "\n",
    "ax[0].imshow(block_posterior_var_an.reshape(1500,1500), norm = LogNorm())\n",
    "ax[0].set_title(\"Analytic\")\n",
    "\n",
    "ax[1].imshow(block_posterior_var_muygps.reshape(1500,1500), norm = LogNorm())\n",
    "ax[1].set_title(\"MuyGPyS\")\n",
    "\n",
    "var_residual = np.abs(block_posterior_var_an.reshape(1500,1500)-block_posterior_var_muygps.reshape(1500,1500))\n",
    "print(\"----------\")\n",
    "print(\"Max Resid = \", np.max(var_residual))\n",
    "print(\"----------\")\n",
    "im = ax[2].imshow(var_residual, norm = LogNorm())\n",
    "fig.colorbar(im, ax = ax[2])\n",
    "ax[2].set_title(\"Residual\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Test\n",
    "\n",
    "The next step is to test the optimizer on the mock data generated above.\n",
    "Recall that the mock data are sampled with a GP given a `length_scale`, which means that if we run hyperparameter optimization, we *should* recover this length scale if the optimizer is working properly.\n",
    "\n",
    "First, create the train tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.gp.tensors import crosswise_tensor, pairwise_tensor\n",
    "from MuyGPyS.optimize.batch import sample_batch\n",
    "from MuyGPyS.optimize import Bayes_optimize\n",
    "from MuyGPyS.optimize.loss import lool_fn, looph_fn, mse_fn\n",
    "from MuyGPyS.gp.hyperparameter import AnalyticScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_targets.shape, train_features.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_targets = targets.swapaxes(0,1)\n",
    "train_features = features\n",
    "print(train_features.shape, train_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shear_model = MuyGPS(\n",
    "    kernel=ShearKernel(\n",
    "        deformation=Isotropy(\n",
    "            F2,\n",
    "            length_scale=Parameter(0.5, [0.3, 0.7]), # this is the raw length scale I think\n",
    "        ),\n",
    "    ),\n",
    "    noise=HomoscedasticNoise(1e-7),\n",
    "    scale=AnalyticScale(),\n",
    ")\n",
    "\n",
    "train_features_count = train_features.shape[0]\n",
    "\n",
    "nn_count = 50\n",
    "nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method='exact', algorithm='ball_tree')\n",
    "    \n",
    "\n",
    "batch_count=500\n",
    "batch_indices, batch_nn_indices = sample_batch(\n",
    "    nbrs_lookup, batch_count, train_features_count\n",
    ")\n",
    "\n",
    "# need pairwise and crosswise diffs\n",
    "batch_crosswise_diffs = crosswise_tensor(\n",
    "    train_features,\n",
    "    train_features,\n",
    "    batch_indices,\n",
    "    batch_nn_indices,\n",
    ")\n",
    "\n",
    "batch_pairwise_diffs = pairwise_tensor(\n",
    "    train_features, batch_nn_indices\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_targets = train_targets[batch_indices]\n",
    "\n",
    "batch_nn_targets= train_targets[batch_nn_indices].swapaxes(-2, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if the optimization correctly predicts the length scale with the `mse` loss fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shear_mse_optimized = Bayes_optimize(\n",
    "    shear_model,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    train_targets,\n",
    "    loss_fn=mse_fn,\n",
    "    verbose=True,\n",
    "    init_points=5,\n",
    "    n_iter=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_count = test_features.shape[0]\n",
    "\n",
    "indices = np.arange(test_features_count)\n",
    "test_nn_indices, _ = nbrs_lookup.get_nns(test_features)\n",
    "\n",
    "(\n",
    "    test_crosswise_diffs,\n",
    "    test_pairwise_diffs,\n",
    "    test_nn_targets,\n",
    ") = make_predict_tensors(\n",
    "    indices,\n",
    "    test_nn_indices,\n",
    "    test_features,\n",
    "    train_features,\n",
    "    train_targets,\n",
    ")\n",
    "\n",
    "test_nn_targets= test_nn_targets.swapaxes(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross = shear_mse_optimized.kernel(test_crosswise_diffs)\n",
    "Kin = shear_mse_optimized.kernel(test_pairwise_diffs)\n",
    "posterior_mean_muygps_optimized = shear_mse_optimized.posterior_mean(Kin, Kcross, test_nn_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_predictions(test_targets, posterior_mean_muygps_optimized, posterior_mean_analytic, \"Optimized MuyGPyS\", \"Analytic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
