{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2023-2023 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shear Kernel Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the specialized lensing shear kernel (hard-coded to RBF at the moment).\n",
    "\n",
    "⚠️ _Note that this is still an experimental feature._ ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "from MuyGPyS._src.gp.tensors import _crosswise_differences, _pairwise_differences\n",
    "from MuyGPyS.gp import MuyGPS\n",
    "from MuyGPyS.gp.deformation import Isotropy, l2, F2\n",
    "from MuyGPyS.gp.hyperparameter import Parameter\n",
    "from MuyGPyS.gp.kernels.experimental import ShearKernel\n",
    "from MuyGPyS.gp.tensors import make_predict_tensors\n",
    "from MuyGPyS.neighbors import NN_Wrapper\n",
    "from MuyGPyS.gp.noise import HomoscedasticNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is required to import the implementation from Bob Armstrong's original repository.\n",
    "Must set `shear_kernel_dir = \"path/to/local/shear_kernel/\"` for things to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "import torch\n",
    "# introduce a variable for path/to/shear_kernel\n",
    "shear_kernel_dir = \"../../../projects/shear_kernel/\"\n",
    "spec_analytic = importlib.util.spec_from_file_location(\"analytic_kernel\", shear_kernel_dir + \"analytic_kernel.py\") \n",
    "bar = importlib.util.module_from_spec(spec_analytic)\n",
    "sys.modules[\"analytic_kernel\"] = bar\n",
    "spec_analytic.loader.exec_module(bar)\n",
    "from analytic_kernel import shear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set a random seed here for consistency when building docs.\n",
    "In practice we would not fix a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build some simple data, which is mean to represent a grid of sky coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25  # number of galaxies on a side\n",
    "xmin = 0\n",
    "xmax = 1\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "\n",
    "xx = np.linspace(xmin, xmax, n)\n",
    "yy = np.linspace(ymin, ymax, n)\n",
    "\n",
    "x, y = np.meshgrid(xx, yy)\n",
    "features = np.vstack((x.flatten(), y.flatten())).T\n",
    "data_count = features.shape[0]\n",
    "diffs = _pairwise_differences(features)\n",
    "length_scale = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use an Isotropic distance functor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_fn = Isotropy(\n",
    "    metric=F2,\n",
    "    length_scale=Parameter(length_scale),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct a shear value kernel (partial differential components of RBF), as well as the original RBF kernel using Bob's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_shear(X1, X2=None, length_scale=1.0):\n",
    "    if X2 is None:\n",
    "        X2 = X1\n",
    "    n1, _ = X1.shape\n",
    "    n2, _ = X2.shape\n",
    "    vals = np.zeros((3 * (n1), 3 * (n2)))\n",
    "    vals[:] = np.nan\n",
    "    for i, (ix, iy) in enumerate(X1):\n",
    "        for j, (jx, jy) in enumerate(X2):\n",
    "            tmp = shear_kernel(ix, iy, jx, jy, b=length_scale)\n",
    "            for a in range(3):\n",
    "                for b in range(3):\n",
    "                    vals[(a * n1) + i, (b * n2) + j] = tmp[a, b]\n",
    "            \n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_analytic = original_shear(features, length_scale=length_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the same using the MuyGPyS implementation. Note the increased efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_muygps = ShearKernel(deformation=dist_fn)(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Kin_muygps` is a more generalized tensor, so we need to flatten it to a conforming shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kin_muygps_flat = Kin_muygps.reshape(data_count * 3, data_count * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Kin_analytic.shape = {Kin_analytic.shape}\")\n",
    "print(f\"Kin_muygps.shape = {Kin_muygps.shape}\")\n",
    "print(f\"Kin_muygps_flat.shape = {Kin_muygps_flat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the two implementations agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.allclose(Kin_analytic, Kin_muygps_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = np.abs(Kin_analytic - Kin_muygps_flat)\n",
    "print(f\"residual max: {np.max(residual)}, min: {np.min(residual)}, mean : {np.mean(residual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results of the baseline and internal implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].set_title(\"original shear kernel\")\n",
    "axes[0].imshow(Kin_analytic)\n",
    "axes[1].set_title(\"MuyGPyS shear kernel\")\n",
    "axes[1].imshow(Kin_muygps_flat)\n",
    "axes[2].set_title(\"Residual\")\n",
    "im = axes[2].imshow(residual, norm=LogNorm())\n",
    "fig.colorbar(im, ax=axes[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform a similar analysis for the cross-covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 200\n",
    "X1 = features[:split]\n",
    "X2 = features[split:]\n",
    "n1, _ = X1.shape\n",
    "n2, _ = X2.shape\n",
    "crosswise_diffs = _crosswise_differences(X1, X2)\n",
    "print(X1.shape, X2.shape, crosswise_diffs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_analytic = original_shear(X1, X2, length_scale=length_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_muygps = ShearKernel(deformation=dist_fn)(crosswise_diffs, adjust=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_muygps_flat = Kcross_muygps.reshape(n1 * 3, n2 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Kcross_analytic.shape = {Kcross_analytic.shape}\")\n",
    "print(f\"Kcross_muygps.shape = {Kcross_muygps.shape}\")\n",
    "print(f\"Kcross_muygps_flat.shape = {Kcross_muygps_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(Kcross_analytic, Kcross_muygps_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_residual = np.abs(Kcross_analytic - Kcross_muygps_flat)\n",
    "print(f\"residual max: {np.max(cross_residual)}, min: {np.min(cross_residual)}, mean : {np.mean(cross_residual)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].set_title(\"original shear kernel\")\n",
    "axes[0].imshow(Kcross_analytic)\n",
    "axes[1].set_title(\"MuyGPyS shear kernel\")\n",
    "axes[1].imshow(Kcross_muygps_flat)\n",
    "axes[2].set_title(\"Residual\")\n",
    "im = axes[2].imshow(cross_residual, norm=LogNorm())\n",
    "fig.colorbar(im, ax=axes[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime comparison of the two implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    %timeit original_shear(features)\n",
    "    %timeit ShearKernel(deformation=dist_fn)(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test the `posterior_mean` of the analytic and muygps implementations.\n",
    "First, we set up an arbitrary taget array.\n",
    "Targets should be square matrices like a grid of a swath of sky.\n",
    "Ulitimately, the target array will have shape `(625,3)`, given `n=25` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_grid_x, targets_grid_y = np.meshgrid(\n",
    "        np.linspace(1, 10, n),\n",
    "        np.linspace(1, 10, n),\n",
    "        indexing = 'ij'\n",
    "    )\n",
    "targets_grid = targets_grid_x * targets_grid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.vstack(\n",
    "    (targets_grid.flatten(), np.rot90(targets_grid, k=3).flatten(), np.rot90(targets_grid).flatten()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.hstack(targets).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data to a unit hypercube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_norm = (targets - np.min(targets)) / (np.max(targets) - np.min(targets))\n",
    "print(np.min(targets_norm), np.max(targets_norm), targets_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1,figsize = (4,12))\n",
    "ax[0].imshow(targets[0,:].reshape(n,n))\n",
    "ax[0].set_ylabel('$\\kappa$', fontsize = 15)\n",
    "ax[1].imshow(targets[1,:].reshape(n,n))\n",
    "ax[1].set_ylabel(\"g1\", fontsize = 15)\n",
    "ax[2].imshow(targets[2,:].reshape(n,n))\n",
    "ax[2].set_ylabel(\"g2\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicitly define the target matrices.\n",
    "Also add leading unitary dimension to `targets_muygpys` for things to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_analytic = targets_norm\n",
    "targets_muygps = targets_norm[None,:]\n",
    "print(targets_analytic.shape, targets_muygps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the `K*` matrices above, compare posterior means, sans-optimization. \n",
    "This comes from (3.4) in Muyskens et al. (2021), which looks like:\n",
    "$$ \\hat{Y}(X^*|X) = K_{\\theta}(X^*,X)(K_{\\theta}(X,X)+\\epsilon)^{-1}Y(X) $$\n",
    "where $\\epsilon = \\sigma^2 I$ with $\\sigma^2$ being the noise variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytic: for the analytic implementation, I'll do things with the full \"flattened\" difference tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_analytic_full = original_shear(features, features, length_scale=length_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Kcross_analytic_full.shape, Kin_analytic.shape, targets_analytic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mean_analytic = np.matmul(\n",
    "    np.matmul(Kcross_analytic_full,np.linalg.inv(Kin_analytic + 1e-4 * np.identity(n**2 * 3))), \n",
    "    np.hstack(targets_analytic)\n",
    "    ).reshape(3, n**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mean_analytic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MuyGPyS implementation using nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create MuyGPS object\n",
    "shear_model = MuyGPS(\n",
    "        kernel=ShearKernel(\n",
    "            deformation=Isotropy(\n",
    "                F2,\n",
    "                length_scale=Parameter(length_scale),\n",
    "            ),\n",
    "        ),\n",
    "        noise = HomoscedasticNoise(1e-4)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_count = 50\n",
    "nbrs_lookup = NN_Wrapper(features, nn_count, nn_method='exact', algorithm='ball_tree')\n",
    "\n",
    "features_count = features.shape[0]\n",
    "\n",
    "indices = np.arange(features_count)\n",
    "test_nn_indices, _ = nbrs_lookup.get_nns(features)\n",
    "\n",
    "(\n",
    "    crosswise_diffs,\n",
    "    pairwise_diffs,\n",
    "    nn_targets,\n",
    ") = make_predict_tensors(\n",
    "    indices,\n",
    "    test_nn_indices,\n",
    "    features,\n",
    "    features,\n",
    "    targets_norm.swapaxes(0,1),\n",
    ")\n",
    "\n",
    "nn_targets= nn_targets.swapaxes(-2, -1)\n",
    "\n",
    "Kcross = shear_model.kernel(crosswise_diffs)\n",
    "K = shear_model.kernel(pairwise_diffs)\n",
    "\n",
    "posterior_mean_muygps = shear_model.posterior_mean(K, Kcross, nn_targets).swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check numerically if things are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(posterior_mean_analytic, posterior_mean_muygps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_residual = np.abs(posterior_mean_muygps - posterior_mean_analytic)\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize = (10, 10))\n",
    "\n",
    "ax[0,0].set_title(\"MuyGPyS Posterior Mean\")\n",
    "ax[0,1].set_title(\"Analytic Posterior Mean\")\n",
    "ax[0,2].set_title(\"Residual\")\n",
    "ax[0,0].set_ylabel('$\\kappa$', fontsize = 15)\n",
    "ax[1,0].set_ylabel(\"g1\", fontsize = 15)\n",
    "ax[2,0].set_ylabel(\"g2\", fontsize = 15)\n",
    "\n",
    "ax[0,0].imshow(posterior_mean_muygps[0,:].reshape(n,n))\n",
    "ax[1,0].imshow(posterior_mean_muygps[1,:].reshape(n,n))\n",
    "ax[2,0].imshow(posterior_mean_muygps[2,:].reshape(n,n))\n",
    "\n",
    "ax[0,1].imshow(posterior_mean_analytic[0,:].reshape(n,n))\n",
    "ax[1,1].imshow(posterior_mean_analytic[1,:].reshape(n,n))\n",
    "ax[2,1].imshow(posterior_mean_analytic[2,:].reshape(n,n))\n",
    "\n",
    "\n",
    "im1 = ax[0,2].imshow(posterior_residual[0,:].reshape(n,n), norm=LogNorm())\n",
    "fig.colorbar(im1, ax=ax[0,2])\n",
    "im2 = ax[1,2].imshow(posterior_residual[1,:].reshape(n,n), norm=LogNorm())\n",
    "fig.colorbar(im2, ax=ax[1,2])\n",
    "im3 = ax[2,2].imshow(posterior_residual[2,:].reshape(n,n), norm=LogNorm())\n",
    "fig.colorbar(im3, ax=ax[2,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual is absolutely wack. \n",
    "Check if the MuyGPyS is even close to the initial targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_residual = np.abs(posterior_mean_muygps - targets_norm)\n",
    "print(muygps_residual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,3, figsize = (10, 10))\n",
    "\n",
    "ax[0,0].set_title(\"Truth\")\n",
    "ax[0,1].set_title(\"MuyGPyS Posterior Mean\")\n",
    "ax[0,2].set_title(\"Residual\")\n",
    "ax[0,0].set_ylabel('$\\kappa$', fontsize = 15)\n",
    "ax[1,0].set_ylabel(\"g1\", fontsize = 15)\n",
    "ax[2,0].set_ylabel(\"g2\", fontsize = 15)\n",
    "\n",
    "ax[0,0].imshow(targets_norm[0,:].reshape(n,n))\n",
    "ax[1,0].imshow(targets_norm[1,:].reshape(n,n))\n",
    "ax[2,0].imshow(targets_norm[2,:].reshape(n,n))\n",
    "\n",
    "\n",
    "ax[0,1].imshow(posterior_mean_muygps[0,:].reshape(n,n))\n",
    "ax[1,1].imshow(posterior_mean_muygps[1,:].reshape(n,n))\n",
    "ax[2,1].imshow(posterior_mean_muygps[2,:].reshape(n,n))\n",
    "\n",
    "\n",
    "\n",
    "im1 = ax[0,2].imshow(muygps_residual[0,:].reshape(n,n))\n",
    "fig.colorbar(im1, ax=ax[0,2])\n",
    "im2 = ax[1,2].imshow(muygps_residual[1,:].reshape(n,n), norm = LogNorm())\n",
    "fig.colorbar(im2, ax=ax[1,2])\n",
    "im3 = ax[2,2].imshow(muygps_residual[2,:].reshape(n,n), norm = LogNorm())\n",
    "fig.colorbar(im3, ax=ax[2,2])\n",
    "plt.show()\n",
    "\n",
    "print(\"Are the predictions close? \", np.allclose(targets_norm, posterior_mean_muygps), f\" min: {np.min(muygps_residual)}, max: {np.max(muygps_residual)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(posterior_mean_muygps[0,], targets_norm[0,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kappa prediction looks good, but the other predictions... don't.\n",
    "How do the analytic ones look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic_residual = np.abs(posterior_mean_analytic - targets_norm)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize = (10, 10))\n",
    "\n",
    "ax[0,0].set_title(\"Truth\")\n",
    "ax[0,1].set_title(\"Analytic Posterior Mean\")\n",
    "ax[0,2].set_title(\"Residual\")\n",
    "ax[0,0].set_ylabel('$\\kappa$', fontsize = 15)\n",
    "ax[1,0].set_ylabel(\"g1\", fontsize = 15)\n",
    "ax[2,0].set_ylabel(\"g2\", fontsize = 15)\n",
    "\n",
    "ax[0,0].imshow(targets_norm[0,:].reshape(n,n))\n",
    "ax[1,0].imshow(targets_norm[1,:].reshape(n,n))\n",
    "ax[2,0].imshow(targets_norm[2,:].reshape(n,n))\n",
    "\n",
    "\n",
    "ax[0,1].imshow(posterior_mean_analytic[0,:].reshape(n,n))\n",
    "ax[1,1].imshow(posterior_mean_analytic[1,:].reshape(n,n))\n",
    "ax[2,1].imshow(posterior_mean_analytic[2,:].reshape(n,n))\n",
    "\n",
    "\n",
    "\n",
    "im1 = ax[0,2].imshow(analytic_residual[0,:].reshape(n,n), norm = LogNorm())\n",
    "fig.colorbar(im1, ax=ax[0,2])\n",
    "im2 = ax[1,2].imshow(analytic_residual[1,:].reshape(n,n), norm = LogNorm())\n",
    "fig.colorbar(im2, ax=ax[1,2])\n",
    "im3 = ax[2,2].imshow(analytic_residual[2,:].reshape(n,n), norm = LogNorm())\n",
    "fig.colorbar(im3, ax=ax[2,2])\n",
    "plt.show()\n",
    "\n",
    "print(\"Are the predictions close? \", np.allclose(targets_norm, posterior_mean_analytic), f\" min: {np.min(analytic_residual)}, max: {np.max(analytic_residual)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
