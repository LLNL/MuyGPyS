{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2023-2023 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonstationary tutorial\n",
    "\n",
    "This notebook demonstrates how to use hierarchical nonstationary hyperparameters to perform nonstationary regression using a hierarchical model.\n",
    "\n",
    "⚠️ _Note that this is still an experimental feature at this point._ ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from MuyGPyS.gp import MuyGPS\n",
    "from MuyGPyS.gp.deformation import Isotropy, l2, F2\n",
    "from MuyGPyS.gp.hyperparameter import Parameter, VectorParameter\n",
    "from MuyGPyS.gp.hyperparameter.experimental import (\n",
    "    sample_knots,\n",
    "    HierarchicalParameter,\n",
    ")\n",
    "from MuyGPyS.gp.kernels import RBF\n",
    "from MuyGPyS.gp.noise import HomoscedasticNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set a random seed here for consistency when building docs.\n",
    "In practice we would not fix a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we start with an isotropic distortion so we only need to use a single `HierarchicalNonstationaryHyperparameter`.\n",
    "Let's also build a GP with a fixed length scale for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some training data with a little bit of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_max = 5\n",
    "data_count = 500\n",
    "train_step = 10\n",
    "train_count = int(data_count / train_step)\n",
    "test_count = data_count - train_count\n",
    "noise_prior = 1e-5\n",
    "noise_actual = 2e-4\n",
    "xs = np.linspace(-data_max, data_max, num=data_count)\n",
    "ys = np.sinc(xs) - np.mean(np.sinc(xs))\n",
    "xs = (xs - np.min(xs)) / (2 * np.max(xs))\n",
    "train_features = xs[::train_step]\n",
    "train_responses = ys[::train_step] + np.random.normal(scale=noise_actual, size=train_count)\n",
    "test_features = xs[np.mod(np.arange(data_count), train_step) != 0]\n",
    "test_responses = ys[np.mod(np.arange(data_count), train_step) != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the true function we are trying to predict, along with the training data with which will optimize a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "ax.plot(xs, ys, label=\"True Response\")\n",
    "ax.plot(train_features, train_responses, '.', label=\"Training Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a stationary MuyGPs object for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_fixed = MuyGPS(\n",
    "    kernel=RBF(\n",
    "        deformation=Isotropy(\n",
    "            l2,\n",
    "            length_scale=Parameter(0.5),\n",
    "        ),\n",
    "    ),\n",
    "    noise=HomoscedasticNoise(noise_prior),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Nonstationary MuyGPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create a hierarchical nonstationary MuyGPs object, where we assume that the `length_scale` of the distance function itself varies according to a Gaussian process with some \"knots\", locations in the range of the function where we assume that we know or can learn the true value of the `length_scale`. We will start by sampling some knots and giving them initial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot_count = 6\n",
    "knot_features = np.squeeze(sample_knots(feature_count=1, knot_count=knot_count))\n",
    "knot_features = np.array(sorted(knot_features))\n",
    "knot_values = VectorParameter(\n",
    "    Parameter(0.3),\n",
    "    Parameter(0.4),\n",
    "    Parameter(0.5),\n",
    "    Parameter(0.5),\n",
    "    Parameter(0.4),\n",
    "    Parameter(0.3),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a `MuyGPS` object like before, except now we specify that the `length_scale` is hierarchical and pass the knots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knot_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knot_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_level_kernel = RBF(\n",
    "    deformation=Isotropy(\n",
    "        F2,\n",
    "        length_scale=Parameter(0.5))\n",
    ")\n",
    "\n",
    "muygps = MuyGPS(\n",
    "    kernel=RBF(\n",
    "        deformation=Isotropy(\n",
    "            l2,\n",
    "            length_scale=HierarchicalParameter(\n",
    "                knot_features, knot_values, high_level_kernel\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    noise=HomoscedasticNoise(noise_prior),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the knots and the resulting `length_scale` surface over the domain of the function.\n",
    "Unlike `ScalarHyperparameter`, `HierarchicalNonstationaryHyperparameter` takes an array of feature vectors for each point where you would like to evaluate the local value of the hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale_curve = muygps.kernel.deformation.length_scale(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a small example, we can evaluate and display the predicted `length_scale` values across the whole domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "ax.set_title(\"Hierarchical Length Scale Surface Over the Domain\")\n",
    "order = np.argsort(knot_features)\n",
    "ax.plot(knot_features[order], knot_values()[order], \"*\", label=\"Knot Values\")\n",
    "ax.plot(xs, length_scale_curve, label=\"Interpolated Surface\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed as usual to generate the nearest neighbors lookup index and tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.neighbors import NN_Wrapper\n",
    "\n",
    "nn_count = 30\n",
    "nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method=\"exact\", algorithm=\"ball_tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this simple example we're using all of the data as batch points, i.e. we're not really batching, since the dataset is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = np.arange(test_count)\n",
    "test_nn_indices, _ = nbrs_lookup.get_nns(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    test_crosswise_dists,\n",
    "    test_pairwise_dists,\n",
    "    test_nn_targets,\n",
    ") = muygps.make_predict_tensors(\n",
    "    test_indices,\n",
    "    test_nn_indices,\n",
    "    test_features,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One notable difference when using a hierarchical model is that the kernel takes an additional tensor, the batch tensor, which can be easily obtained using the `batch_features_tensor` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.gp.tensors import batch_features_tensor\n",
    "\n",
    "batch_test_features = batch_features_tensor(test_features, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're ready to realize the kernel tensors and use them to predict the response of the test data. First using the GP with a fixed length scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_flat_fixed = muygps_fixed.kernel(test_crosswise_dists)\n",
    "Kin_flat_fixed = muygps_fixed.kernel(test_pairwise_dists)\n",
    "mean_flat_fixed = muygps_fixed.posterior_mean(\n",
    "    Kin_flat_fixed, Kcross_flat_fixed, test_nn_targets\n",
    ")\n",
    "var_flat_fixed = muygps_fixed.posterior_variance(\n",
    "    Kin_flat_fixed, Kcross_flat_fixed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the hierarchical GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_hierarchical_fixed = muygps.kernel(\n",
    "    test_crosswise_dists, batch_features=batch_test_features\n",
    ")\n",
    "Kin_hierarchical_fixed = muygps.kernel(\n",
    "    test_pairwise_dists, batch_features=batch_test_features\n",
    ")\n",
    "mean_hierarchical_fixed = muygps.posterior_mean(\n",
    "    Kin_hierarchical_fixed, Kcross_hierarchical_fixed, test_nn_targets\n",
    ")\n",
    "var_hierarchical_fixed = muygps.posterior_variance(\n",
    "    Kin_hierarchical_fixed, Kcross_hierarchical_fixed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize the results by plotting the predicted means as well as one predicted standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "ax.plot(xs, ys, label=\"truth\")\n",
    "ax.plot(test_features, mean_flat_fixed, \".-\", label=\"flat fixed\")\n",
    "ax.fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(mean_flat_fixed + np.sqrt(var_flat_fixed) * 1.96),\n",
    "    np.ravel(mean_flat_fixed - np.sqrt(var_flat_fixed) * 1.96),\n",
    "    facecolor=\"C1\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "ax.plot(test_features, mean_hierarchical_fixed, \"--\", label=\"hierarchical fixed\")\n",
    "ax.fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(mean_hierarchical_fixed + np.sqrt(var_hierarchical_fixed) * 1.96),\n",
    "    np.ravel(mean_hierarchical_fixed - np.sqrt(var_hierarchical_fixed) * 1.96),\n",
    "    facecolor=\"C2\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knot values of hierarchical nonstationary hyperparameters can be optimized using like any other hyperparameters, using the `optimize_from_tensors` utility. But first, we need to initialize them as `ScalarHyperparameter`s with bounds rather than as fixed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [0.1, 0.5]\n",
    "knot_values_to_be_optimized = VectorParameter(\n",
    "    *[Parameter(\"sample\", bounds) for _ in range(knot_count)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(knot_values_to_be_optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recreate a MuyGPs object. It's identical to the one we've created before except for the knot values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_to_be_optimized = MuyGPS(\n",
    "    kernel=RBF(\n",
    "        deformation=Isotropy(\n",
    "            l2,\n",
    "            length_scale=HierarchicalParameter(\n",
    "                knot_features, knot_values_to_be_optimized, high_level_kernel\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    noise=HomoscedasticNoise(noise_prior),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use `make_train_tensors` to obtain the training tensors. Once again, we use all the training data instead of batching it due to the small size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.arange(train_count)\n",
    "train_nn_indices, _ = nbrs_lookup.get_batch_nns(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    ") = hierarchical_to_be_optimized.make_train_tensors(\n",
    "    train_indices,\n",
    "    train_nn_indices,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need the prediction features (in this case, of the training batch) in order to evaluate the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_features = batch_features_tensor(train_features, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.optimize import Bayes_optimize\n",
    "from MuyGPyS.optimize.loss import lool_fn, mse_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_optimized = Bayes_optimize(\n",
    "    hierarchical_to_be_optimized,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    batch_features=batch_features,\n",
    "    loss_fn=mse_fn,\n",
    "    verbose=True,\n",
    "    random_state=1,\n",
    "    init_points=5,\n",
    "    n_iter=45,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use `optimize_from_tensors` to create a new MuyGPs object for which the knot values have been fixed to their optimal values. Note that we must pass the `batch_features` tensor, this time from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ _The next four cells are for testing purposes and should be removed prior to finalizing this tutorial._ ⚠️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized model has set new knot values, which we can use to visualize the learned `length_scale` surface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_optimized.kernel.deformation.length_scale.knot_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale_curve_optimized = hierarchical_optimized.kernel.deformation.length_scale(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "ax.set_title(\"Trained Hierarchical Length Scale Surface Over the Domain\")\n",
    "order = np.argsort(knot_features)\n",
    "ax.plot(\n",
    "    knot_features[order], \n",
    "    hierarchical_optimized.kernel.deformation.length_scale.knot_values()[order], \n",
    "    \"*\", \n",
    "    label=\"Knot Values\")\n",
    "ax.plot(xs, length_scale_curve_optimized, label=\"Interpolated Surface\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the optimized the kernel to predict the test responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_hierarchical_opt = hierarchical_optimized.kernel(test_crosswise_dists, batch_features=batch_test_features)\n",
    "Kin_hierarchical_opt = hierarchical_optimized.kernel(test_pairwise_dists, batch_features=batch_test_features)\n",
    "mean_hierarchical_opt = hierarchical_optimized.posterior_mean(\n",
    "    Kin_hierarchical_opt, Kcross_hierarchical_opt, test_nn_targets\n",
    ")\n",
    "var_hierarchical_opt = hierarchical_optimized.posterior_variance(\n",
    "    Kin_hierarchical_opt, Kcross_hierarchical_opt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also optimize a flat model with the same batch for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_to_be_optimized = MuyGPS(\n",
    "    kernel=RBF(\n",
    "        deformation=Isotropy(\n",
    "            l2,\n",
    "            length_scale=Parameter(\"sample\", [0.001, 5.0]),\n",
    "        ),\n",
    "    ),\n",
    "    noise=HomoscedasticNoise(noise_prior),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_optimized = Bayes_optimize(\n",
    "    flat_to_be_optimized,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    batch_features=batch_features,\n",
    "    loss_fn=mse_fn,\n",
    "    verbose=True,\n",
    "    random_state=1,\n",
    "    init_points=5,\n",
    "    n_iter=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_flat_opt = flat_optimized.kernel(test_crosswise_dists)\n",
    "Kin_flat_opt = flat_optimized.kernel(test_pairwise_dists)\n",
    "mean_flat_opt = flat_optimized.posterior_mean(\n",
    "    Kin_flat_opt, Kcross_flat_opt, test_nn_targets\n",
    ")\n",
    "var_flat_opt = flat_optimized.posterior_variance(\n",
    "    Kin_flat_opt, Kcross_flat_opt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for axi in axes:\n",
    "    for ax in axi:\n",
    "        ax.set_ylim([-1, 1.5])\n",
    "#         ax.plot(xs, ys, label=\"truth\")\n",
    "        for knot in knot_features:\n",
    "            ax.axvline(x=knot)\n",
    "axes[0, 0].set_title(\"flat fixed\")\n",
    "axes[0, 0].plot(test_features, mean_flat_fixed, \"-\", label=\"flat fixed\")\n",
    "axes[0, 0].fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(mean_flat_fixed + np.sqrt(var_flat_fixed) * 1.96),\n",
    "    np.ravel(mean_flat_fixed - np.sqrt(var_flat_fixed) * 1.96),\n",
    "    facecolor=\"C1\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "axes[0, 1].set_title(\"hierarchical fixed\")\n",
    "axes[0, 1].plot(test_features, mean_hierarchical_fixed, \"-\", label=\"hierarchical fixed\")\n",
    "axes[0, 1].fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(mean_hierarchical_fixed + np.sqrt(var_hierarchical_fixed) * 1.96),\n",
    "    np.ravel(mean_hierarchical_fixed - np.sqrt(var_hierarchical_fixed) * 1.96),\n",
    "    facecolor=\"C2\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "axes[1, 0].set_title(\"flat optimized\")\n",
    "axes[1, 0].plot(test_features, mean_flat_opt, \"-\", label=\"flat optimized\")\n",
    "axes[1, 0].fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(mean_flat_opt + np.sqrt(var_flat_opt) * 1.96),\n",
    "    np.ravel(mean_flat_opt - np.sqrt(var_flat_opt) * 1.96),\n",
    "    facecolor=\"C3\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "axes[1, 1].set_title(\"hierarchical optimized\")\n",
    "axes[1, 1].plot(test_features, mean_hierarchical_opt, \"-\", label=\"hierarchical optimized\")\n",
    "axes[1, 1].fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(mean_hierarchical_opt + np.sqrt(var_hierarchical_opt) * 1.96),\n",
    "    np.ravel(mean_hierarchical_opt - np.sqrt(var_hierarchical_opt) * 1.96),\n",
    "    facecolor=\"C3\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "for knot in knot_features:\n",
    "    ax.axvline(x=knot)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
