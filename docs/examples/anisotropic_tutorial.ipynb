{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021-2024 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anisotropic Metric Tutorial\n",
    "\n",
    "This notebook walks through a simple anisotropic regression workflow and illustrates anisotropic features of `MuyGPyS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "for m in sys.modules.keys():\n",
    "    if m.startswith(\"Muy\"):\n",
    "        sys.modules.pop(m)\n",
    "%env MUYGPYS_BACKEND=numpy\n",
    "%env MUYGPYS_FTYPE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from MuyGPyS._test.sampler import UnivariateSampler2D, print_results\n",
    "from MuyGPyS.gp import MuyGPS\n",
    "from MuyGPyS.gp.deformation import Anisotropy, Isotropy, l2\n",
    "from MuyGPyS.gp.hyperparameter import AnalyticScale, Parameter, VectorParameter\n",
    "from MuyGPyS.gp.kernels import Matern\n",
    "from MuyGPyS.gp.noise import HomoscedasticNoise\n",
    "from MuyGPyS.neighbors import NN_Wrapper\n",
    "from MuyGPyS.optimize import Bayes_optimize\n",
    "from MuyGPyS.optimize.batch import sample_batch\n",
    "from MuyGPyS.optimize.loss import lool_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set a random seed here for consistency when building docs.\n",
    "In practice we would not fix a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling a 2D Surface from a Conventional GP\n",
    "\n",
    "This notebook will use a simple two-dimensional curve sampled from a conventional Gaussian process.\n",
    "We will specify the domain as a simple grid on a one-dimensional surface and divide the observations näively into train and test data.\n",
    "\n",
    "Feel free to download the source notebook and experiment with different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we specify the data size and the proportion of the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_per_dim = 60\n",
    "train_ratio = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that the true data is produced with no noise, so we specify a very small noise prior for numerical stability.\n",
    "This is an idealized experiment with effectively no instrument error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nugget_noise = HomoscedasticNoise(1e-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perturb our simulated observations (the training data) with some i.i.d Gaussian measurement noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_noise = HomoscedasticNoise(1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will specify a Matérn kernel with hyperparameters.\n",
    "`smoothness` determines how differentiable the GP prior is.\n",
    "The larger `smoothness` grows, the smoother sampled functions will become."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_smoothness = Parameter(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an anisotropic deformation, where displacement along the dimensions are weighted differently.\n",
    "Each dimension has a corresponding `length_scale` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_length_scale0 = Parameter(0.1)\n",
    "sim_length_scale1 = Parameter(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use all of these parameters to define a Matérn kernel GP and a sampler for convenience.\n",
    "The `UnivariateSampler2D` class is a convenience class for this tutorial, and is not a part of the library.\n",
    "We will use an anisotropic deformation to ensure that we sample data from the appropriate distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampler = UnivariateSampler2D(\n",
    "    points_per_dim=points_per_dim,\n",
    "    train_ratio=train_ratio,\n",
    "    kernel=Matern(\n",
    "        smoothness=sim_smoothness,\n",
    "        deformation=Anisotropy(\n",
    "            l2,\n",
    "            length_scale=VectorParameter(\n",
    "                sim_length_scale0,\n",
    "                sim_length_scale1,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    noise=nugget_noise,\n",
    "    measurement_noise=measurement_noise,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will sample a curve from this GP prior and visualize it.\n",
    "Note that we perturb the train responses (the values that our model will actual receive) with Gaussian measurement noise.\n",
    "Further note that this is not especially fast, as sampling from a conventional Gaussian process requires computing the Cholesky decomposition of a `(data_count, data_count)` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = sampler.features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_responses, test_responses = sampler.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampler.plot_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that our choice of anisotropy has caused the globular Gaussian features in the sampled surface to \"smear\" in the direction of the more heavily weighted axis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Anisotropic Model\n",
    "\n",
    "We will not belabor the details covered in the [Univariate Regression Tutorial](./univariate_regression_tutorial.ipynb).\n",
    "We must similarly construct a nearest neighbors index and sample a training batch in order to optimize a model.\n",
    "\n",
    "⚠️ For now, we use isotropic nearest neighbors as we do not have a guess as to the anisotropic scaling. Future versions of the library will use learned anisotropy to modify neighborhood structure during optimization. ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_count = 30\n",
    "nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method=\"exact\", algorithm=\"ball_tree\")\n",
    "batch_count = sampler.train_count\n",
    "batch_indices, batch_nn_indices = sample_batch(\n",
    "    nbrs_lookup, batch_count, sampler.train_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a MuyGPs object with a Matérn kernel.\n",
    "For simplicity, we will fix `smoothness` and attempt to optimize the two `length_scale` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_anisotropic = MuyGPS(\n",
    "    kernel=Matern(\n",
    "        smoothness=sim_smoothness,\n",
    "        deformation=Anisotropy(\n",
    "            l2,\n",
    "            length_scale=VectorParameter(\n",
    "                Parameter(\"log_sample\", (0.01, 1.0)),\n",
    "                Parameter(\"log_sample\", (0.01, 1.0)),\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    noise=measurement_noise,\n",
    "    scale=AnalyticScale(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create and optimze an isotropic model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_isotropic = MuyGPS(\n",
    "    kernel=Matern(\n",
    "        smoothness=sim_smoothness,\n",
    "        deformation=Isotropy(\n",
    "            l2,\n",
    "            length_scale=Parameter(\"log_sample\", (0.01, 1.0)),\n",
    "        ),\n",
    "    ),\n",
    "    noise=measurement_noise,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our difference tensors as usual and use Bayesian optimization.\n",
    "Note that there is a difference between the crosswise and pairwise tensors that we create here, versus those we create for an isotropic kernel.\n",
    "Anisotropic models create _difference_ tensors rather than _distance_ tensors, which have an extra dimension recording the feature dimension-wise comparisons (in this case, differences) between the items being compared.\n",
    "This is an important distinction, as anisotropic models need to record feature-dimension-wise comparisons to be scaled by trainable parameters, whereas isotropic models do not and collapse differences directory into distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    ") = muygps_anisotropic.make_train_tensors(\n",
    "    batch_indices,\n",
    "    batch_nn_indices,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword arguments for the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_kwargs = {\n",
    "    \"loss_fn\": lool_fn,\n",
    "    \"verbose\": True,\n",
    "    \"random_state\": 1,\n",
    "    \"init_points\": 5,\n",
    "    \"n_iter\": 30,\n",
    "    \"allow_duplicate_points\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "muygps_anisotropic = Bayes_optimize(\n",
    "    muygps_anisotropic,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    **opt_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BayesianOptimization finds an optimimal pair of length scales: {muygps_anisotropic.kernel.deformation.length_scale()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that these returned length scale values might be a little different than what we used to sample the surface.\n",
    "This can be due to a few factors:\n",
    "1. optimizer might not have run enough iterations to converge, or\n",
    "2. there is some mutual unidentifiability between the length scale parameters and the variance `scale` parameter.\n",
    "\n",
    "However, `length_scale0 < length_scale1` as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also optimize the isotropic benchmark.\n",
    "Notice that we need to construct new _distance_ tensors for the isotropic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    batch_crosswise_dists,\n",
    "    batch_pairwise_dists,\n",
    "    _,\n",
    "    _,\n",
    ") = muygps_isotropic.make_train_tensors(\n",
    "    batch_indices,\n",
    "    batch_nn_indices,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_isotropic = Bayes_optimize(\n",
    "    muygps_isotropic,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    "    batch_crosswise_dists,\n",
    "    batch_pairwise_dists,\n",
    "    **opt_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BayesianOptimization finds that the optimimal isotropic length scale is {muygps_isotropic.kernel.deformation.length_scale()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that when fixed to an isotropic length scale, Bayesian optimization tends to favor the smallest true length scale.\n",
    "We'll see how this affects modeling, prediction, and uncertainty quanlity below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separately optimize the `scale` variance scale parameter for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_anisotropic = muygps_anisotropic.optimize_scale(\n",
    "    batch_pairwise_diffs, batch_nn_targets\n",
    ")\n",
    "muygps_isotropic = muygps_isotropic.optimize_scale(\n",
    "    batch_pairwise_diffs, batch_nn_targets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "As in the [Univariate Regression Tutorial](./univariate_regression_tutorial.ipynb), we must realize difference tensors formed from the testing data and apply them to form Gaussian process predictions for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count, _ = test_features.shape\n",
    "indices = np.arange(test_count)\n",
    "test_nn_indices, _ = nbrs_lookup.get_nns(test_features)\n",
    "(\n",
    "    test_crosswise_diffs,\n",
    "    test_pairwise_diffs,\n",
    "    test_nn_targets,\n",
    ") = muygps_anisotropic.make_predict_tensors(\n",
    "    indices,\n",
    "    test_nn_indices,\n",
    "    test_features,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")\n",
    "(\n",
    "    test_crosswise_dists,\n",
    "    test_pairwise_dists,\n",
    "    _,\n",
    ") = muygps_isotropic.make_predict_tensors(\n",
    "    indices,\n",
    "    test_nn_indices,\n",
    "    test_features,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the [Univariate Regression Tutorial](univariate_regression_tutorial.ipynb) we will evaluate the prediction performance of our models in terms of RMSE, mean diagonal posterior variance, the mean 95% confidence interval size, and the coverage, which ideally should be near 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_anisotropic = muygps_anisotropic.kernel(test_crosswise_diffs)\n",
    "Kin_anisotropic = muygps_anisotropic.kernel(test_pairwise_diffs)\n",
    "\n",
    "predictions_anisotropic = muygps_anisotropic.posterior_mean(\n",
    "    Kin_anisotropic, Kcross_anisotropic, test_nn_targets\n",
    ")\n",
    "variances_anisotropic = muygps_anisotropic.posterior_variance(\n",
    "    Kin_anisotropic, Kcross_anisotropic\n",
    ")\n",
    "confidence_intervals_anisotropic = np.sqrt(variances_anisotropic) * 1.96\n",
    "coverage_anisotropic = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_responses - predictions_anisotropic) < confidence_intervals_anisotropic\n",
    "    ) / test_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also evaluate the isotropic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_isotropic = muygps_isotropic.kernel(test_crosswise_dists)\n",
    "Kin_isotropic = muygps_isotropic.kernel(test_pairwise_dists)\n",
    "\n",
    "predictions_isotropic = muygps_isotropic.posterior_mean(Kin_isotropic, Kcross_isotropic, test_nn_targets)\n",
    "variances_isotropic = muygps_isotropic.posterior_variance(Kin_isotropic, Kcross_isotropic)\n",
    "\n",
    "confidence_intervals_isotropic = np.sqrt(variances_isotropic) * 1.96\n",
    "coverage_isotropic = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_responses - predictions_isotropic) < confidence_intervals_isotropic\n",
    "    ) / test_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results comparison\n",
    "\n",
    "A comparison of our trained models reveals that the anisotropic kernel gets close to the true `(0.1, 0.5)` length scale, whereas the isotropic model has to learn a single parameter that has to split the difference somehow.\n",
    "This results in both a higher RMSE and larger confidence intervals in order to achieve similar coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(\n",
    "    test_responses,\n",
    "    (\"anisotropic\", muygps_anisotropic, predictions_anisotropic, variances_anisotropic, confidence_intervals_anisotropic, coverage_anisotropic),\n",
    "    (\"isotropic\", muygps_isotropic, predictions_isotropic, variances_isotropic, confidence_intervals_isotropic, coverage_isotropic),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is low-dimensional so we can plot our predictions and visually evaluate their performance. \n",
    "We plot below the expected (true) surface, and the surface that our model predicts.\n",
    "Note that they are visually similar and major trends are captured, although there are some differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampler.plot_predictions((\"Anisotropic\", predictions_anisotropic), (\"Isotropic\", predictions_isotropic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the anisotropic model learns a surface that is much visually closer to what is expected.\n",
    "In particular, the isotropic surface has blobby circular features as it to be expected, as it is unable to differentiate between distances along the different axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also investigate more details information about the errors.\n",
    "Below we produce three plots that help us to understand our results.\n",
    "The left plot shows the residual, which is the difference between the true values and our expectations.\n",
    "The middle plot shows the magnitude of the 95% confidence interval.\n",
    "The larger the confidence interval, the less certain the model is of its predictions.\n",
    "Finally, the right plot shows the difference between the 95% confidence interval length and the magnitude of the residual.\n",
    "All of the points larger than zero (in red) are not captured by the confidence interval.\n",
    "Hence, this plot shows our coverage distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampler.plot_errors(\n",
    "    (\"Anisotropic\", predictions_anisotropic, confidence_intervals_anisotropic),\n",
    "    (\"Isotropic\", predictions_isotropic, confidence_intervals_isotropic),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rightmost columns shows that the anisotropic assumptions both obtains lower residuals, i.e. the posterior means are more accurate.\n",
    "The middle column shows that the the posterior variances (and resulting confidence intervals) are smaller, and therefore the anisotropic model is also more confident in its predictions.\n",
    "Finally, the rightmost plot reveals the uncovered points - all red-scale residuals exceed the confidence interval.\n",
    "Not only does the isotropic model appear to have more uncovered points, they tend to be further outside of the confidence interval than those of the anisotropic model.\n",
    "These results demonstrate the importance of correct model assumptions, both on predictions and uncertainty quantification."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
