{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021-2023 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anisotropic Metric Tutorial\n",
    "\n",
    "This notebook walks through a simple anisotropic regression workflow and illustrates anisotropic features of `MuyGPyS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "for m in sys.modules.keys():\n",
    "    if m.startswith(\"Muy\"):\n",
    "        sys.modules.pop(m)\n",
    "%env MUYGPYS_BACKEND=numpy\n",
    "%env MUYGPYS_FTYPE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from MuyGPyS._test.sampler import UnivariateSampler2D, print_results\n",
    "from MuyGPyS.gp import MuyGPS\n",
    "from MuyGPyS.gp.distortion import AnisotropicDistortion, IsotropicDistortion, l2\n",
    "from MuyGPyS.gp.hyperparameter import ScalarHyperparameter\n",
    "from MuyGPyS.gp.kernels import Matern\n",
    "from MuyGPyS.gp.noise import HomoscedasticNoise\n",
    "from MuyGPyS.gp.tensors import make_predict_tensors, make_train_tensors\n",
    "from MuyGPyS.neighbors import NN_Wrapper\n",
    "from MuyGPyS.optimize import optimize_from_tensors\n",
    "from MuyGPyS.optimize.batch import sample_batch\n",
    "from MuyGPyS.optimize.loss import lool_fn\n",
    "from MuyGPyS.optimize.sigma_sq import muygps_sigma_sq_optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set a random seed here for consistency when building docs.\n",
    "In practice we would not fix a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling a 2D Surface from a Conventional GP\n",
    "\n",
    "This notebook will use a simple two-dimensional curve sampled from a conventional Gaussian process.\n",
    "We will specify the domain as a simple grid on a one-dimensional surface and divide the observations näively into train and test data.\n",
    "\n",
    "Feel free to download the source notebook and experiment with different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we specify the data size and the proportion of the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_per_dim = 60\n",
    "train_ratio = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that the true data is produced with no noise, so we specify a very small noise prior for numerical stability.\n",
    "This is an idealized experiment with effectively no instrument error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nugget_noise = HomoscedasticNoise(1e-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perturb our simulated observations (the training data) with some i.i.d Gaussian measurement noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_noise = HomoscedasticNoise(1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will specify a `nu` kernel hyperparameters.\n",
    "`nu` determines how smooth the GP prior is.\n",
    "The larger `nu` grows, the smoother sampled functions will become."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_nu = ScalarHyperparameter(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an anisotropic distance metric, where displacement along the dimensions are weighted differently.\n",
    "Each dimension has a corresponding `length_scale` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_length_scale0 = ScalarHyperparameter(0.1)\n",
    "sim_length_scale1 = ScalarHyperparameter(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use all of these parameters to define a Matérn kernel GP and a sampler for convenience.\n",
    "The `UnivariateSampler2D` class is a convenience class for this tutorial, and is not a part of the library.\n",
    "We will use an anisotropic distance metric for our kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampler = UnivariateSampler2D(\n",
    "    points_per_dim=points_per_dim,\n",
    "    train_ratio=train_ratio,\n",
    "    kernel=Matern(\n",
    "        nu=sim_nu,\n",
    "        metric=AnisotropicDistortion(\n",
    "            l2,\n",
    "            length_scale0=sim_length_scale0,\n",
    "            length_scale1=sim_length_scale1,\n",
    "        ),\n",
    "    ),\n",
    "    eps=nugget_noise,\n",
    "    measurement_eps=measurement_noise,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will sample a curve from this GP prior and visualize it.\n",
    "Note that we perturb the train responses (the values that our model will actual receive) with Gaussian measurement noise.\n",
    "Further note that this is not especially fast, as sampling from a conventional Gaussian process requires computing the Cholesky decomposition of a `(data_count, data_count)` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = sampler.features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_responses, test_responses = sampler.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampler.plot_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that our choice of anisotropy has caused the globular Gaussian features in the sampled surface to \"smear\" in the direction of the more heavily weighted axis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Anisotropic Model\n",
    "\n",
    "We will not belabor the details covered in the [Univariate Regression Tutorial](./univariate_regression_tutorial.ipynb).\n",
    "We must similarly construct a nearest neighbors index and sample a training batch in order to optimize a model.\n",
    "\n",
    "⚠️ For now, we use isotropic nearest neighbors as we do not have a guess as to the anisotropic scaling. Future versions of the library will use learned anisotropy to modify neighborhood structure during optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_count = 30\n",
    "nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method=\"exact\", algorithm=\"ball_tree\")\n",
    "batch_count = sampler.train_count\n",
    "batch_indices, batch_nn_indices = sample_batch(\n",
    "    nbrs_lookup, batch_count, sampler.train_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a MuyGPs object with a Matérn kernel.\n",
    "For simplicity, we will fix `nu` and attempt to optimize the two `length_scale`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_length_scale0 = ScalarHyperparameter(\"log_sample\", (0.01, 1.0))\n",
    "exp_length_scale1 = ScalarHyperparameter(\"log_sample\", (0.01, 1.0))\n",
    "muygps_anisotropic = MuyGPS(\n",
    "    kernel=Matern(\n",
    "        nu=sim_nu,\n",
    "        metric=AnisotropicDistortion(\n",
    "            l2,\n",
    "            length_scale0=exp_length_scale0,\n",
    "            length_scale1=exp_length_scale1,\n",
    "        ),\n",
    "    ),\n",
    "    eps=measurement_noise,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ We will also create a fixed muygps object with the hyperparameters that we used for simulation, as well as an isotropic muygps that will be optimized for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_fixed = MuyGPS(\n",
    "    kernel=Matern(\n",
    "        nu=sim_nu,\n",
    "        metric=AnisotropicDistortion(\n",
    "            l2,\n",
    "            length_scale0=sim_length_scale0,\n",
    "            length_scale1=sim_length_scale1,\n",
    "        ),\n",
    "    ),\n",
    "    eps=measurement_noise,\n",
    ")\n",
    "muygps_isotropic = MuyGPS(\n",
    "    kernel=Matern(\n",
    "        nu=sim_nu,\n",
    "        metric=IsotropicDistortion(\n",
    "            l2,\n",
    "            length_scale=ScalarHyperparameter(\"log_sample\", (0.01, 1.0)),\n",
    "        ),\n",
    "    ),\n",
    "    eps=measurement_noise,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our difference tensors as usual and use Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    ") = make_train_tensors(\n",
    "    batch_indices,\n",
    "    batch_nn_indices,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword arguments for the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_kwargs = {\n",
    "    \"loss_fn\": lool_fn,\n",
    "    \"obj_method\": \"loo_crossval\",\n",
    "    \"opt_method\": \"bayesian\",\n",
    "    \"verbose\": True,\n",
    "    \"random_state\": 1,\n",
    "    \"init_points\": 5,\n",
    "    \"n_iter\": 30,\n",
    "    \"allow_duplicate_points\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "muygps_anisotropic = optimize_from_tensors(\n",
    "    muygps_anisotropic,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    **opt_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BayesianOptimization finds an optimimal pair of length scales\")\n",
    "print(f\"\\tlength_scale0 is {muygps_anisotropic.kernel.distortion_fn.length_scale['length_scale0']()}\")\n",
    "print(f\"\\tlength_scale1 is {muygps_anisotropic.kernel.distortion_fn.length_scale['length_scale1']()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that these returned values might be a little different than what we used to sample the surface due to mutual unidentifiability between each other and the `sigma_sq` parameter.\n",
    "However, `length_scale0 < length_scale1` as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Here we optimize the isotropic benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_isotropic = optimize_from_tensors(\n",
    "    muygps_isotropic,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    **opt_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BayesianOptimization finds that the optimimal isotropic length scale is {muygps_isotropic.kernel.distortion_fn.length_scale()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_anisotropic = muygps_sigma_sq_optim(\n",
    "    muygps_anisotropic, \n",
    "    batch_pairwise_diffs, \n",
    "    batch_nn_targets, \n",
    "    sigma_method=\"analytic\"\n",
    ")\n",
    "print(f\"Optimized anisotropic sigma_sq: {muygps_anisotropic.sigma_sq()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ And the isotropic comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_isotropic = muygps_sigma_sq_optim(\n",
    "    muygps_isotropic, \n",
    "    batch_pairwise_diffs, \n",
    "    batch_nn_targets, \n",
    "    sigma_method=\"analytic\"\n",
    ")\n",
    "print(f\"Optimized isotropic sigma_sq: {muygps_isotropic.sigma_sq()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "As in the [Univariate Regression Tutorial](./univariate_regression_tutorial.ipynb), we must realize difference tensors formed from the testing data and apply them to form Gaussian process predictions for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count, _ = test_features.shape\n",
    "indices = np.arange(test_count)\n",
    "test_nn_indices, _ = nbrs_lookup.get_nns(test_features)\n",
    "(\n",
    "    test_crosswise_diffs,\n",
    "    test_pairwise_diffs,\n",
    "    test_nn_targets,\n",
    ") = make_predict_tensors(\n",
    "    indices,\n",
    "    test_nn_indices,\n",
    "    test_features,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we will evaluate our prediction performance in terms of RMSE, mean diagonal posterior variance, the mean 95% confidence interval size, and the coverage, which ideally should be near 95%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_anisotropic = muygps_anisotropic.kernel(test_crosswise_diffs)\n",
    "K_anisotropic = muygps_anisotropic.kernel(test_pairwise_diffs)\n",
    "\n",
    "predictions_anisotropic = muygps_anisotropic.posterior_mean(\n",
    "    K_anisotropic, Kcross_anisotropic, test_nn_targets\n",
    ")\n",
    "variances_anisotropic = muygps_anisotropic.posterior_variance(\n",
    "    K_anisotropic, Kcross_anisotropic\n",
    ")\n",
    "\n",
    "confidence_intervals_anisotropic = np.sqrt(variances_anisotropic) * 1.96\n",
    "coverage_anisotropic = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_responses - predictions_anisotropic) < confidence_intervals_anisotropic\n",
    "    ) / test_count\n",
    ")\n",
    "print_results(\n",
    "    \"anisotropic\", test_responses, predictions_anisotropic, variances_anisotropic, confidence_intervals_anisotropic, coverage_anisotropic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ We also evaluate the fixed and isotropic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_fixed = muygps_fixed.kernel(test_crosswise_diffs)\n",
    "K_fixed = muygps_fixed.kernel(test_pairwise_diffs)\n",
    "\n",
    "predictions_fixed = muygps_fixed.posterior_mean(\n",
    "    K_fixed, Kcross_fixed, test_nn_targets\n",
    ")\n",
    "variances_fixed = muygps_fixed.posterior_variance(\n",
    "    K_fixed, Kcross_fixed\n",
    ")\n",
    "\n",
    "confidence_intervals_fixed = np.sqrt(variances_fixed) * 1.96\n",
    "coverage_fixed = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_responses - predictions_fixed) < confidence_intervals_fixed\n",
    "    ) / test_count\n",
    ")\n",
    "print_results(\n",
    "    \"fixed anisotropic\", test_responses, predictions_fixed, variances_fixed, confidence_intervals_fixed, coverage_fixed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_isotropic = muygps_isotropic.kernel(test_crosswise_diffs)\n",
    "K_isotropic = muygps_isotropic.kernel(test_pairwise_diffs)\n",
    "\n",
    "predictions_isotropic = muygps_isotropic.posterior_mean(K_isotropic, Kcross_isotropic, test_nn_targets)\n",
    "variances_isotropic = muygps_isotropic.posterior_variance(K_isotropic, Kcross_isotropic)\n",
    "\n",
    "confidence_intervals_isotropic = np.sqrt(variances_isotropic) * 1.96\n",
    "coverage_isotropic = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_responses - predictions_isotropic) < confidence_intervals_isotropic\n",
    "    ) / test_count\n",
    ")\n",
    "print_results(\n",
    "    \"isotropic\", test_responses, predictions_isotropic, variances_isotropic, confidence_intervals_isotropic, coverage_isotropic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is low-dimensional so we can plot our predictions and visually evaluate their performance. \n",
    "We plot below the expected (true) surface, and the surface that our model predicts.\n",
    "Note that they are visually similar and major trends are captured, although there are some differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampler.plot_predictions(predictions_anisotropic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also plot information about the errors.\n",
    "Below we produce three plots that help us to understand our results.\n",
    "The left plot shows the residual, which is the difference between the true values and our expectations.\n",
    "The middle plot shows the magnitude of the 95% confidence interval.\n",
    "The larger the confidence interval, the less certain the model is of its predictions.\n",
    "Finally, the right plot shows the difference between the 95% confidence interval length and the magnitude of the residual.\n",
    "All of the points larger than zero (in red) are not captured by the confidence interval.\n",
    "Hence, this plot shows our coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampler.plot_errors(\n",
    "    \"Anisotropic\",\n",
    "    predictions_anisotropic,\n",
    "    confidence_intervals_anisotropic,\n",
    "    \"Fixed\",\n",
    "    predictions_fixed,\n",
    "    confidence_intervals_fixed,\n",
    "    \"Isotropic\",\n",
    "    predictions_isotropic,\n",
    "    confidence_intervals_isotropic,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
