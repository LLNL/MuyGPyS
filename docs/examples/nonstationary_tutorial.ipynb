{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2023-2023 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonstationary tutorial\n",
    "\n",
    "This notebook demonstrates how to use hierarchical nonstationary hyperparameters to perform nonstationary regression using a hierarchical model.\n",
    "\n",
    "⚠️ _Note that this is still an experimental feature at this point._ ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from MuyGPyS.gp import MuyGPS\n",
    "from MuyGPyS.gp.distortion import IsotropicDistortion, l2, F2\n",
    "from MuyGPyS.gp.hyperparameter import ScalarHyperparameter\n",
    "from MuyGPyS.gp.hyperparameter.experimental import (\n",
    "    sample_knots,\n",
    "    HierarchicalNonstationaryHyperparameter,\n",
    ")\n",
    "from MuyGPyS.gp.kernels import RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set a random seed here for consistency when building docs.\n",
    "In practice we would not fix a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we start with an isotropic distortion so we only need to use a single `HierarchicalNonstationaryHyperparameter`.\n",
    "Let's also build a GP with a fixed length scale for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some training data with a little bit of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_max = 5\n",
    "data_count = 500\n",
    "train_step = 10\n",
    "train_count = int(data_count / train_step)\n",
    "test_count = data_count - train_count\n",
    "x = np.linspace(-data_max, data_max, num=data_count)\n",
    "y = np.sinc(x) - np.mean(np.sinc(x))\n",
    "train_features = np.reshape(x[::train_step] + np.random.normal(scale=0.02, size=train_count), (-1, 1))\n",
    "train_responses = np.reshape(y[::train_step] + np.random.normal(scale=0.02, size=train_count), (-1, 1))\n",
    "test_features = x[np.mod(np.arange(data_count), train_step) != 0].reshape(test_count, 1)\n",
    "test_responses = y[np.mod(np.arange(data_count), train_step) != 0].reshape(test_count, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the true function we are trying to predict, along with the training data with which will optimize a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "ax.plot(x, y, label=\"True Response\")\n",
    "ax.plot(train_features, train_responses, '.', label=\"Training Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a stationary MuyGPs object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_fixed = MuyGPS(\n",
    "    kernel=RBF(\n",
    "        metric=IsotropicDistortion(\n",
    "            l2,\n",
    "            length_scale=ScalarHyperparameter(5.0),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create a hierarchical nonstationary MuyGPs object, where we assume that the `length_scale` of the distance function itself varies according to a Gaussian process with some \"knots\", locations in the range of the function where we assume that we know or can learn the true value of the `length_scale`. We will start by sampling some knots and giving them initial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot_count = 6\n",
    "knot_features = sample_knots(feature_count=1, knot_count=knot_count)\n",
    "knot_features *= data_max * 2\n",
    "knot_features -= data_max\n",
    "knot_features = np.array(sorted(knot_features))\n",
    "knot_values = np.array([10.0, 7.5, 5.0, 5.0, 7.5, 10.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a `MuyGPS` object like before, except now we specify that the `length_scale` is hierarchical and pass the knots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_level_kernel = RBF(\n",
    "    IsotropicDistortion(\n",
    "        F2,\n",
    "        length_scale=ScalarHyperparameter(10.0))\n",
    ")\n",
    "\n",
    "muygps = MuyGPS(\n",
    "    kernel=RBF(\n",
    "        metric=IsotropicDistortion(\n",
    "            l2,\n",
    "            length_scale=HierarchicalNonstationaryHyperparameter(\n",
    "                knot_features, knot_values, high_level_kernel\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the knots and the resulting `length_scale` surface over the domain of the function.\n",
    "Unlike `ScalarHyperparameter`, `HierarchicalNonstationaryHyperparameter` takes an array of feature vectors for each point where you would like to evaluate the local value of the hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale_curve = muygps.kernel.distortion_fn.length_scale(x.reshape(data_count, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a small example, we can evaluate and display the predicted `length_scale` values across the whole domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "ax.set_title(\"Hierarchical Length Scale Surface Over the Domain\")\n",
    "order = np.argsort(knot_features[:,0])\n",
    "ax.plot(knot_features[order,:], knot_values[order], \"*\", label=\"Knot Values\")\n",
    "ax.plot(x, length_scale_curve, label=\"Interpolated Surface\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed as usual to generate the nearest neighbors lookup index and tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.neighbors import NN_Wrapper\n",
    "\n",
    "nn_count = 30\n",
    "nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method=\"exact\", algorithm=\"ball_tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this simple example we're using all of the data as batch points, i.e. we're not really batching, since the dataset is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.gp.tensors import make_predict_tensors\n",
    "\n",
    "test_indices = np.arange(test_count)\n",
    "test_nn_indices, _ = nbrs_lookup.get_nns(test_features)\n",
    "\n",
    "(\n",
    "    test_crosswise_diffs,\n",
    "    test_pairwise_diffs,\n",
    "    test_nn_targets,\n",
    ") = make_predict_tensors(\n",
    "    test_indices,\n",
    "    test_nn_indices,\n",
    "    test_features,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, at this point, we would optimize hyperparameters, but that part hasn't been implemented yet so we'll skip it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One notable difference when using a hierarchical model is that the kernel takes an additional tensor, the batch tensor, which can be easily obtained using the `batch_features_tensor` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.gp.tensors import batch_features_tensor\n",
    "\n",
    "batch_features = batch_features_tensor(test_features, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're ready to realize the kernel tensors and use them to predict the response of the test data. First using the GP with a fixed length scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_fixed = muygps_fixed.kernel(test_crosswise_diffs)\n",
    "K_fixed = muygps_fixed.kernel(test_pairwise_diffs)\n",
    "test_responses_mean_fixed = muygps_fixed.posterior_mean(K_fixed, Kcross_fixed, test_nn_targets)\n",
    "test_responses_var_fixed = muygps_fixed.posterior_variance(K_fixed, Kcross_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the hierarchical GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross = muygps.kernel(test_crosswise_diffs, batch_features)\n",
    "K = muygps.kernel(test_pairwise_diffs, batch_features)\n",
    "test_responses_mean = muygps.posterior_mean(K, Kcross, test_nn_targets)\n",
    "test_responses_var = muygps.posterior_variance(K, Kcross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize the results by plotting the predicted means as well as one predicted standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "ax.plot(x, y, label=\"truth\")\n",
    "ax.plot(test_features, test_responses_mean_fixed, \".-\", label=\"fixed\")\n",
    "ax.plot(test_features, test_responses_mean, \"--\", label=\"hierarchical\")\n",
    "ax.fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(test_responses_mean_fixed + np.sqrt(test_responses_var_fixed) * 1.96),\n",
    "    np.ravel(test_responses_mean_fixed - np.sqrt(test_responses_var_fixed) * 1.96),\n",
    "    facecolor=\"C1\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "ax.fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(test_responses_mean + np.sqrt(test_responses_var) * 1.96),\n",
    "    np.ravel(test_responses_mean - np.sqrt(test_responses_var) * 1.96),\n",
    "    facecolor=\"C2\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
