{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2023-2023 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonstationary tutorial\n",
    "\n",
    "This notebook demonstrates how to use hierarchical nonstationary hyperparameters to perform nonstationary regression using a hierarchical model.\n",
    "\n",
    "⚠️ _Note that this is still an experimental feature at this point._ ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from MuyGPyS.gp import MuyGPS\n",
    "from MuyGPyS.gp.distortion import IsotropicDistortion, l2, F2\n",
    "from MuyGPyS.gp.hyperparameter import ScalarHyperparameter\n",
    "from MuyGPyS.gp.hyperparameter.experimental import (\n",
    "    sample_knots,\n",
    "    HierarchicalNonstationaryHyperparameter,\n",
    ")\n",
    "from MuyGPyS.gp.kernels import RBF\n",
    "from MuyGPyS.gp.noise import HomoscedasticNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set a random seed here for consistency when building docs.\n",
    "In practice we would not fix a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we start with an isotropic distortion so we only need to use a single `HierarchicalNonstationaryHyperparameter`.\n",
    "Let's also build a GP with a fixed length scale for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some training data with a little bit of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_max = 5\n",
    "data_count = 500\n",
    "train_step = 10\n",
    "train_count = int(data_count / train_step)\n",
    "test_count = data_count - train_count\n",
    "noise_prior = 1e-5\n",
    "noise_actual = 2e-4\n",
    "x = np.linspace(-data_max, data_max, num=data_count)\n",
    "y = np.sinc(x) - np.mean(np.sinc(x))\n",
    "x = (x - np.min(x)) / (2 * np.max(x))\n",
    "train_features = np.reshape(x[::train_step] + np.random.normal(scale=noise_actual, size=train_count), (-1, 1))\n",
    "train_responses = np.reshape(y[::train_step] + np.random.normal(scale=noise_actual, size=train_count), (-1, 1))\n",
    "test_features = x[np.mod(np.arange(data_count), train_step) != 0].reshape(test_count, 1)\n",
    "test_responses = y[np.mod(np.arange(data_count), train_step) != 0].reshape(test_count, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the true function we are trying to predict, along with the training data with which will optimize a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "ax.plot(x, y, label=\"True Response\")\n",
    "ax.plot(train_features, train_responses, '.', label=\"Training Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a stationary MuyGPs object for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_fixed = MuyGPS(\n",
    "    kernel=RBF(\n",
    "        metric=IsotropicDistortion(\n",
    "            l2,\n",
    "            length_scale=ScalarHyperparameter(1.0),\n",
    "        ),\n",
    "    ),\n",
    "    eps=HomoscedasticNoise(noise_prior),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Nonstationary MuyGPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create a hierarchical nonstationary MuyGPs object, where we assume that the `length_scale` of the distance function itself varies according to a Gaussian process with some \"knots\", locations in the range of the function where we assume that we know or can learn the true value of the `length_scale`. We will start by sampling some knots and giving them initial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot_count = 6\n",
    "knot_features = sample_knots(feature_count=1, knot_count=knot_count)\n",
    "knot_features = np.array(sorted(knot_features))\n",
    "knot_values = np.array([2.0, 1.5, 1.0, 1.0, 1.5, 2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a `MuyGPS` object like before, except now we specify that the `length_scale` is hierarchical and pass the knots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_level_kernel = RBF(\n",
    "    IsotropicDistortion(\n",
    "        F2,\n",
    "        length_scale=ScalarHyperparameter(1.0))\n",
    ")\n",
    "\n",
    "muygps = MuyGPS(\n",
    "    kernel=RBF(\n",
    "        metric=IsotropicDistortion(\n",
    "            l2,\n",
    "            length_scale=HierarchicalNonstationaryHyperparameter(\n",
    "                knot_features, knot_values, high_level_kernel\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    eps=HomoscedasticNoise(noise_prior),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the knots and the resulting `length_scale` surface over the domain of the function.\n",
    "Unlike `ScalarHyperparameter`, `HierarchicalNonstationaryHyperparameter` takes an array of feature vectors for each point where you would like to evaluate the local value of the hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale_curve = muygps.kernel.distortion_fn.length_scale(x.reshape(data_count, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a small example, we can evaluate and display the predicted `length_scale` values across the whole domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "ax.set_title(\"Hierarchical Length Scale Surface Over the Domain\")\n",
    "order = np.argsort(knot_features[:,0])\n",
    "ax.plot(knot_features[order,:], knot_values[order], \"*\", label=\"Knot Values\")\n",
    "ax.plot(x, length_scale_curve, label=\"Interpolated Surface\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed as usual to generate the nearest neighbors lookup index and tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.neighbors import NN_Wrapper\n",
    "\n",
    "nn_count = 30\n",
    "nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method=\"exact\", algorithm=\"ball_tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this simple example we're using all of the data as batch points, i.e. we're not really batching, since the dataset is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.gp.tensors import make_predict_tensors\n",
    "\n",
    "test_indices = np.arange(test_count)\n",
    "test_nn_indices, _ = nbrs_lookup.get_nns(test_features)\n",
    "\n",
    "(\n",
    "    test_crosswise_diffs,\n",
    "    test_pairwise_diffs,\n",
    "    test_nn_targets,\n",
    ") = make_predict_tensors(\n",
    "    test_indices,\n",
    "    test_nn_indices,\n",
    "    test_features,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One notable difference when using a hierarchical model is that the kernel takes an additional tensor, the batch tensor, which can be easily obtained using the `batch_features_tensor` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.gp.tensors import batch_features_tensor\n",
    "\n",
    "batch_test_features = batch_features_tensor(test_features, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're ready to realize the kernel tensors and use them to predict the response of the test data. First using the GP with a fixed length scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_flat_fixed = muygps_fixed.kernel(test_crosswise_diffs)\n",
    "K_flat_fixed = muygps_fixed.kernel(test_pairwise_diffs)\n",
    "mean_flat_fixed = muygps_fixed.posterior_mean(\n",
    "    K_flat_fixed, Kcross_flat_fixed, test_nn_targets\n",
    ")\n",
    "var_flat_fixed = muygps_fixed.posterior_variance(\n",
    "    K_flat_fixed, Kcross_flat_fixed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the hierarchical GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_hierarchical_fixed = muygps.kernel(\n",
    "    test_crosswise_diffs, batch_features=batch_test_features\n",
    ")\n",
    "K_hierarchical_fixed = muygps.kernel(\n",
    "    test_pairwise_diffs, batch_features=batch_test_features\n",
    ")\n",
    "mean_hierarchical_fixed = muygps.posterior_mean(\n",
    "    K_hierarchical_fixed, Kcross_hierarchical_fixed, test_nn_targets\n",
    ")\n",
    "var_hierarchical_fixed = muygps.posterior_variance(\n",
    "    K_hierarchical_fixed, Kcross_hierarchical_fixed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize the results by plotting the predicted means as well as one predicted standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "ax.plot(x, y, label=\"truth\")\n",
    "ax.plot(test_features, mean_flat_fixed, \".-\", label=\"flat fixed\")\n",
    "ax.fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(mean_flat_fixed + np.sqrt(var_flat_fixed) * 1.96),\n",
    "    np.ravel(mean_flat_fixed - np.sqrt(var_flat_fixed) * 1.96),\n",
    "    facecolor=\"C1\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "ax.plot(test_features, mean_hierarchical_fixed, \"--\", label=\"hierarchical fixed\")\n",
    "ax.fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(mean_hierarchical_fixed + np.sqrt(var_hierarchical_fixed) * 1.96),\n",
    "    np.ravel(mean_hierarchical_fixed - np.sqrt(var_hierarchical_fixed) * 1.96),\n",
    "    facecolor=\"C2\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knot values of hierarchical nonstationary hyperparameters can be optimized using like any other hyperparameters, using the `optimize_from_tensors` utility. But first, we need to initialize them as `ScalarHyperparameter`s with bounds rather than as fixed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [0.1, 5.0]\n",
    "knot_values_to_be_optimized = [ScalarHyperparameter(\"sample\", bounds) for _ in range(knot_count)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recreate a MuyGPs object. It's identical to the one we've created before except for the knot values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_to_be_optimized = MuyGPS(\n",
    "    kernel=RBF(\n",
    "        metric=IsotropicDistortion(\n",
    "            l2,\n",
    "            length_scale=HierarchicalNonstationaryHyperparameter(\n",
    "                knot_features, knot_values_to_be_optimized, high_level_kernel\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    eps=HomoscedasticNoise(noise_prior),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use `make_train_tensors` to obtain the training tensors. Once again, we use all the training data instead of batching it due to the small size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.gp.tensors import make_train_tensors\n",
    "\n",
    "train_indices = np.arange(train_count)\n",
    "train_nn_indices, _ = nbrs_lookup.get_nns(train_features)\n",
    "\n",
    "(\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    ") = make_train_tensors(\n",
    "    train_indices,\n",
    "    train_nn_indices,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")\n",
    "\n",
    "batch_train_features = batch_features_tensor(train_features, train_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use `optimize_from_tensors` to create a new MuyGPs object for which the knot values have been fixed to their optimal values. Note that we must pass the `batch_features` tensor, this time from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ _The next four cells are for testing purposes and should be removed prior to finalizing this tutorial._ ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.optimize.loss import get_loss_func\n",
    "from MuyGPyS.optimize.objective import make_obj_fn, make_kernels_fn, make_raw_predict_and_loss_fn, make_var_predict_and_loss_fn\n",
    "from MuyGPyS.optimize.sigma_sq import make_sigma_sq_optim\n",
    "from MuyGPyS.optimize.utils import _switch_on_loss_method\n",
    "\n",
    "loss_method = \"lool\"\n",
    "sigma_method = \"analytic\"\n",
    "loss_fn = get_loss_func(loss_method)\n",
    "kernel_fn = hierarchical_to_be_optimized.kernel.get_opt_fn()\n",
    "mean_fn = hierarchical_to_be_optimized.get_opt_mean_fn()\n",
    "var_fn = hierarchical_to_be_optimized.get_opt_var_fn()\n",
    "sigma_sq_fn = make_sigma_sq_optim(sigma_method, hierarchical_to_be_optimized)\n",
    "\n",
    "kernels_fn = make_kernels_fn(\n",
    "    kernel_fn,\n",
    "    batch_pairwise_diffs,\n",
    "    batch_crosswise_diffs,\n",
    ")\n",
    "\n",
    "predict_and_loss_fn = _switch_on_loss_method(\n",
    "    loss_method,\n",
    "    make_raw_predict_and_loss_fn,\n",
    "    make_raw_predict_and_loss_fn,\n",
    "    make_var_predict_and_loss_fn,\n",
    "    make_raw_predict_and_loss_fn,\n",
    "    loss_fn,\n",
    "    mean_fn,\n",
    "    var_fn,\n",
    "    sigma_sq_fn,\n",
    "    batch_nn_targets,\n",
    "    batch_targets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, Kcross = kernels_fn(batch_features=batch_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_fn = make_obj_fn(\n",
    "    \"loo_crossval\",\n",
    "    loss_method,\n",
    "    loss_fn,\n",
    "    kernel_fn,\n",
    "    mean_fn,\n",
    "    var_fn,\n",
    "    sigma_sq_fn,\n",
    "    batch_pairwise_diffs,\n",
    "    batch_crosswise_diffs,\n",
    "    batch_nn_targets,\n",
    "    batch_targets,\n",
    "    batch_features=batch_train_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "print(f\"kernel_fn: {inspect.signature(kernel_fn)}\")\n",
    "print(f\"kernels_fn: {inspect.signature(kernels_fn)}\")\n",
    "print(f\"mean_fn: {inspect.signature(mean_fn)}\")\n",
    "print(f\"var_fn: {inspect.signature(var_fn)}\")\n",
    "print(f\"predict_and_loss_fn: {inspect.signature(predict_and_loss_fn)}\")\n",
    "print(f\"obj_fn: {inspect.signature(obj_fn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.optimize import optimize_from_tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from MuyGPyS.optimize import optimize_from_tensors\n",
    "\n",
    "batch_train_features = batch_features_tensor(train_features, train_indices)\n",
    "\n",
    "hierarchical_optimized = optimize_from_tensors(\n",
    "    hierarchical_to_be_optimized,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    loss_method=loss_method,\n",
    "    obj_method=\"loo_crossval\",\n",
    "    opt_method=\"bayes\",\n",
    "    verbose=True,\n",
    "    batch_features=batch_train_features,\n",
    "    init_points=20,\n",
    "    n_iter=20,\n",
    "    allow_duplicate_points=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized model has set new knot values, which we can use to visualize the learned `length_scale` surface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_optimized.kernel.distortion_fn.length_scale._knot_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale_curve_optimized = hierarchical_optimized.kernel.distortion_fn.length_scale(x.reshape(data_count, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "ax.set_title(\"Trained Hierarchical Length Scale Surface Over the Domain\")\n",
    "order = np.argsort(knot_features[:,0])\n",
    "ax.plot(\n",
    "    knot_features[order,:], \n",
    "    hierarchical_optimized.kernel.distortion_fn.length_scale._knot_values()[order], \n",
    "    \"*\", \n",
    "    label=\"Knot Values\")\n",
    "ax.plot(x, length_scale_curve_optimized, label=\"Interpolated Surface\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the optimized the kernel to predict the test responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_hierarchical_opt = hierarchical_optimized.kernel(test_crosswise_diffs, batch_test_features)\n",
    "K_hierarchical_opt = hierarchical_optimized.kernel(test_pairwise_diffs, batch_test_features)\n",
    "mean_hierarchical_opt = hierarchical_optimized.posterior_mean(\n",
    "    K_hierarchical_opt, Kcross_hierarchical_opt, test_nn_targets\n",
    ")\n",
    "var_hierarchical_opt = hierarchical_optimized.posterior_variance(\n",
    "    K_hierarchical_opt, Kcross_hierarchical_opt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also optimize a flat model with the same batch for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_to_be_optimized = MuyGPS(\n",
    "    kernel=RBF(\n",
    "        metric=IsotropicDistortion(\n",
    "            l2,\n",
    "            length_scale=ScalarHyperparameter(\"sample\", [0.001, 5.0]),\n",
    "        ),\n",
    "    ),\n",
    "    eps=HomoscedasticNoise(noise_prior),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_optimized = optimize_from_tensors(\n",
    "    flat_to_be_optimized,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    loss_method=loss_method,\n",
    "    obj_method=\"loo_crossval\",\n",
    "    opt_method=\"bayes\",\n",
    "    verbose=True,\n",
    "    allow_duplicate_points=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross_flat_opt = flat_optimized.kernel(test_crosswise_diffs)\n",
    "K_flat_opt = flat_optimized.kernel(test_pairwise_diffs)\n",
    "mean_flat_opt = flat_optimized.posterior_mean(\n",
    "    K_flat_opt, Kcross_flat_opt, test_nn_targets\n",
    ")\n",
    "var_flat_opt = flat_optimized.posterior_variance(\n",
    "    K_flat_opt, Kcross_flat_opt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "ax.plot(x, y, label=\"truth\")\n",
    "ax.plot(test_features, mean_flat_fixed, \".-\", label=\"flat fixed\")\n",
    "ax.fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(mean_flat_fixed + np.sqrt(var_flat_fixed) * 1.96),\n",
    "    np.ravel(mean_flat_fixed - np.sqrt(var_flat_fixed) * 1.96),\n",
    "    facecolor=\"C1\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "ax.plot(test_features, mean_hierarchical_fixed, \"--\", label=\"hierarchical fixed\")\n",
    "ax.fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(mean_hierarchical_fixed + np.sqrt(var_hierarchical_fixed) * 1.96),\n",
    "    np.ravel(mean_hierarchical_fixed - np.sqrt(var_hierarchical_fixed) * 1.96),\n",
    "    facecolor=\"C2\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "ax.plot(test_features, mean_flat_opt, \"--\", label=\"flat optimized\")\n",
    "ax.fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(mean_flat_opt + np.sqrt(var_flat_opt) * 1.96),\n",
    "    np.ravel(mean_flat_opt - np.sqrt(var_flat_opt) * 1.96),\n",
    "    facecolor=\"C3\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "ax.plot(test_features, mean_hierarchical_opt, \"--\", label=\"hierarchical optimized\")\n",
    "ax.fill_between(\n",
    "    np.ravel(test_features),\n",
    "    np.ravel(mean_hierarchical_opt + np.sqrt(var_hierarchical_opt) * 1.96),\n",
    "    np.ravel(mean_hierarchical_opt - np.sqrt(var_hierarchical_opt) * 1.96),\n",
    "    facecolor=\"C3\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
