{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c123e93",
   "metadata": {},
   "source": [
    "Copyright 2021-2023 Lawrence Livermore National Security, LLC and other MuyGPyS Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT\n",
    "\n",
    "# Deep Kernels with MuyGPs in PyTorch Tutorial\n",
    "\n",
    "In this tutorial, we outline how to construct a simple deep kernel model using the PyTorch implementation of MuyGPs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83dc4cd",
   "metadata": {},
   "source": [
    "We use the MNIST classification problem as a benchmark.\n",
    "We will use the deep kernel MuyGPs model to classify images of handwritten digits between 0 and 9.\n",
    "In order to reduce the runtime of the training loop, we will use a fully-connected architecture, meaning we will have to vectorize each image prior to training.\n",
    "We download the training and testing data using the torchvision.datasets API.\n",
    "\n",
    "First, we will import necessary dependencies.\n",
    "We also force MuyGPyS to use the `\"torch\"` backend.\n",
    "This can also be done by setting the `MUYGPYS_BACKEND` environment variable to `\"torch\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f72675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS import config\n",
    "config.update(\"muygpys_backend\",\"torch\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from torch.nn.functional import one_hot\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124c8d0e",
   "metadata": {},
   "source": [
    "We use torch's utilities to download MNIST and transform it into an appropriately normalized tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e51b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5,),(1.0,)),\n",
    "    ]\n",
    ")\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root=root, train=True, transform=trans, download=True\n",
    ")\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root=root, train=False, transform=trans, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66125f37",
   "metadata": {},
   "source": [
    "MNIST is a popular benchmark dataset of hand-written digits, 0-9.\n",
    "Each digit is a 28x28 pixel image, with 784 total pixel features.\n",
    "In the interest of reducing runtime, we will use vectorized images as our features in\n",
    "this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabad0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_train_samples = 60000\n",
    "num_test_samples = 10000\n",
    "num_pixels = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcde3c",
   "metadata": {},
   "source": [
    "We will collect 60,000 training samples and 10,000 test samples.\n",
    "We vectorize the images and one-hot encode the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71dd370",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.zeros((num_train_samples,num_pixels))\n",
    "train_responses = torch.zeros((num_train_samples,num_classes))\n",
    "\n",
    "for i in range(num_train_samples):\n",
    "    train_features[i,:] = train_set[i][0].flatten()\n",
    "    train_responses[i,:] = one_hot(\n",
    "        torch.tensor(train_set[i][1]).to(torch.int64),\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "\n",
    "test_features = torch.zeros((num_test_samples,num_pixels))\n",
    "test_responses = torch.zeros((num_test_samples,num_classes))\n",
    "\n",
    "for i in range(num_test_samples):\n",
    "    test_features[i,:] = test_set[i][0].flatten()\n",
    "    test_responses[i,:] = one_hot(\n",
    "        torch.tensor(test_set[i][1]).to(torch.int64),\n",
    "        num_classes=num_classes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319abcfa",
   "metadata": {},
   "source": [
    "We set up our nearest neighbor lookup structure using the NN_Wrapper data structure in MuyGPs.\n",
    "We then define our batch and construct tensor containing the features and targets of the batched elements and their 30 nearest neighbors.\n",
    "We choose an algorithm that will return the exact nearest neighbors.\n",
    "We set a random seed for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "np.random.seed(0)\n",
    "test_count, _ = test_features.shape\n",
    "train_count, _ = train_features.shape\n",
    "\n",
    "\n",
    "from MuyGPyS.neighbors import NN_Wrapper\n",
    "nn_count = 30\n",
    "nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method=\"exact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42073b2d",
   "metadata": {},
   "source": [
    "We sample a training batch of 500 elements and record their indices and those of their nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We will make use of batching in our hyperparameter training\n",
    "from MuyGPyS.optimize.batch import sample_batch\n",
    "batch_count = 500\n",
    "batch_indices, batch_nn_indices = sample_batch(\n",
    "    nbrs_lookup, batch_count, train_count\n",
    ")\n",
    "\n",
    "batch_indices = batch_indices.astype(np.int64)\n",
    "batch_nn_indices = batch_nn_indices.astype(np.int64)\n",
    "batch_indices = torch.from_numpy(batch_indices)\n",
    "batch_nn_indices = torch.from_numpy(batch_nn_indices)\n",
    "\n",
    "batch_features = train_features[batch_indices,:]\n",
    "batch_targets = train_responses[batch_indices, :]\n",
    "batch_nn_targets = train_responses[batch_nn_indices, :]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_features = train_features.cuda()\n",
    "    train_responses = train_responses.cuda()\n",
    "    test_features = test_features.cuda()\n",
    "    test_responses = test_responses.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef039a",
   "metadata": {},
   "source": [
    "We now define a stochastic variational deep kernel MuyGPs class.\n",
    "This class composes a dense neural network embedding with a `MuyGPyS.torch.muygps_layer` Gaussian process layer.\n",
    "Presently, this layer only supports the Matérn kernel with special values of the `nu` or smoothness parameter set to 0.5, 1.5, 2.5, or $\\infty$.\n",
    "The smoothness values are limited because `torch` does not implement modified bessel functions of the second kind.\n",
    "Future versions of the library will also support other kernel types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa79b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.torch.muygps_layer import MuyGPs_layer\n",
    "print('Building Stochastic Variational Deep Kernel MuyGPs model')\n",
    "\n",
    "class SVDKMuyGPs(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_models,\n",
    "        kernel_eps,\n",
    "        nu,\n",
    "        length_scale,\n",
    "        batch_indices,\n",
    "        batch_nn_indices,\n",
    "        batch_targets,\n",
    "        batch_nn_targets,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(784,400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400,200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200,100),\n",
    "        )\n",
    "        self.eps = kernel_eps\n",
    "        self.nu = nu\n",
    "        self.length_scale = length_scale\n",
    "        self.batch_indices = batch_indices\n",
    "        self.num_models = num_models\n",
    "        self.batch_nn_indices = batch_nn_indices\n",
    "        self.batch_targets = batch_targets\n",
    "        self.batch_nn_targets = batch_nn_targets\n",
    "        self.GP_layer = MuyGPs_layer(\n",
    "            kernel_eps,\n",
    "            nu,\n",
    "            length_scale,\n",
    "            batch_indices,\n",
    "            batch_nn_indices,\n",
    "            batch_targets,\n",
    "            batch_nn_targets,\n",
    "        )\n",
    "    \n",
    "    def forward(self,x): \n",
    "        predictions = self.embedding(x)\n",
    "        predictions, variances, sigma_sq = self.GP_layer(predictions)\n",
    "        return predictions, variances, sigma_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df56f9",
   "metadata": {},
   "source": [
    "## Training a Deep Kernel MuyGPs Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25af10b7",
   "metadata": {},
   "source": [
    "We instantiate a `SVDKMuyGPs` model with initial guess hyperparameters.\n",
    "We fix a Matérn kernel smoothness parameter of 0.5 and a Guassian homoscedastic noise prior variance of `1e-6`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b9f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVDKMuyGPs(\n",
    "    num_models=num_classes,\n",
    "    kernel_eps=1e-6,\n",
    "    nu=0.5,\n",
    "    length_scale=1.0,\n",
    "    batch_indices=batch_indices,\n",
    "    batch_nn_indices=batch_nn_indices,\n",
    "    batch_targets=batch_targets,\n",
    "    batch_nn_targets=batch_nn_targets,\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a3525",
   "metadata": {},
   "source": [
    "We use the Adam optimizer over 10 training iterations, with an initial learning rate of `1e-2` and decay of `0.97`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b50e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iterations = 10\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params': model.parameters()}], lr=1e-2\n",
    ") \n",
    "scheduler = ExponentialLR(optimizer, gamma=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a69655",
   "metadata": {},
   "source": [
    "We will use cross-entropy loss, as it is commonly performant for classification problems.\n",
    "Other losses are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "# mse_loss = nn.MSELoss()\n",
    "# l1_loss = nn.L1Loss()\n",
    "# bce_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c025ff",
   "metadata": {},
   "source": [
    "We construct a standard PyTorch training loop function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nbrs_lookup):\n",
    "    for i in range(training_iterations):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions,variances,sigma_sq = model(train_features)\n",
    "        loss = ce_loss(predictions,batch_targets)\n",
    "        loss.backward()      \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if np.mod(i,1) == 0:\n",
    "            print(f\"Iter {i + 1}/{training_iterations} - Loss: {loss.item()}\")\n",
    "            model.eval()\n",
    "            nbrs_lookup = NN_Wrapper(\n",
    "                model.embedding(train_features).detach().numpy(), \n",
    "                nn_count, nn_method=\"exact\"\n",
    "            )\n",
    "            batch_nn_indices,_ = nbrs_lookup._get_nns(\n",
    "                model.embedding(batch_features).detach().numpy(),\n",
    "                nn_count=nn_count,\n",
    "            )\n",
    "            batch_nn_indices = torch.from_numpy(\n",
    "                batch_nn_indices.astype(np.int64)\n",
    "            )\n",
    "            batch_nn_targets = train_responses[batch_nn_indices, :]  \n",
    "            model.batch_nn_indices = batch_nn_indices\n",
    "            model.batch_nn_targets = batch_nn_targets\n",
    "        torch.cuda.empty_cache()\n",
    "    nbrs_lookup = NN_Wrapper(\n",
    "        model.embedding(train_features).detach().numpy(),\n",
    "        nn_count,\n",
    "        nn_method=\"exact\",\n",
    "    )\n",
    "    batch_nn_indices,_ = nbrs_lookup._get_nns(\n",
    "        model.embedding(batch_features).detach().numpy(),\n",
    "        nn_count=nn_count,\n",
    "    )\n",
    "    batch_nn_indices = torch.from_numpy(\n",
    "        batch_nn_indices.astype(np.int64)\n",
    "    )\n",
    "    batch_nn_targets = train_responses[batch_nn_indices, :]\n",
    "    model.batch_nn_indices = batch_nn_indices\n",
    "    model.batch_nn_targets = batch_nn_targets\n",
    "    return nbrs_lookup, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11548ab",
   "metadata": {},
   "source": [
    "Finally, we execute the training function and evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb287061",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs_lookup, model_trained = train(nbrs_lookup)\n",
    "model_trained.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f54d60",
   "metadata": {},
   "source": [
    "We then compute and report the performance of the predicted test responses using this trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101aa630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.examples.muygps_torch import predict_model\n",
    "predictions, variances,sigma_sq = predict_model(\n",
    "    model=model_trained,\n",
    "    test_features=test_features,\n",
    "    train_features=train_features,\n",
    "    train_responses=train_responses,\n",
    "    nbrs_lookup=nbrs_lookup,\n",
    "    nn_count=nn_count,\n",
    ")\n",
    "print(\"MNIST Prediction Accuracy Using Low-Level Torch Implementation:\")\n",
    "print(\n",
    "    (\n",
    "        torch.sum(\n",
    "            torch.argmax(predictions,dim=1) == torch.argmax(test_responses,dim=1)\n",
    "        ) / 10000\n",
    "    ).numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb5469a",
   "metadata": {},
   "source": [
    "## Training a Deep Kernel MuyGPs Model Using Our Example API Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed699898",
   "metadata": {},
   "source": [
    "Similar to our one-line regression tutorial API, we support a one-line Deep MuyGPs regression API.\n",
    "This snippet performs the same work as above with a singular function execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf19103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import high-level API function train_deep_kernel_muygps \n",
    "from MuyGPyS.examples.muygps_torch import train_deep_kernel_muygps\n",
    "\n",
    "#Use leave-one-out-likelihood loss function to train model\n",
    "model = SVDKMuyGPs(\n",
    "    num_models=num_classes,\n",
    "    kernel_eps=1e-6,\n",
    "    nu=0.5,\n",
    "    length_scale=1.0,\n",
    "    batch_indices=batch_indices,\n",
    "    batch_nn_indices=batch_nn_indices,\n",
    "    batch_targets=batch_targets,\n",
    "    batch_nn_targets=batch_nn_targets)\n",
    "\n",
    "nbrs_lookup, model_trained = train_deep_kernel_muygps(\n",
    "    model=model,\n",
    "    train_features=train_features,\n",
    "    train_responses=train_responses,\n",
    "    batch_indices=batch_indices,\n",
    "    nbrs_lookup=nbrs_lookup,\n",
    "    training_iterations=10,\n",
    "    optimizer_method=torch.optim.Adam,\n",
    "    learning_rate=1e-2,\n",
    "    scheduler_decay=0.97,\n",
    "    loss_function=\"ce\",\n",
    "    update_frequency=1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "model_trained.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc45bd",
   "metadata": {},
   "source": [
    "We similarly report our prediction performance on the test responses using this trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e4230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.examples.muygps_torch import predict_model\n",
    "predictions,variances,sigma_sq = predict_model(\n",
    "    model=model_trained,\n",
    "    test_features=test_features,\n",
    "    train_features=train_features,\n",
    "    train_responses=train_responses,\n",
    "    nbrs_lookup=nbrs_lookup,\n",
    "    nn_count=nn_count,\n",
    ")\n",
    "\n",
    "print(\"MNIST Prediction Accuracy Using High-Level Training API:\")\n",
    "print(\n",
    "    (\n",
    "        torch.sum(\n",
    "            torch.argmax(predictions,dim=1) == torch.argmax(test_responses,dim=1)\n",
    "        ) / 10000\n",
    "    ).numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a5434",
   "metadata": {},
   "source": [
    "We note that this is quite mediocre performance on MNIST. In the interest of reducing notebook runtime we have used a simple fully-connected neural network model to construct the Gaussian process kernel. To achieve results closer to the state-of-the-art (near 100% accuracy), we recommend using more complex architectures which integrate convolutional kernels into the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "434749ae7207e94f9d6928c9f347c2cd1a679cf18b55a36c093c3f406aed8e17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
