{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021-2023 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function Tutorial\n",
    "\n",
    "This notebook illustrates the loss functions available in the `MuyGPyS` library.\n",
    "These functions are used to formulate the objective function to be optimized while fitting hyperparameters, and so have a large effect on the outcome of training.\n",
    "We will describe each of these loss functions and plot their behaviors to help the user to select the right loss for their problem.\n",
    "\n",
    "Each function in this notebook is available for import from `MuyGPyS.optimize.loss`, and is an object of class `MuyGPyS.optimize.loss.LossFn`.\n",
    "It is possible to define new loss functions by creating a new `LossFn` object.\n",
    "View its documentation for more details.\n",
    "\n",
    "We assume throughout a vector of targets $y$, a prediction (posterior mean) vector $\\mu$, and a posterior variance vector $\\sigma$ for a training batch $B$ with $b$ elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "for m in sys.modules.keys():\n",
    "    if m.startswith(\"Muy\"):\n",
    "        sys.modules.pop(m)\n",
    "%env MUYGPYS_BACKEND=numpy\n",
    "%env MUYGPYS_FTYPE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import SymLogNorm\n",
    "from MuyGPyS.optimize.loss import mse_fn, cross_entropy_fn, lool_fn, lool_fn_unscaled, pseudo_huber_fn, looph_fn\n",
    "from MuyGPyS._src.optimize.loss.numpy import _looph_fn_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('tableau-colorblind10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmax = 3.0\n",
    "point_count = 50\n",
    "ys = np.zeros(point_count)\n",
    "residuals = np.linspace(-mmax, mmax, point_count)\n",
    "smax = 3.0\n",
    "smin = 1e-1\n",
    "sigma_count = 50\n",
    "sigmas = np.linspace(smin, smax, sigma_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance-free Loss Functions\n",
    "\n",
    "`MuyGPyS` features several loss functions that depend only upon the targets $y$ and posterior mean predictions $\\bar{\\mu}$ of your training batch.\n",
    "These loss functions are situationally useful, although they leave the fitting of variance parameters entirely up to the separate, analytic `sigma_sq` optimization function and might not be sensitive to certain variance parameters.\n",
    "As they do not require evaluating the posterior variance $\\sigma^2$ or optimizing the variance scale parameter, these loss functions are generally more efficient to use in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (`mse_fn`)\n",
    "\n",
    "The mean squared error (MSE) is a classic loss that computes\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ell_\\textrm{MSE}(\\bar{\\mu}, y) = \\frac{1}{b} \\sum_{i \\in B} (\\bar{\\mu}_i - y_i)^2.\n",
    "\\end{equation*}\n",
    "\n",
    "The following plot illustrates the MSE as a function of the residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4,3))\n",
    "ax.set_title(\"MSE as a function of the residual\", fontsize=20)\n",
    "ax.set_ylabel(\"loss\", fontsize=15)\n",
    "ax.set_xlabel(\"$\\mu_i - y_i$\", fontsize=15)\n",
    "mses = [mse_fn(ys[i].reshape(1, 1), residuals[i].reshape(1, 1)) for i in range(point_count)]\n",
    "ax.plot(residuals, mses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss (`cross_entropy_fn`)\n",
    "\n",
    "The cross entropy loss is a classic classification loss often used in the fitting of neural networks.\n",
    "For targets in $\\{0, 1\\}$, the library first transforms the predictions to be row-stochastic and then computes\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ell_\\textrm{cross-entropy}(\\bar{\\mu}, y) =\n",
    "\\sum_{i \\in B} y_i \\log(\\bar{\\mu}_i) - (1 - y_i) \\log(1 - \\bar{\\mu}_i)\n",
    "\\end{equation*}      \n",
    "\n",
    "⚠️ This section is under construction. ⚠️ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-Huber Loss (`pseudo_huber_fn`)\n",
    "\n",
    "The pseudo-Huber loss is a smooth approximation to the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss), which is approximately quadratic for small residuals and approximately linear for large residuals.\n",
    "This means that the pseudo-Huber loss is less sensitive to large outliers, which might otherwise force the optimizer to overcompensate in undesirable ways.\n",
    "The pseudo-Huber loss computes\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ell_\\textrm{Pseudo-Huber}(\\bar{\\mu}, y \\mid \\delta) =\n",
    "\\sum_{i=1}^b \\delta^2 \\left ( \n",
    "\\sqrt{1 + \\left ( \\frac{\\bar{\\mu}_i - y_i}{\\delta} \\right )^2} - 1\n",
    "\\right ),\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\delta$ is a parameter that indicates the scale of the boundary between the quadratic and linear parts of the function.\n",
    "The `pseudo_huber_fn` accepts this parameter as the `boundary_scale` keyword argument.\n",
    "Note that the scale of $\\delta$ depends on the units of $y$ and $\\mu$.\n",
    "The following plots show the behavior of the pseudo-Huber loss for a few values of $\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_scales = [0.5, 1.0, 2.5]\n",
    "phs = np.array([\n",
    "    [pseudo_huber_fn(ys[i].reshape(1, 1), residuals[i].reshape(1, 1), boundary_scale=bs) for i in range(point_count)]\n",
    "    for bs in boundary_scales\n",
    "])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "# for i, ax in enumerate(axes):\n",
    "ax.set_title(f\"Pseudo-Huber\", fontsize=20)\n",
    "ax.set_ylabel(\"loss\", fontsize=15)\n",
    "ax.set_xlabel(\"$\\mu_i - y_i$\", fontsize=15)\n",
    "ax.plot(residuals, phs[0, :], linestyle=\"solid\", label=f\"$\\delta = {boundary_scales[0]}$\")\n",
    "ax.plot(residuals, phs[1, :], linestyle=\"dotted\", label=f\"$\\delta = {boundary_scales[1]}$\")\n",
    "ax.plot(residuals, phs[2, :], linestyle=\"dashed\", label=f\"$\\delta = {boundary_scales[2]}$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance-Sensitive Loss Functions\n",
    "\n",
    "`MuyGPyS` also includes loss functions that explicitly depend upon the posterior variances $\\bar{\\Sigma}$, which is a diagonal matrix for a univariate MuyGPs model.\n",
    "These loss functions penalize large variances, and so tend to be more sensitive to variance parameters.\n",
    "This comes at increasing the cost of the linear algebra involved in each evaluation of the objective function by a constant factor.\n",
    "This causes an overall increase in compute time per optimization loop, but that is often worth the trade for sensitivity in practice.\n",
    "\n",
    "$\\sigma$ involves multiplying the unscaled `MuyGPS` variance by the `sigma_sq` variance scaling parameter, which at present must by optimized during each evaluation of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Out Loss (`lool_fn`)\n",
    "\n",
    "The leave-one-out-loss or lool scales and regularizes the MSE to make the loss more sensitive to parameters that primarily act on the variance.\n",
    "lool computes \n",
    "\n",
    "\\begin{equation*}\n",
    "\\ell_\\textrm{lool}(\\bar{\\mu}, y \\mid \\bar{\\Sigma}) = \n",
    "\\sum_{i \\in B} \\left ( \\frac{\\bar{\\mu}_i - y_i}{\\bar{\\Sigma}_{ii}} \\right )^2 + \\log \\bar{\\Sigma}_{ii}.\n",
    "\\end{equation*}\n",
    "\n",
    "The next plot illustrates the loss as a function of both the residual and of $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lools = np.array([\n",
    "    [\n",
    "        lool_fn_unscaled(\n",
    "            ys[i].reshape(1, 1),\n",
    "            residuals[i].reshape(1, 1),\n",
    "            sigmas[sigma_count - 1 - j]\n",
    "        )\n",
    "        for i in range(point_count)\n",
    "    ]\n",
    "    for j in range(sigma_count)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "axes[0].set_title(\"lool\", fontsize=20)\n",
    "axes[0].set_ylabel(\"$\\sigma_i$\", fontsize=15)\n",
    "axes[0].set_xlabel(\"$\\mu_i - y_i$\", fontsize=15)\n",
    "im = axes[0].imshow(\n",
    "    lools, extent=[-mmax, mmax, smin, smax], norm=SymLogNorm(1e-1), cmap=\"coolwarm\", aspect=2.0\n",
    ")\n",
    "fig.colorbar(im, ax=axes[0])\n",
    "\n",
    "axes[1].set_title(\"lool residual cross-section\", fontsize=14)\n",
    "axes[1].set_ylabel(\"lool\", fontsize=15)\n",
    "axes[1].set_xlabel(\"$\\mu_i - y_i$\", fontsize=15)\n",
    "axes[1].plot(residuals, lools[15, :], linestyle=\"solid\", label=\"$\\sigma_i = 1.0$\")\n",
    "axes[1].plot(residuals, lools[7, :], linestyle=\"dotted\", label=\"$\\sigma_i = 0.5$\")\n",
    "axes[1].plot(residuals, lools[0, :], linestyle=\"dashed\", label=\"$\\sigma_i = 0.1$\")\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].set_title(\"lool, variance cross-section\", fontsize=14)\n",
    "axes[2].set_ylabel(\"lool\", fontsize=15)\n",
    "axes[2].set_xlabel(\"$\\sigma_i$\", fontsize=15)\n",
    "axes[2].plot(sigmas, np.flip(lools[:, 33]), linestyle=\"solid\", label=\"$\\mid \\mu_i - y_i \\mid = 1.0$\")\n",
    "axes[2].plot(sigmas, np.flip(lools[:, 29]), linestyle=\"dotted\", label=\"$\\mid \\mu_i - y_i \\mid = 0.5$\")\n",
    "axes[2].plot(sigmas, np.flip(lools[:, 24]), linestyle=\"dashed\", label=\"$\\mid \\mu_i - y_i \\mid = 0.0$\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the cross-section of the lool surface for a fixed $\\sigma$ is quadratic, while the cross section of the lool surface for a fixed residual is logarithmic.\n",
    "For small enough residuals, this curve inverts and assumes negative values for small $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Out Pseudo-Huber (`looph_fn`)\n",
    "\n",
    "The leave-one-out pseudo-Huber loss (looph) is similar in nature to the lool, but is applied to the pseudo-Huber loss instead of MSE.\n",
    "looph computes\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ell_\\textrm{looph}(\\bar{\\mu}, y \\mid \\delta, \\bar{\\Sigma}) =\n",
    "\\sum_{i=1}^b \\delta^2 \\left ( \n",
    "\\sqrt{1 + \\left ( \\frac{\\bar{\\mu}_i - y_i}{\\delta \\bar{\\Sigma}_{ii}} \\right )^2} - 1\n",
    "\\right ) + \\log \\bar{\\Sigma}_{ii},\n",
    "\\end{equation*}\n",
    "\n",
    "where again $\\delta$ is the boundary scale.\n",
    "The next plots illustrate the looph as a function of the residual, $\\sigma$, and $\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loophs = np.array([\n",
    "    [\n",
    "        [\n",
    "            _looph_fn_unscaled(\n",
    "                ys[i].reshape(1, 1),\n",
    "                residuals[i].reshape(1, 1),\n",
    "                sigmas[sigma_count - 1 - j],\n",
    "                boundary_scale=bs\n",
    "            )\n",
    "            for i in range(point_count)\n",
    "        ]\n",
    "        for j in range(sigma_count)\n",
    "    ]\n",
    "    for bs in boundary_scales\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(14,12))\n",
    "for i, bs in enumerate(boundary_scales):\n",
    "    axes[i, 0].set_title(f\"looph ($\\delta={bs}$)\", fontsize=20)\n",
    "    axes[i, 0].set_ylabel(\"$\\sigma_i$\", fontsize=15)\n",
    "    axes[i, 0].set_xlabel(\"$\\mu_i - y_i$\", fontsize=15)\n",
    "    im = axes[i, 0].imshow(\n",
    "        loophs[i, :, :], extent=[-mmax, mmax, smin, smax], norm=SymLogNorm(1e-1), cmap=\"coolwarm\", aspect=2.0\n",
    "    )\n",
    "    fig.colorbar(im, ax=axes[i, 0])\n",
    "\n",
    "    axes[i, 1].set_title(f\"looph residual cross-section ($\\delta={bs}$)\", fontsize=14)\n",
    "    axes[i, 1].set_ylabel(\"looph\", fontsize=15)\n",
    "    axes[i, 1].set_xlabel(\"$\\mu_i - y_i$\", fontsize=15)\n",
    "    axes[i, 1].plot(residuals, loophs[i, 15, :], linestyle=\"solid\", label=\"$\\sigma_i = 1.0$\")\n",
    "    axes[i, 1].plot(residuals, loophs[i, 7, :], linestyle=\"dotted\", label=\"$\\sigma_i = 0.5$\")\n",
    "    axes[i, 1].plot(residuals, loophs[i, 0, :], linestyle=\"dashed\", label=\"$\\sigma_i = 0.1$\")\n",
    "    axes[i, 1].legend()\n",
    "\n",
    "    axes[i, 2].set_title(f\"looph variance cross-section ($\\delta={bs}$)\", fontsize=14)\n",
    "    axes[i, 2].set_ylabel(\"looph\", fontsize=15)\n",
    "    axes[i, 2].set_xlabel(\"$\\sigma_i$\", fontsize=15)\n",
    "    axes[i, 2].plot(sigmas, np.flip(loophs[i, :, 33]), linestyle=\"solid\", label=\"$\\mid \\mu_i - y_i \\mid = 1.0$\")\n",
    "    axes[i, 2].plot(sigmas, np.flip(loophs[i, :, 29]), linestyle=\"dotted\", label=\"$\\mid \\mu_i - y_i \\mid = 0.5$\")\n",
    "    axes[i, 2].plot(sigmas, np.flip(loophs[i, :, 24]), linestyle=\"dashed\", label=\"$\\mid \\mu_i - y_i \\mid = 0.0$\")\n",
    "    axes[i, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show us that the looph function can exhibit a more exaggerated upward slope where the residual is in the linear component of the pseudo-Huber curve but is not so large that it still outweighs the variance component of the loss.\n",
    "Note that in practice that both pseudo Huber loss functions may require more training iterations to converge than their alternatives."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
