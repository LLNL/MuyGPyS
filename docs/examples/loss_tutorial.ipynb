{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021-2023 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function Tutorial\n",
    "\n",
    "This notebook illustrates the loss functions available in the `MuyGPyS` library.\n",
    "These functions are used to formulate the objective function to be optimized while fitting hyperparameters, and so have a large effect on the outcome of workflows.\n",
    "We will describe each of these loss functions and plot their behaviors to help the user to select the right loss for their problem.\n",
    "\n",
    "We assume throughout a vector of targets $y$, a prediction (posterior mean) vector $\\mu$, and a posterior variance vector $\\sigma$ for a training batch $B$ with $b$ elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "for m in sys.modules.keys():\n",
    "    if m.startswith(\"Muy\"):\n",
    "        sys.modules.pop(m)\n",
    "%env MUYGPYS_BACKEND=numpy\n",
    "%env MUYGPYS_FTYPE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "from MuyGPyS.optimize.loss import mse_fn, cross_entropy_fn, lool_fn, lool_fn_unscaled, pseudo_huber_fn, looph_fn\n",
    "from MuyGPyS._src.optimize.loss.numpy import _looph_fn_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmax = 3.0\n",
    "point_count = 50\n",
    "ys = np.zeros(point_count)\n",
    "mus = np.linspace(-mmax, mmax, point_count)\n",
    "smax = 3.0\n",
    "smin = 1e-1\n",
    "sigma_count = 50\n",
    "sigmas = np.linspace(smin, smax, sigma_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance-free Loss Functions\n",
    "\n",
    "`MuyGPyS` features several loss functions that depend only upon $y$ and $\\mu$ of your training batch.\n",
    "These loss functions are situationally useful, although they leave the fitting of variance parameters entirely up to the separate, analytic `sigma_sq` optimization function and might not be sensitive to certain variance parameters.\n",
    "As they do not require evaluating $\\sigma$ or optimizing the variance scaling parameter, these loss functions are generally more efficient to use in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (`mse_fn`)\n",
    "\n",
    "The mean squared error (MSE) is a classic loss that computes\n",
    "$$\n",
    "\\frac{1}{b} \\sum_{i \\in B} (\\mu_i - y)^2.\n",
    "$$\n",
    "The string used to indicate MSE loss in optimization functions is `\"mse\"`.\n",
    "The following plot illustrates the MSE as a function of the residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4,3))\n",
    "ax.set_title(\"MSE as a function of the residual\", fontsize=20)\n",
    "ax.set_ylabel(\"loss\", fontsize=15)\n",
    "ax.set_xlabel(\"$\\mu_i - y_i$\", fontsize=15)\n",
    "mses = [mse_fn(ys[i].reshape(1, 1), mus[i].reshape(1, 1)) for i in range(point_count)]\n",
    "ax.plot(mus, mses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss (`cross_entropy_fn`)\n",
    "\n",
    "The cross entropy loss is a classic classification loss often used in the fitting of neural networks.\n",
    "Optimization functions recognize `\"cross_entropy\"` or `\"log\"` as arguments to `loss_method`. \n",
    "\n",
    "⚠️ This section is under construction. ⚠️ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-Huber Loss (`pseudo_huber_fn`)\n",
    "\n",
    "The pseudo-Huber loss is a smooth approximation to the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss), which is approximately quadratic for small residuals and approximately linear for large residuals.\n",
    "This means that the pseudo-Huber loss is less sensitive to large outliers, which might otherwise force the optimizer to overcompensate in undesirable ways.\n",
    "The pseudo-Huber loss computes\n",
    "$$\n",
    "\\delta^2 \\sum_{i=1}^b \\left ( \n",
    "    \\sqrt{1 + \\left ( \\frac{\\mu_i - y_i}{\\delta} \\right )^2} - 1\n",
    "\\right ),\n",
    "$$\n",
    "where $\\delta$ is a parameter that indicates the barrier between the quadratic and linear parts of the function.\n",
    "The `pseudo_huber_fn` accepts this parameter as the `boundary_scale` keyword argument.\n",
    "Note that the scale of $\\delta$ depends on the units of $y$ and $mu$.\n",
    "The following plots show the behavior of the pseudo-Huber loss for a few values of $\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_scales = [0.5, 1.5, 2.5]\n",
    "phs = np.array([\n",
    "    [pseudo_huber_fn(ys[i].reshape(1, 1), mus[i].reshape(1, 1), boundary_scale=bs) for i in range(point_count)]\n",
    "    for bs in boundary_scales\n",
    "])\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_title(f\"Pseudo-Huber with $\\delta$={boundary_scales[i]}\", fontsize=20)\n",
    "    ax.set_ylabel(\"loss\", fontsize=15)\n",
    "    ax.set_xlabel(\"$\\mu_i - y_i$\", fontsize=15)\n",
    "    ax.plot(mus, phs[i, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance-Sensitive Loss Functions\n",
    "\n",
    "`MuyGPyS` also includes loss functions that explicitly depend upon the posterior variances ($\\sigma$).\n",
    "These loss functions penalize large variances, and so tend to be more sensitive to variance parameters.\n",
    "This comes at increasing the cost of the linear algebra involved in each evaluation of the objective function by a constant factor.\n",
    "This causes an overall increase in compute time per optimization loop, but that is often worth the trade for sensitivity in practice.\n",
    "\n",
    "$\\sigma$ involves multiplying the unscaled `MuyGPS` variance by the `sigma_sq` variance scaling parameter, which at present must by optimized during each evaluation of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Out Loss (`lool_fn`)\n",
    "\n",
    "The leave-one-out-loss or lool scales and regularizes the MSE to make the loss more sensitive to parameters that primarily act on the variance.\n",
    "lool computes \n",
    "$$\n",
    "\\sum_{i \\in B} \\frac{(\\mu_i - y_i)^2}{\\sigma_i} + \\log \\sigma_i.\n",
    "$$\n",
    "The next plot illustrates the loss as a function of both the residual and of $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lools = np.array([\n",
    "    [\n",
    "        lool_fn_unscaled(\n",
    "            ys[i].reshape(1, 1),\n",
    "            mus[i].reshape(1, 1),\n",
    "            sigmas[sigma_count - 1 - j]\n",
    "        )\n",
    "        for i in range(point_count)\n",
    "    ]\n",
    "    for j in range(sigma_count)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(19,4))\n",
    "axes[0].set_title(\"lool\", fontsize=20)\n",
    "axes[0].set_ylabel(\"$\\sigma_i$\", fontsize=15)\n",
    "axes[0].set_xlabel(\"$\\mu_i - y_i$\", fontsize=15)\n",
    "im = axes[0].imshow(\n",
    "    lools, \n",
    "    extent=[-mmax, mmax, smin, smax], \n",
    "    norm=LogNorm(vmin=0.01, vmax=100.0)\n",
    ")\n",
    "fig.colorbar(im, ax=axes[0])\n",
    "\n",
    "axes[1].set_title(\"lool, $\\sigma_i=0.5$\", fontsize=20)\n",
    "axes[1].set_ylabel(\"lool\", fontsize=15)\n",
    "axes[1].set_xlabel(\"$\\mu_i - y_i$\", fontsize=15)\n",
    "axes[1].plot(mus, lools[7, :])\n",
    "\n",
    "axes[2].set_title(\"lool, $\\mid \\mu_i - y_i \\mid=1.0$\", fontsize=20)\n",
    "axes[2].set_ylabel(\"lool\", fontsize=15)\n",
    "axes[2].set_xlabel(\"$\\sigma_i$\", fontsize=15)\n",
    "axes[2].plot(sigmas, np.flip(lools[:, 33]))\n",
    "\n",
    "axes[3].set_title(\"lool, $\\mid \\mu_i - y_i \\mid=0.0$\", fontsize=20)\n",
    "axes[3].set_ylabel(\"lool\", fontsize=15)\n",
    "axes[3].set_xlabel(\"$\\sigma_i$\", fontsize=15)\n",
    "axes[3].plot(sigmas, np.flip(lools[:, 24]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the cross-section of the lool surface for a fixed $\\sigma$ is quadratic, while the cross section of the lool surface for a fixed residual is logarithmic.\n",
    "For small enough residuals, this curve inverts and assumes negative values for small $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Out Pseudo-Huber (`looph_fn`)\n",
    "\n",
    "The leave-one-out pseudo-Huber loss (looph) is similar in nature to the lool, but is applied to the pseudo-Huber loss instead of MSE.\n",
    "looph computes\n",
    "$$\n",
    "\\delta^2 \\sum_{i=1}^b \\left ( \n",
    "    \\sqrt{1 + \\left ( \\frac{\\mu_i - y_i}{\\sigma_i \\delta} \\right )^2} - 1\n",
    "\\right ) + \\log \\sigma_i,\n",
    "$$\n",
    "where again $\\delta$ is the boundary scale.\n",
    "The next plots illustrate the looph as a function of the residual, $\\sigma$, and $\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loophs = np.array([\n",
    "    [\n",
    "        [\n",
    "            _looph_fn_unscaled(\n",
    "                ys[i].reshape(1, 1),\n",
    "                mus[i].reshape(1, 1),\n",
    "                sigmas[sigma_count - 1 - j],\n",
    "                boundary_scale=bs\n",
    "            )\n",
    "            for i in range(point_count)\n",
    "        ]\n",
    "        for j in range(sigma_count)\n",
    "    ]\n",
    "    for bs in boundary_scales\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(19,14))\n",
    "for i, bs in enumerate(boundary_scales):\n",
    "    axes[i, 0].set_title(f\"looph, $\\delta={bs}$\", fontsize=20)\n",
    "    axes[i, 0].set_ylabel(\"$\\sigma_i$\", fontsize=15)\n",
    "    axes[i, 0].set_xlabel(\"$\\mu_i - y_i$\", fontsize=15)\n",
    "    im = axes[i, 0].imshow(\n",
    "        lools, \n",
    "        extent=[-mmax, mmax, smin, smax], \n",
    "        norm=LogNorm(vmin=0.01, vmax=100.0)\n",
    "    )\n",
    "    fig.colorbar(im, ax=axes[i, 0])\n",
    "\n",
    "    axes[i, 1].set_title(f\"looph, $\\delta={bs}$, $\\sigma_i=0.5$\", fontsize=20)\n",
    "    axes[i, 1].set_ylabel(\"looph\", fontsize=15)\n",
    "    axes[i, 1].set_xlabel(\"$\\mu_i - y_i$\", fontsize=15)\n",
    "    axes[i, 1].plot(mus, loophs[i, 7, :])\n",
    "\n",
    "    axes[i, 2].set_title(f\"looph, $\\delta={bs}$, $\\mid \\mu_i - y_i \\mid=1.0$\", fontsize=20)\n",
    "    axes[i, 2].set_ylabel(\"looph\", fontsize=15)\n",
    "    axes[i, 2].set_xlabel(\"$\\sigma_i$\", fontsize=15)\n",
    "    axes[i, 2].plot(sigmas, np.flip(loophs[i, :, 33]))\n",
    "\n",
    "    axes[i, 3].set_title(f\"looph, $\\delta={bs}$, $\\mid \\mu_i - y_i \\mid=0.0$\", fontsize=20)\n",
    "    axes[i, 3].set_ylabel(\"looph\", fontsize=15)\n",
    "    axes[i, 3].set_xlabel(\"$\\sigma_i$\", fontsize=15)\n",
    "    axes[i, 3].plot(sigmas, np.flip(loophs[i, :, 24]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show us that the looph function can exhibit a more exaggerated upward slope where the residual is in the linear component of the pseudo-Huber curve but is not so large that it still outweighs the variance component of the loss."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
