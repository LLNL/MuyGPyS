{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021-2022 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Regression Tutorial\n",
    "\n",
    "This notebook walks through a simple regression workflow and explains the componenets of `MuyGPyS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# This is necessary if JAX is installed as the benchmark GP is not designed with JAX in mind.\n",
    "from MuyGPyS import config\n",
    "if config.muygpys_jax_enabled is True:\n",
    "    config.update(\"muygpys_jax_enabled\", False)\n",
    "if config.muygpys_torch_enabled is True:\n",
    "    config.update(\"muygpys_torch_enabled\", False)\n",
    "\n",
    "from MuyGPyS._test.gp import benchmark_sample, benchmark_sample_full, BenchmarkGP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set a random seed here for consistency when building docs.\n",
    "In practice we would not fix a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling a Curve from a Conventional GP\n",
    "\n",
    "This notebook will use a simple one-dimensional curve sampled from a conventional Gaussian process.\n",
    "The Gaussian process in question is implemented in `MuyGPyS.testing.gp.BenchmarkGP`. \n",
    "It is intended for testing purposes only. \n",
    "We will specify the domain as a grid on a one-dimensional surface and divide the observations into train and test data.\n",
    "Feel free to experiment with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = -10.0\n",
    "ub = 10.0\n",
    "data_count = 5001\n",
    "train_step = 10\n",
    "x = np.linspace(lb, ub, data_count).reshape(data_count, 1)\n",
    "test_features = x[np.mod(np.arange(data_count), train_step) != 0, :]\n",
    "train_features = x[::train_step, :]\n",
    "test_count, _ = test_features.shape\n",
    "train_count, _ = train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define a Matern kernel function and create a conventional GP.\n",
    "Note here that we are setting a small value for `eps`, the prior variance of the \"nugget\" or diagonal noise matrix added to the raw kernel matrix to a small value purely for numerical stability.\n",
    "This is an idealized experiment with no instrument error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nugget_var = 1e-14\n",
    "fixed_length_scale = 1.0\n",
    "benchmark_kwargs = {\n",
    "    \"kern\": \"matern\",\n",
    "    \"metric\": \"l2\",\n",
    "    \"eps\": {\"val\": nugget_var},\n",
    "    \"nu\": {\"val\": 2.0},\n",
    "    \"length_scale\": {\"val\": fixed_length_scale},\n",
    "}\n",
    "gp = BenchmarkGP(**benchmark_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will sample a curve from this GP prior and visualize it.\n",
    "Note that we perturb the train responses (the values that our model will actual receive) with Gaussian measurement noise.\n",
    "Further note that this is not especially fast, as sampling from a conventional Gaussian process requires computing the Cholesky decomposition of a `(data_count, data_count)` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = benchmark_sample(gp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_responses = y[np.mod(np.arange(data_count), train_step) != 0, :]\n",
    "measurement_eps = 1e-5\n",
    "train_responses = y[::train_step, :] + np.random.normal(0, measurement_eps, size=(train_count,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(15, 11))\n",
    "\n",
    "axes[0].set_title(\"Sampled Curve\", fontsize=24)\n",
    "axes[0].set_xlabel(\"Feature Domain\", fontsize=20)\n",
    "axes[0].set_ylabel(\"Response Range\", fontsize=20)\n",
    "axes[0].plot(train_features, train_responses, \"k*\", label=\"perturbed train response\")\n",
    "axes[0].plot(test_features, test_responses, \"g-\", label=\"test response\")\n",
    "axes[0].legend(fontsize=20) \n",
    "\n",
    "vis_subset_size = 10\n",
    "mid = int(train_count / 2)\n",
    "\n",
    "axes[1].set_title(\"Sampled Curve (subset)\", fontsize=24)\n",
    "axes[1].set_xlabel(\"Feature Domain\", fontsize=20)\n",
    "axes[1].set_ylabel(\"Response Range\", fontsize=20)\n",
    "axes[1].plot(\n",
    "    train_features[mid:mid + vis_subset_size], \n",
    "    train_responses[mid:mid + vis_subset_size], \n",
    "    \"k*\", label=\"perturbed train response\"\n",
    ")\n",
    "axes[1].plot(\n",
    "    test_features[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))], \n",
    "    test_responses[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))], \n",
    "    \"g-\", label=\"test response\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now attempt to recover the response on the held-out test data by training a univariate `MuyGPS` model on the perturbed training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Nearest Neighbor Lookups\n",
    "\n",
    "[NN_Wrapper](../MuyGPyS/neighbors.rst) \n",
    "is an api for tasking several KNN libraries with the construction of lookup indexes that empower fast training and inference. \n",
    "The wrapper constructor expects the training features, the number of nearest neighbors, and a method string specifying which algorithm to use, as well as any additional kwargs used by the methods. \n",
    "Currently supported implementations include \n",
    "[exact KNN using sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html) (\"exact\") and \n",
    "[approximate KNN using hnsw](https://github.com/nmslib/hnswlib) (\"hnsw\", requires installing `MuyGPyS` using the `hnswlib` extras flag).\n",
    "\n",
    "Here we construct an exact KNN data example with k = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.neighbors import NN_Wrapper\n",
    "nn_count = 30\n",
    "nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method=\"exact\", algorithm=\"ball_tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `nbrs_lookup` index is then usable to find the nearest neighbors of queries in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Batches of Data\n",
    "\n",
    "`MuyGPyS` includes convenience functions for sampling batches of data from existing datasets.\n",
    "These batches are returned in the form of row indices, both of the sampled data as well as their nearest neighbors.\n",
    "\n",
    "Here we sample a random batch of `train_count` elements. \n",
    "This results in using *all* of the train data for training. \n",
    "We only do that in this case because this example uses a relatively small amount of data.\n",
    "In practice, we would instead set `batch_count` to a resaonable number.\n",
    "In practice we find reasonable values to be in the range of 500-2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.optimize.batch import sample_batch\n",
    "batch_count = train_count\n",
    "batch_indices, batch_nn_indices = sample_batch(\n",
    "    nbrs_lookup, batch_count, train_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `indices` and `nn_indices` arrays are the basic operating blocks of `MuyGPyS` linear algebraic inference.\n",
    "The elements of `indices.shape == (batch_count,)` lists all of the row indices into `train_features` and `train_responses` corresponding to the sampled data.\n",
    "The rows of `nn_indices.shape == (batch_count, nn_count)` list the row indices into `train_features` and `train_responses` corresponding to the nearest neighbors of the sampled data.\n",
    "\n",
    "While the user need not use the \n",
    "[MuyGPyS.optimize.batch](../MuyGPyS/optimize/batch.rst) \n",
    "sampling tools to construct these data, they will need to construct similar indices into their data in order to use `MuyGPyS`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting and Optimizing Hyperparameters\n",
    "\n",
    "One initializes a \n",
    "[MuyGPS](../MuyGPyS/gp/MuyGPS.rst)\n",
    "object by indicating the kernel, as well as optionally specifying hyperparameters.\n",
    "\n",
    "Consider the following example, which constructs a Matern kernel with all parameters fixed aside from `\"nu\"` and `\"sigma_sq\"`.\n",
    "Note that we are here making the simplifying assumtions that we know the true `length_scale` and `measurement error`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.gp.muygps import MuyGPS\n",
    "k_kwargs = {\n",
    "    \"kern\": \"matern\",\n",
    "    \"metric\": \"l2\",\n",
    "    \"eps\": {\"val\": measurement_eps},\n",
    "    \"nu\": {\"val\": \"log_sample\", \"bounds\": (0.1, 5.0)},\n",
    "    \"length_scale\": {\"val\": fixed_length_scale},\n",
    "}\n",
    "muygps = MuyGPS(**k_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters can be initialized or reset using dictionary arguments containing the optional `\"val\"` and `\"bounds\"` keys.\n",
    "`\"val\"` sets the hyperparameter to the given value, and `\"bounds\"` determines the upper and lower bounds to be used for optimization.\n",
    "If `\"bounds\"` is set, `\"val\"` can also take the arguments `\"sample\"` and `\"log_sample\"` to generate a uniform or log uniform sample, respectively.\n",
    "If `\"bounds\"` is set to `\"fixed\"`, the hyperparameter will remain fixed during any optimization.\n",
    "This is the default behavior for all hyperparameters if `\"bounds\"` is unset by the user.\n",
    "\n",
    "One sets the model hyperparameter `eps`, as well as kernel-specific hyperparameters, e.g. `nu` and  `length_scale` for the Matern kernel, at initialization as above.\n",
    "\n",
    "There is one common hyperparameter, the `sigma_sq` scale parameter, that violates these rules.\n",
    "`sigma_sq` cannot be directly set by the user, and always initializes to the value `\"unlearned\"`. \n",
    "We will show how to train `sigma_sq` below.\n",
    "All hyperparameters other than `sigma_sq` are assumed to be fixed unless otherwise specified.\n",
    "\n",
    "MuyGPyS depends upon linear operations on specially-constructed tensors in order to efficiently estimate GP realizations.\n",
    "Constructing these tensors depends upon the nearest neighbor index matrices that we described above.\n",
    "We can construct a distance tensor coalescing all of the square pairwise distance matrices of the nearest neighbors of a batch of points.\n",
    "\n",
    "This snippet constructs a matrix of shape `(batch_count, nn_count)` coalescing all of the distance vectors between the same batch of points and their nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.gp.distance import crosswise_distances\n",
    "batch_crosswise_dists = crosswise_distances(\n",
    "    train_features, \n",
    "    train_features, \n",
    "    batch_indices,\n",
    "    batch_nn_indices,\n",
    "    metric=\"l2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similarly construct a Euclidean distance tensor of shape `(batch_count, nn_count, nn_count)` containing the pairwise distances of the nearest neighbor sets of each sampled batch element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.gp.distance import pairwise_distances\n",
    "pairwise_dists = pairwise_distances(\n",
    "    train_features, batch_nn_indices, metric=\"l2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MuyGPS` object we created earlier allows us to easily realize kernel tensors by way of its kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross = muygps.kernel(batch_crosswise_dists)\n",
    "K = muygps.kernel(pairwise_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform Gaussian process regression, we must utilize these kernel tensors in conjunction with their associated known responses.\n",
    "We can construct these matrices using the index matrices we derived earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_targets = train_responses[batch_indices, :]\n",
    "batch_nn_targets = train_responses[batch_nn_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we often must realize `batch_targets` and `batch_nn_targets` in close proximity to `batch_crosswise_dists` and `batch_pairwise_dists`, the library includes a convenience function \n",
    "[`make_train_tensors()`](../MuyGPyS/gp/distance.rst)\n",
    "that bundles these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.gp.distance import make_train_tensors\n",
    "(\n",
    "    batch_crosswise_dists,\n",
    "    batch_pairwise_dists,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    ") = make_train_tensors(\n",
    "    muygps.kernel.metric,\n",
    "    batch_indices,\n",
    "    batch_nn_indices,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We supply a convenient leave-one-out cross-validation utility \n",
    "([`optimize_from_tensors()`](../MuyGPyS/gp/distance.rst))\n",
    "that utilizes these tensors to repeatedly realize kernel tensors during optimization.\n",
    "This optimization loop wraps a few different batch optimization methods:\n",
    "* [`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.minimize.html) - specifically uses the \"L-BFGS-B\" algorithm.\n",
    "* [`bayes_opt.BayesianOptimization`](https://github.com/fmfn/BayesianOptimization) - the `optimize_from_tensors` wrapper only supports batch mode; examine the internals of the function if you would like to use Bayesian optimization interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use `scipy`-style optimization, we pass the `opt_method=\"scipy\"` kwarg.\n",
    "While it is possible to pass additional kwargs based upon \n",
    "[`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.minimize.html),\n",
    "it is presently unlikely that a user would want to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.optimize.chassis import optimize_from_tensors\n",
    "muygps_scipy = optimize_from_tensors(\n",
    "    muygps,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    "    batch_crosswise_dists,\n",
    "    batch_pairwise_dists,\n",
    "    loss_method=\"mse\",\n",
    "    obj_method=\"loo_crossval\",\n",
    "    opt_method=\"scipy\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can use `bayesian-optimization` style optimization by passing the\n",
    "`opt_method=\"bayesian\"` (alternately `\"bayes\"` or `\"bayes_opt\"`) kwarg.\n",
    "There are several additional parameters that a user might want to set.\n",
    "In particular, `init_points` (the number of \"exploration\" objective function evaluations to perform)\n",
    "and `n_iter` (the number of \"exploitation\" objective function evaluations to perform) are of use to most users.\n",
    "This example also sets `random_state` for consistency.\n",
    "See the documentation of [BayesianOptimization](https://github.com/fmfn/BayesianOptimization) for more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_bayes = optimize_from_tensors(\n",
    "    muygps,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    "    batch_crosswise_dists,\n",
    "    batch_pairwise_dists,\n",
    "    loss_method=\"mse\",\n",
    "    obj_method=\"loo_crossval\",\n",
    "    opt_method=\"bayesian\",\n",
    "    verbose=True,\n",
    "    random_state=1,\n",
    "    init_points=5,\n",
    "    n_iter=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"scipy.optimize.opt.minimize finds that the optimimal `nu` is {muygps_scipy.kernel.nu()}\")\n",
    "print(f\"BayesianOptimization finds that the optimimal `nu` is {muygps_bayes.kernel.nu()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that the returned value for `nu` might be different from the `nu` used by the conventional GP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not need to retain the distances tensors or batch targets for future reference, you can use a related function that realizes them internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from MuyGPyS.optimize.chassis import optimize_from_indices\n",
    "# muygps = optimize_from_indices(\n",
    "#     muygps,\n",
    "#     batch_indices,\n",
    "#     batch_nn_indices,\n",
    "#     train_features,\n",
    "#     train_responses,\n",
    "#     loss_method=\"mse\",\n",
    "#     obj_method=\"loo_crossval\",\n",
    "#     opt_method=\"scipy\",\n",
    "#     verbose=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is a variance scaling parameter that is insensitive to prediction-based optimization, we separately optimize `sigma_sq`.\n",
    "In this case, we invoke \n",
    "[muygps_sigma_sq_optim()](../MuyGPyS/optimize/sigma_sq.rst), \n",
    "which approximates `sigma_sq` based upon the mean of the closed-form `sigma_sq` solutions associated with each of its batched nearest neighbor sets.\n",
    "Note that this method is sensitive to several factors, include `batch_count`, `nn_count`, and the overall size of the dataset, tending to perform better as each of these factors increases. \n",
    "\n",
    "This is usually performed after optimizing other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.optimize.sigma_sq import muygps_sigma_sq_optim\n",
    "\n",
    "scipy_K = muygps_scipy.kernel(batch_pairwise_dists)\n",
    "muygps_scipy = muygps_sigma_sq_optim(muygps_scipy, batch_pairwise_dists, batch_nn_targets, sigma_method=\"analytic\")\n",
    "bayes_K = muygps_bayes.kernel(batch_pairwise_dists)\n",
    "muygps_bayes = muygps_sigma_sq_optim(muygps_bayes, batch_pairwise_dists, batch_nn_targets, sigma_method=\"analytic\")\n",
    "print(f\"scipy-optimized sigma_sq: {muygps_scipy.sigma_sq()}\")\n",
    "print(f\"BayesianOptimization-optimized sigma_sq: {muygps_bayes.sigma_sq()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "With set (or learned) hyperparameters, we are able to use the `muygps` object to predict the response of test data.\n",
    "Several workflows are supported.\n",
    "\n",
    "See below a simple regression workflow, using the data structures built up in this example.\n",
    "This workflow uses the compact tensor-making function \n",
    "[make_regress_tensors()](../MuyGPyS/gp/distance.rst)\n",
    "to succinctly create tensors defining the `pairwise_dists` among each nearest neighbor set, the `crosswise_dists` between each test point and its nearest neighbor set, and the `nn_targets` or responses of the nearest neighbors in each set.\n",
    "We then create the `Kcross` cross-covariance matrix and `K` covariance tensor and pass them to [MuyGPS.regress()](../MuyGPyS/gp/MuyGPS.rst) in order to obtain our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.gp.distance import make_regress_tensors\n",
    "\n",
    "# make the indices\n",
    "test_count, _ = test_features.shape\n",
    "indices = np.arange(test_count)\n",
    "nn_indices, _ = nbrs_lookup.get_nns(test_features)\n",
    "\n",
    "# make distance and target tensors\n",
    "(\n",
    "    crosswise_dists,\n",
    "    pairwise_dists,\n",
    "    nn_targets,\n",
    ") = make_regress_tensors(\n",
    "    muygps.kernel.metric,\n",
    "    indices,\n",
    "    nn_indices,\n",
    "    test_features,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")\n",
    "\n",
    "# Make the kernels\n",
    "scipy_Kcross = muygps_scipy.kernel(crosswise_dists)\n",
    "scipy_K = muygps_scipy.kernel(pairwise_dists)\n",
    "\n",
    "bayes_Kcross = muygps_bayes.kernel(crosswise_dists)\n",
    "bayes_K = muygps_bayes.kernel(pairwise_dists)\n",
    "\n",
    "# perform Gaussian process regression \n",
    "scipy_predictions, scipy_variances = muygps_scipy.regress(\n",
    "    scipy_K,\n",
    "    scipy_Kcross,\n",
    "    train_responses[nn_indices, :],\n",
    "    variance_mode=\"diagonal\",\n",
    "    apply_sigma_sq=True,\n",
    ")\n",
    "\n",
    "bayes_predictions, bayes_variances = muygps_bayes.regress(\n",
    "    bayes_K,\n",
    "    bayes_Kcross,\n",
    "    train_responses[nn_indices, :],\n",
    "    variance_mode=\"diagonal\",\n",
    "    apply_sigma_sq=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here evaluate our prediction performance in terms of RMSE, mean diagonal posterior variance, the mean 95% confidence interval size, and the coverage, which ideally should be near 95%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuyGPyS.optimize.loss import mse_fn\n",
    "\n",
    "scipy_confidence_intervals = np.sqrt(scipy_variances) * 1.96\n",
    "scipy_coverage = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_responses - scipy_predictions)\n",
    "        < scipy_confidence_intervals.reshape(test_count, 1)\n",
    "    )\n",
    "    / test_count\n",
    ")\n",
    "bayes_confidence_intervals = np.sqrt(bayes_variances) * 1.96\n",
    "bayes_coverage = (\n",
    "    np.count_nonzero(\n",
    "        np.abs(test_responses - bayes_predictions)\n",
    "        < bayes_confidence_intervals.reshape(test_count, 1)\n",
    "    )\n",
    "    / test_count\n",
    ")\n",
    "print(f\"scipy results:\")\n",
    "print(f\"\\tRMSE: {np.sqrt(mse_fn(scipy_predictions, test_responses))}\")\n",
    "print(f\"\\tmean diagonal variance: {np.mean(scipy_variances)}\")\n",
    "print(f\"\\tmean confidence interval size: {np.mean(scipy_confidence_intervals * 2)}\")\n",
    "print(f\"\\tcoverage: {scipy_coverage}\")\n",
    "\n",
    "print(f\"bayes results:\")\n",
    "print(f\"\\tRMSE: {np.sqrt(mse_fn(bayes_predictions, test_responses))}\")\n",
    "print(f\"\\tmean diagonal variance: {np.mean(bayes_variances)}\")\n",
    "print(f\"\\tmean confidence interval size: {np.mean(bayes_confidence_intervals * 2)}\")\n",
    "print(f\"\\tcoverage: {bayes_coverage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if you do not need to reuse your tensors, you can run the more compact workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make the indices\n",
    "# test_count, _ = test_features.shape\n",
    "# indices = np.arange(test_count)\n",
    "# nn_indices, _ = nbrs_lookup.get_nns(test_features)\n",
    "\n",
    "# scipy_predictions, scipy_variance = scipy_muygps.regress_from_indices(\n",
    "#     indices, \n",
    "#     nn_indices,\n",
    "#     test_features,\n",
    "#     train_features,\n",
    "#     train_responses,\n",
    "#     variance_mode=\"diagonal\",\n",
    "#     apply_sigma_sq=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These regression examples return predictions (posterior means) and variances for each element of the test dataset.\n",
    "These variances are in the form of diagonal and independent variances that encode the uncertaintainty of the model's predictions at each test point.\n",
    "To scale the variances, they should be multiplied by the trained `sigma_sq` scaling parameters, of which there will be one scalar associated with each dimension of the response.\n",
    "The kwarg `apply_sigma_sq=True` indicates that this scaling will be performed automatically.\n",
    "This is the default behavior, but will be skipped if `sigma_sq == \"unlearned\"`.\n",
    "\n",
    "For a univariate resonse whose variance is obtained with `apply_sigma_sq=False`, the scaled predicted variance is equivalent to multiplying the predicted variances by `muygps.sigma_sq()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy_predictions, scipy_variances = scipy_muygps.regress_from_indices(\n",
    "#     indices, \n",
    "#     nn_indices,\n",
    "#     test_features,\n",
    "#     train_features,\n",
    "#     train_responses,\n",
    "#     variance_mode=\"diagonal\",\n",
    "#     apply_sigma_sq=False,\n",
    "# )\n",
    "# scipy_scaled_variance = scipy_muygps.sigma_sq() * scipy_variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot our responses and evaluate their performance. \n",
    "We plot below the predicted and true curves, as well as the 95% confidence interval.\n",
    "We plot a smaller subset of the data in the lower curve in order to better scrutinize the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(15, 16))\n",
    "\n",
    "axes[0].set_title(\"Sampled Curve with scipy optimized model\", fontsize=24)\n",
    "axes[0].set_xlabel(\"Feature Domain\", fontsize=20)\n",
    "axes[0].set_ylabel(\"Response Range\", fontsize=20)\n",
    "axes[0].plot(train_features, train_responses, \"k*\", label=\"perturbed train response\")\n",
    "axes[0].plot(test_features, test_responses, \"g-\", label=\"test response\")\n",
    "axes[0].plot(test_features, scipy_predictions, \"r--\", label=\"test predictions\")\n",
    "axes[0].fill_between(\n",
    "    test_features[:, 0], \n",
    "    (scipy_predictions[:, 0] - scipy_confidence_intervals),\n",
    "    (scipy_predictions[:, 0] + scipy_confidence_intervals),\n",
    "    facecolor=\"red\",\n",
    "    alpha=0.25,\n",
    "    label=\"95% Confidence Interval\",\n",
    ")\n",
    "axes[0].legend(fontsize=20)\n",
    "\n",
    "axes[1].set_title(\"Sampled Curve (subset) with scipy optimized model\", fontsize=24)\n",
    "axes[1].set_xlabel(\"Feature Domain\", fontsize=20)\n",
    "axes[1].set_ylabel(\"Response Range\", fontsize=20)\n",
    "axes[1].plot(\n",
    "    train_features[mid:mid + vis_subset_size], \n",
    "    train_responses[mid:mid + vis_subset_size], \n",
    "    \"k*\", label=\"perturbed train response\"\n",
    ")\n",
    "axes[1].plot(\n",
    "    test_features[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))], \n",
    "    test_responses[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))], \n",
    "    \"g-\", label=\"test response\"\n",
    ")\n",
    "axes[1].plot(\n",
    "    test_features[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))], \n",
    "    scipy_predictions[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))],\n",
    "    \"r--\", label=\"test predictions\")\n",
    "axes[1].fill_between(\n",
    "    test_features[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))][:, 0], \n",
    "    (scipy_predictions[:, 0] - scipy_confidence_intervals)[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))],\n",
    "    (scipy_predictions[:, 0] + scipy_confidence_intervals)[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))],\n",
    "    facecolor=\"red\",\n",
    "    alpha=0.25,\n",
    "    label=\"95% Confidence Interval\",\n",
    ")\n",
    "axes[1].legend(fontsize=20)\n",
    "\n",
    "axes[2].set_title(\"Sampled Curve (subset) with Bayesian optimized model\", fontsize=24)\n",
    "axes[2].set_xlabel(\"Feature Domain\", fontsize=20)\n",
    "axes[2].set_ylabel(\"Response Range\", fontsize=20)\n",
    "axes[2].plot(\n",
    "    train_features[mid:mid + vis_subset_size], \n",
    "    train_responses[mid:mid + vis_subset_size], \n",
    "    \"k*\", label=\"perturbed train response\"\n",
    ")\n",
    "axes[2].plot(\n",
    "    test_features[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))], \n",
    "    test_responses[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))], \n",
    "    \"g-\", label=\"test response\"\n",
    ")\n",
    "axes[2].plot(\n",
    "    test_features[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))], \n",
    "    bayes_predictions[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))],\n",
    "    \"b--\", label=\"test predictions\")\n",
    "axes[2].fill_between(\n",
    "    test_features[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))][:, 0], \n",
    "    (bayes_predictions[:, 0] - bayes_confidence_intervals)[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))],\n",
    "    (bayes_predictions[:, 0] + bayes_confidence_intervals)[mid * (train_step - 1):mid * (train_step - 1) + (vis_subset_size * (train_step - 1))],\n",
    "    facecolor=\"blue\",\n",
    "    alpha=0.25,\n",
    "    label=\"95% Confidence Interval\",\n",
    ")\n",
    "axes[2].legend(fontsize=20)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
