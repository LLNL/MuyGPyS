{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021-2023 Lawrence Livermore National Security, LLC and other MuyGPyS\n",
    "Project Developers. See the top-level COPYRIGHT file for details.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Regression Tutorial\n",
    "\n",
    "This notebook walks through a simple regression workflow and explains the components of `MuyGPyS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "for m in sys.modules.keys():\n",
    "    if m.startswith(\"Muy\"):\n",
    "        sys.modules.pop(m)\n",
    "%env MUYGPYS_BACKEND=numpy\n",
    "%env MUYGPYS_FTYPE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from MuyGPyS._test.sampler import UnivariateSampler, print_results\n",
    "from MuyGPyS.gp import MuyGPS\n",
    "from MuyGPyS.gp.deformation import Isotropy, l2\n",
    "from MuyGPyS.gp.hyperparameter import AnalyticScale, Parameter\n",
    "from MuyGPyS.gp.kernels import Matern\n",
    "from MuyGPyS.gp.noise import HomoscedasticNoise\n",
    "from MuyGPyS.gp.tensors import crosswise_tensor, pairwise_tensor, make_predict_tensors, make_train_tensors\n",
    "from MuyGPyS.neighbors import NN_Wrapper\n",
    "from MuyGPyS.optimize import Bayes_optimize\n",
    "from MuyGPyS.optimize.batch import sample_batch\n",
    "from MuyGPyS.optimize.loss import lool_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set a random seed here for consistency when building docs.\n",
    "In practice we would not fix a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling a Curve from a Conventional GP\n",
    "\n",
    "This notebook will use a simple one-dimensional curve sampled from a conventional Gaussian process.\n",
    "We will specify the domain as a grid on a one-dimensional surface and divide the observations into train and test data.\n",
    "\n",
    "Feel free to download the source notebook and experiment with different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we specify the region of space, the data size, and the proportion of the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_count = 3000\n",
    "train_ratio = 0.075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that the true data is produced with no noise, so we specify a very small noise prior for numerical stability.\n",
    "This is an idealized experiment with effectively no instrument error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nugget_noise = HomoscedasticNoise(1e-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perturb our simulated observations (the training data) with some i.i.d Gaussian measurement noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_noise = HomoscedasticNoise(1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will specify kernel hyperparameters `smoothness` and `length_scale`.\n",
    "The `length_scale` scales the distances that are inputs to the kernel function, while the `smoothness` parameter determines how differentiable the GP prior is. The larger `smoothness` grows, the smoother sampled functions will become."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_length_scale = Parameter(0.05)\n",
    "sim_smoothness = Parameter(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use all of these parameters to define a Matérn kernel GP and a sampler for convenience.\n",
    "The `UnivariateSampler` class is a convenience class for this tutorial, and is not a part of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = UnivariateSampler(\n",
    "    data_count=data_count,\n",
    "    train_ratio=train_ratio,\n",
    "    kernel=Matern(\n",
    "        smoothness=sim_smoothness,\n",
    "        deformation=Isotropy(\n",
    "            l2,\n",
    "            length_scale=sim_length_scale,\n",
    "        ),\n",
    "    ),\n",
    "    noise=nugget_noise,\n",
    "    measurement_noise=measurement_noise,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will sample a curve from this GP prior and visualize it.\n",
    "Note that we perturb the train responses (the values that our model will actual receive) with Gaussian measurement noise.\n",
    "Further note that this is not especially fast, as sampling from a conventional Gaussian process requires computing the Cholesky decomposition of a `(data_count, data_count)` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = sampler.features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_responses, test_responses = sampler.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampler.plot_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now attempt to recover the response on the held-out test data by training a univariate `MuyGPS` model on the perturbed training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Nearest Neighbor Lookups\n",
    "\n",
    "[NN_Wrapper](../MuyGPyS/neighbors.rst) \n",
    "is an api for tasking several KNN libraries with the construction of lookup indexes that empower fast training and inference. \n",
    "The wrapper constructor expects the training features, the number of nearest neighbors, and a method string specifying which algorithm to use, as well as any additional kwargs used by the methods. \n",
    "Currently supported implementations include \n",
    "[exact KNN using sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html) (\"exact\") and \n",
    "[approximate KNN using hnsw](https://github.com/nmslib/hnswlib) (\"hnsw\", requires installing `MuyGPyS` using the `hnswlib` extras flag).\n",
    "\n",
    "Here we construct an exact KNN data example with k = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_count = 30\n",
    "nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method=\"exact\", algorithm=\"ball_tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `nbrs_lookup` index is then usable to find the nearest neighbors of queries in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Batches of Data\n",
    "\n",
    "`MuyGPyS` includes convenience functions for sampling batches of data from existing datasets.\n",
    "These batches are returned in the form of row indices, both of the sampled data as well as their nearest neighbors.\n",
    "\n",
    "Here we sample a random batch of `train_count` elements. \n",
    "This results in using *all* of the train data for training. \n",
    "We only do that in this case because this example uses a relatively small amount of data.\n",
    "In practice, we would instead set `batch_count` to a resaonable number.\n",
    "In practice we find reasonable values to be in the range of 500-2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = sampler.train_count\n",
    "batch_indices, batch_nn_indices = sample_batch(\n",
    "    nbrs_lookup, batch_count, sampler.train_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `indices` and `nn_indices` arrays are the basic operating blocks of `MuyGPyS` linear algebraic inference.\n",
    "The elements of `indices.shape == (batch_count,)` lists all of the row indices into `train_features` and `train_responses` corresponding to the sampled data.\n",
    "The rows of `nn_indices.shape == (batch_count, nn_count)` list the row indices into `train_features` and `train_responses` corresponding to the nearest neighbors of the sampled data.\n",
    "\n",
    "While the user need not use the \n",
    "[MuyGPyS.optimize.batch](../MuyGPyS/optimize/batch.rst) \n",
    "sampling tools to construct these data, they will need to construct similar indices into their data in order to use `MuyGPyS`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting and Optimizing Hyperparameters\n",
    "\n",
    "One initializes a \n",
    "[MuyGPS](../MuyGPyS/gp/MuyGPS.rst)\n",
    "object by indicating the kernel, as well as optionally specifying hyperparameters.\n",
    "\n",
    "Consider the following example, which constructs a MuyGPs object with a Matérn kernel.\n",
    "The `MuyGPS` object expects a kernel function object, a `noise` noise model parameter, and a variance scale parameter.\n",
    "We will use an `AnalyticScale` instance, which has an analytic optimization method.\n",
    "The `Matern` object expects a deformation function object and a smoothness parameter.\n",
    "We use an isotropic deformation, so `Isotropy` expects a Callable indicating the metric to use (`l2` distance in this case) and a length scale parameter.\n",
    "\n",
    "Hyperparameters can be optionally given a lower and upper optimization bound tuple on creation.\n",
    "If `\"bounds\"` is set, one can also set the hyperparameter value with the arguments `\"sample\"` and `\"log_sample\"` to generate a uniform or log uniform sample, respectively.\n",
    "Hyperparameters without optimization bounds will remain fixed during optimization.\n",
    "\n",
    "In this experiment, we make the simplifying assumptions that we know the true `length_scale` and `measurement_noise`, and reuse the parameters used to create the sampler.\n",
    "We will try to learn the smoothness parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps = MuyGPS(\n",
    "    kernel=Matern(\n",
    "        smoothness=Parameter(\"log_sample\", (0.1, 5.0)),\n",
    "        deformation=Isotropy(\n",
    "            l2,\n",
    "            length_scale=sim_length_scale,\n",
    "        ),\n",
    "    ),\n",
    "    noise=measurement_noise,\n",
    "    scale=AnalyticScale(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one additionally common hyperparameter, the `scale` variance scale parameter, that is treated differently than the others.\n",
    "`scale` cannot be directly set by the user, and always initializes to the value `\"unlearned\"`. \n",
    "We will show how to train `scale` below.\n",
    "All hyperparameters other than `scale` are assumed to be fixed unless otherwise specified.\n",
    "\n",
    "MuyGPyS depends upon linear operations on specially-constructed tensors in order to efficiently estimate GP realizations.\n",
    "Constructing these tensors depends upon the nearest neighbor index matrices that we described above.\n",
    "We can construct a distance tensor coalescing all of the square pairwise distance matrices of the nearest neighbors of a batch of points.\n",
    "\n",
    "This snippet constructs a matrix of shape `(batch_count, nn_count, response_count)` coalescing all of the pairwise difference vectors between the same batch of points and their nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_crosswise_diffs = crosswise_tensor(\n",
    "    train_features, \n",
    "    train_features, \n",
    "    batch_indices,\n",
    "    batch_nn_indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similarly construct a difference tensor of shape `(batch_count, nn_count, nn_count, response_count)` containing the pairwise differences of each response dimension of the nearest neighbor sets of each sampled batch element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_diffs = pairwise_tensor(\n",
    "    train_features, batch_nn_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MuyGPS` object we created earlier allows us to easily realize corresponding kernel tensors by way of its kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross = muygps.kernel(batch_crosswise_diffs)\n",
    "Kcov = muygps.kernel(pairwise_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform Gaussian process regression, we must utilize these kernel tensors in conjunction with their associated known responses.\n",
    "We can construct these matrices using the index matrices we derived earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_targets = train_responses[batch_indices, :]\n",
    "batch_nn_targets = train_responses[batch_nn_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we often must realize `batch_targets` and `batch_nn_targets` in close proximity to `batch_crosswise_diffs` and `batch_pairwise_diffs`, the library includes a convenience function \n",
    "[`make_train_tensors()`](../MuyGPyS/gp/tensors.rst)\n",
    "that bundles these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    ") = make_train_tensors(\n",
    "    batch_indices,\n",
    "    batch_nn_indices,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We supply a convenient leave-one-out cross-validation utility functor class\n",
    "([`OptimizeFn`](../MuyGPyS/gp/optimize.rst))\n",
    "that utilizes these tensors to repeatedly realize kernel tensors during optimization.\n",
    "Optimization implementations are objects of this class.\n",
    "The library currently natively supports two optimization workflows:\n",
    "This optimization loop wraps a few different batch optimization methods (importable from `MuyGPyS.optimize`):\n",
    "* [`Bayes_optimize`](../MuyGPyS/gp/optimize.rst), which wraps [`bayes_opt.BayesianOptimization`](https://github.com/fmfn/BayesianOptimization) in batch mode only.\n",
    "* [`L_BFGS_B_optimize`](../MuyGPyS/gp/optimize.rst), which wraps the \"L-BFGS-B\" implementation in [`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.minimize.html).\n",
    "It is possible to create a new instance of `OptimizeFn` to support custom outer-loop optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses `Bayes_optimize`.\n",
    "There are several additional parameters inherited from the implementation that a user might want to set.\n",
    "In particular, `init_points` (the number of \"exploration\" objective function evaluations to perform)\n",
    "and `n_iter` (the number of \"exploitation\" objective function evaluations to perform) are of use to most users.\n",
    "This example also sets `random_state` for consistency.\n",
    "See the documentation of [BayesianOptimization](https://github.com/fmfn/BayesianOptimization) for more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_optimized = Bayes_optimize(\n",
    "    muygps,\n",
    "    batch_targets,\n",
    "    batch_nn_targets,\n",
    "    batch_crosswise_diffs,\n",
    "    batch_pairwise_diffs,\n",
    "    loss_fn=lool_fn,\n",
    "    verbose=True,\n",
    "    random_state=1,\n",
    "    init_points=5,\n",
    "    n_iter=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is a variance scaling parameter that is insensitive to prediction-based optimization, we separately optimize `scale`.\n",
    "In this case, we invoke \n",
    "[muygps.optimize_scale()](../MuyGPyS/gp/muygps.rst), \n",
    "which approximates `scale` based upon the mean of the closed-form `scale` solutions associated with each of its batched nearest neighbor sets.\n",
    "Note that this method is sensitive to several factors, include `batch_count`, `nn_count`, and the overall size of the dataset, tending to perform better as each of these factors increases.\n",
    "If we had instead used the optimization-free `MuyGPyS.gp.hyperparameter.scale.Scale` class, this function would effectively be a no-op and leave the value of `muygps_optimized.scale` unchanged.\n",
    "\n",
    "This is usually performed after optimizing other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muygps_optimized = muygps_optimized.optimize_scale(batch_pairwise_diffs, batch_nn_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "With set (or learned) hyperparameters, we are able to use the `muygps` object to predict the response of test data.\n",
    "Several workflows are supported.\n",
    "\n",
    "See below a simple regression workflow, using the data structures built up in this example.\n",
    "This workflow uses the compact tensor-making function \n",
    "[make_predict_tensors()](../MuyGPyS/gp/tensors.rst)\n",
    "to succinctly create tensors defining the `pairwise_dists` among each nearest neighbor set, the `crosswise_dists` between each test point and its nearest neighbor set, and the `nn_targets` or responses of the nearest neighbors in each set.\n",
    "We then create the `Kcross` cross-covariance matrix and `Kcov` covariance tensor and pass them to [MuyGPS.posterior_mean()](../MuyGPyS/gp/MuyGPS.rst) and [MuyGPS.posterior_variance()](../MuyGPyS/gp/MuyGPS.rst) in order to obtain our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we find the indices of the nearest neighbors of all of the test elements and save the results in `test_nn_indices`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count, _ = test_features.shape\n",
    "indices = np.arange(test_count)\n",
    "test_nn_indices, _ = nbrs_lookup.get_nns(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use `nn_indices` to make difference and target tensors for the test data.\n",
    "These tensors are similar to those used for batch optimization, except that we do not assume that we know the targets of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    test_crosswise_diffs,\n",
    "    test_pairwise_diffs,\n",
    "    test_nn_targets,\n",
    ") = make_predict_tensors(\n",
    "    indices,\n",
    "    test_nn_indices,\n",
    "    test_features,\n",
    "    train_features,\n",
    "    train_responses,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the kernel tensors for the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcross = muygps_optimized.kernel(test_crosswise_diffs)\n",
    "Kcov = muygps_optimized.kernel(test_pairwise_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regression example returns predictions (posterior means) and variances for each element of the test dataset.\n",
    "These variances are in the form of diagonal and independent variances that encode the uncertaintainty of the model's predictions at each test point.\n",
    "To scale the variances, they should be multiplied by the trained `scale` scaling parameters, of which there will be one scalar associated with each dimension of the response.\n",
    "The kwarg `apply_scale=True` indicates that this scaling will be performed automatically.\n",
    "This is the default behavior, but will be skipped if `scale == \"unlearned\"`.\n",
    "\n",
    "For a univariate resonse whose variance is obtained with `scale=False`, the scaled predicted variance is equivalent to multiplying the predicted variances by `muygps.scale()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `MuyGPS.posterior_mean()` and `MuyGPS.posterior_variance()` functions to find the posterior means and variances associated with each training prediction.\n",
    "The 95% confidence interval sizes are straightforward to compute as $\\sigma * 1.96$, where $\\sigma$ is the standard deviation.\n",
    "We compute coverage as the proportion of posterior means that differ from the true response by no more than the confidence interval size.\n",
    "We coverage for the 95% confidence intervals ideally should be near 95%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = muygps_optimized.posterior_mean(Kcov, Kcross, test_nn_targets)\n",
    "variances = muygps_optimized.posterior_variance(Kcov, Kcross)\n",
    "confidence_intervals = np.sqrt(variances) * 1.96\n",
    "coverage = np.count_nonzero(np.abs(test_responses - predictions) < confidence_intervals) / test_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use RMSE, the mean diagonal variance and confidence interval size, as well as coverage to analyze our fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(\n",
    "    test_responses, (\"optimized\", muygps_optimized, predictions, variances, confidence_intervals, coverage)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that the returned value for `smoothness` might be different from the `smoothness` used by the conventional GP.\n",
    "Also, the value of $\\sigma^2$ is a little different from the \"true\" value of 1.0.\n",
    "However, our mean predictions have low RMSE and our confidence intervals are low on average while our 95% confidence intervals succeed in covering ~95% of the true responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot our responses and evaluate their performance. \n",
    "We plot below the predicted and true curves, as well as the 95% confidence interval.\n",
    "We plot a smaller subset of the data in the lower curve in order to better scrutinize the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampler.plot_results((\"optimized\", predictions, confidence_intervals))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
